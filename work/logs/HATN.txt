loading data...
source domain:  books target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 11843
vocab-size:  100530
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'favorite', 'fantastic', 'enjoyed', 'funny', 'love', 'brilliant', 'classic', 'awesome', 'well', 'entertaining', 'interesting', 'useful', 'incredible', 'fascinating', 'nice', 'loved', 'refreshing', 'inspiring', 'amazing', 'superb', 'fun', 'valuable', 'helps', 'compelling', 'wonderful', 'solid', 'inspirational', 'helpful', 'important', 'wonderfully', 'hilarious', 'invaluable', 'essential', 'riveting', 'emotional', 'impressive', 'readable', 'authentic', 'honest', 'gives', 'sad', 'enlightening', 'pleasant', 'powerful', 'loves', 'liked', 'read', 'humorous', 'deserves', 'heartwarming', 'worthwhile', 'believable']
['disappointing', 'poorly', 'boring', 'disappointed', 'worst', 'horrible', 'useless', 'annoying', 'repetitive', 'confusing', 'lacks', 'hard', 'tedious', 'misleading', 'awful', 'ridiculous', 'difficult', 'pathetic', 'simplistic', 'unnecessary', 'flawed', 'terrible', 'shallow', 'slow', 'mediocre', 'better', 'laughable', 'frustrating', 'tired', 'wasted', 'bad', 'outdated', 'uninspired', 'contrived', 'waste', 'amusing', 'trite', 'uninteresting', 'disjointed', 'sloppy', 'predictable', 'silly', 'dissapointed', 'absurd', 'sorry', 'wrong', 'tiresome', 'impossible', 'insulting', 'worthless', 'weakest', 'inaccurate', 'ok', 'unreadable', 'lame', 'unbelievable', 'unfortunate', 'wastes', 'sophomoric', 'deceived', 'fails', 'weak', 'unlikeable', 'terribly', 'repetitious', 'overrated', 'hackneyed', 'trying', 'dangerous']
max  story size: 226
mean story size: 8
max  sentence size: 783
mean sentence size: 19
max memory size: 20
5600 400 6000 15750 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 392.64447570, sen-loss: 76.86802083, dom-loss: 79.25041121, src-aux-loss: 122.87815541, tar-aux-loss: 113.64788836
Epoch: [1  ] train-acc: 0.67660714, dom-acc: 0.67116071, val-acc: 0.67750000, val_loss: 0.65289891
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 365.20356846, sen-loss: 70.31201476, dom-loss: 76.50024581, src-aux-loss: 114.92448002, tar-aux-loss: 103.46682864
Epoch: [2  ] train-acc: 0.75178571, dom-acc: 0.72392857, val-acc: 0.75750000, val_loss: 0.58705676
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 349.69688082, sen-loss: 62.86016980, dom-loss: 75.74408245, src-aux-loss: 110.41742474, tar-aux-loss: 100.67520231
Epoch: [3  ] train-acc: 0.79982143, dom-acc: 0.71000000, val-acc: 0.81500000, val_loss: 0.50924724
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 335.45945287, sen-loss: 54.81946769, dom-loss: 75.40760678, src-aux-loss: 107.21053791, tar-aux-loss: 98.02184254
Epoch: [4  ] train-acc: 0.83035714, dom-acc: 0.68580357, val-acc: 0.82750000, val_loss: 0.43385270
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 324.74610257, sen-loss: 47.77352390, dom-loss: 75.36138135, src-aux-loss: 104.66494983, tar-aux-loss: 96.94624764
Epoch: [5  ] train-acc: 0.84500000, dom-acc: 0.67616071, val-acc: 0.84500000, val_loss: 0.37871873
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 315.04613638, sen-loss: 42.29169697, dom-loss: 75.40706897, src-aux-loss: 102.74105793, tar-aux-loss: 94.60631311
Epoch: [6  ] train-acc: 0.85517857, dom-acc: 0.71196429, val-acc: 0.86000000, val_loss: 0.35618311
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 310.06212330, sen-loss: 39.23055144, dom-loss: 75.30012053, src-aux-loss: 101.52661496, tar-aux-loss: 94.00483513
Epoch: [7  ] train-acc: 0.86892857, dom-acc: 0.74580357, val-acc: 0.85750000, val_loss: 0.32813293
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 305.85202503, sen-loss: 36.67870724, dom-loss: 75.34418941, src-aux-loss: 100.33670157, tar-aux-loss: 93.49242699
Epoch: [8  ] train-acc: 0.87678571, dom-acc: 0.74392857, val-acc: 0.87000000, val_loss: 0.31892562
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 302.67563009, sen-loss: 35.14108400, dom-loss: 75.50985926, src-aux-loss: 99.09538400, tar-aux-loss: 92.92930448
Epoch: [9  ] train-acc: 0.88303571, dom-acc: 0.73026786, val-acc: 0.87250000, val_loss: 0.31747267
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 300.20505714, sen-loss: 33.56984374, dom-loss: 75.62812400, src-aux-loss: 98.40732753, tar-aux-loss: 92.59976453
Epoch: [10 ] train-acc: 0.88303571, dom-acc: 0.72008929, val-acc: 0.86250000, val_loss: 0.31359842
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 298.15914702, sen-loss: 32.83644764, dom-loss: 75.76719749, src-aux-loss: 97.96653718, tar-aux-loss: 91.58896399
Epoch: [11 ] train-acc: 0.89035714, dom-acc: 0.71151786, val-acc: 0.88250000, val_loss: 0.31513894
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 297.35196304, sen-loss: 31.97150369, dom-loss: 75.82183552, src-aux-loss: 96.86201638, tar-aux-loss: 92.69660735
Epoch: [12 ] train-acc: 0.89321429, dom-acc: 0.71901786, val-acc: 0.88500000, val_loss: 0.30857715
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 295.44029713, sen-loss: 31.42694026, dom-loss: 76.06901914, src-aux-loss: 96.39830470, tar-aux-loss: 91.54603356
Epoch: [13 ] train-acc: 0.89285714, dom-acc: 0.70678571, val-acc: 0.87000000, val_loss: 0.31056195
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 293.21204829, sen-loss: 30.70320117, dom-loss: 76.26043177, src-aux-loss: 95.90252632, tar-aux-loss: 90.34589034
Epoch: [14 ] train-acc: 0.89767857, dom-acc: 0.70785714, val-acc: 0.87500000, val_loss: 0.30643854
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 292.12090921, sen-loss: 30.32234441, dom-loss: 76.17801493, src-aux-loss: 95.40844053, tar-aux-loss: 90.21210808
Epoch: [15 ] train-acc: 0.89857143, dom-acc: 0.70625000, val-acc: 0.88250000, val_loss: 0.30385667
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 291.86312771, sen-loss: 29.73324004, dom-loss: 76.36384320, src-aux-loss: 94.91791600, tar-aux-loss: 90.84812605
Epoch: [16 ] train-acc: 0.90232143, dom-acc: 0.70294643, val-acc: 0.89500000, val_loss: 0.30659276
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 291.27866983, sen-loss: 29.49423937, dom-loss: 76.69019705, src-aux-loss: 94.31964904, tar-aux-loss: 90.77458435
Epoch: [17 ] train-acc: 0.90446429, dom-acc: 0.70062500, val-acc: 0.88500000, val_loss: 0.30105442
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 289.61727905, sen-loss: 28.84227601, dom-loss: 76.62629557, src-aux-loss: 93.98466861, tar-aux-loss: 90.16403741
Epoch: [18 ] train-acc: 0.90607143, dom-acc: 0.69169643, val-acc: 0.88750000, val_loss: 0.30072266
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 288.77602983, sen-loss: 28.45657235, dom-loss: 76.90683430, src-aux-loss: 93.41044527, tar-aux-loss: 90.00217527
Epoch: [19 ] train-acc: 0.90678571, dom-acc: 0.68276786, val-acc: 0.88000000, val_loss: 0.30089697
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 289.26867700, sen-loss: 28.23137576, dom-loss: 77.12306559, src-aux-loss: 93.16030103, tar-aux-loss: 90.75393468
Epoch: [20 ] train-acc: 0.90946429, dom-acc: 0.68080357, val-acc: 0.89000000, val_loss: 0.29873630
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 286.38849878, sen-loss: 27.71124094, dom-loss: 77.13523674, src-aux-loss: 92.57374346, tar-aux-loss: 88.96827775
Epoch: [21 ] train-acc: 0.90785714, dom-acc: 0.67035714, val-acc: 0.88000000, val_loss: 0.29937926
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 287.70777583, sen-loss: 27.53231567, dom-loss: 77.24395770, src-aux-loss: 92.37568969, tar-aux-loss: 90.55581254
Epoch: [22 ] train-acc: 0.91375000, dom-acc: 0.67008929, val-acc: 0.89750000, val_loss: 0.29934996
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 285.14222741, sen-loss: 27.26028327, dom-loss: 77.28453434, src-aux-loss: 91.83977407, tar-aux-loss: 88.75763696
Epoch: [23 ] train-acc: 0.91303571, dom-acc: 0.66160714, val-acc: 0.88500000, val_loss: 0.29775628
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 285.63218045, sen-loss: 26.87915391, dom-loss: 77.47785574, src-aux-loss: 91.43775535, tar-aux-loss: 89.83741695
Epoch: [24 ] train-acc: 0.91410714, dom-acc: 0.66348214, val-acc: 0.88750000, val_loss: 0.29691893
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 285.64271522, sen-loss: 26.63510048, dom-loss: 77.53053856, src-aux-loss: 91.32608980, tar-aux-loss: 90.15098590
Epoch: [25 ] train-acc: 0.91375000, dom-acc: 0.64758929, val-acc: 0.88750000, val_loss: 0.29826805
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 282.98125029, sen-loss: 26.21978919, dom-loss: 77.54650074, src-aux-loss: 90.64487088, tar-aux-loss: 88.57008988
Epoch: [26 ] train-acc: 0.91750000, dom-acc: 0.64776786, val-acc: 0.89000000, val_loss: 0.29793116
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 282.59057379, sen-loss: 26.06393100, dom-loss: 77.63942808, src-aux-loss: 90.40628964, tar-aux-loss: 88.48092562
Epoch: [27 ] train-acc: 0.91678571, dom-acc: 0.64758929, val-acc: 0.89250000, val_loss: 0.29637420
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 282.49487686, sen-loss: 25.75001772, dom-loss: 77.65408927, src-aux-loss: 89.80477297, tar-aux-loss: 89.28599674
Epoch: [28 ] train-acc: 0.91678571, dom-acc: 0.64758929, val-acc: 0.89000000, val_loss: 0.30207068
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 280.65411639, sen-loss: 25.42443407, dom-loss: 77.61935395, src-aux-loss: 89.45257187, tar-aux-loss: 88.15775436
Epoch: [29 ] train-acc: 0.91892857, dom-acc: 0.63946429, val-acc: 0.89500000, val_loss: 0.29604408
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 280.75475883, sen-loss: 25.27235761, dom-loss: 77.83290178, src-aux-loss: 89.15816313, tar-aux-loss: 88.49133700
Epoch: [30 ] train-acc: 0.92017857, dom-acc: 0.63821429, val-acc: 0.89500000, val_loss: 0.29701760
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 281.10515642, sen-loss: 24.91225534, dom-loss: 77.74084717, src-aux-loss: 88.91234171, tar-aux-loss: 89.53971213
Epoch: [31 ] train-acc: 0.91928571, dom-acc: 0.64017857, val-acc: 0.89500000, val_loss: 0.30479655
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 279.00280309, sen-loss: 24.59930433, dom-loss: 77.85771960, src-aux-loss: 88.68390435, tar-aux-loss: 87.86187506
Epoch: [32 ] train-acc: 0.92053571, dom-acc: 0.63589286, val-acc: 0.89250000, val_loss: 0.30353719
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 278.92756820, sen-loss: 24.45075216, dom-loss: 77.74425185, src-aux-loss: 88.03232610, tar-aux-loss: 88.70023763
Epoch: [33 ] train-acc: 0.92482143, dom-acc: 0.64125000, val-acc: 0.89500000, val_loss: 0.29523385
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 278.45342278, sen-loss: 23.99629885, dom-loss: 77.72774571, src-aux-loss: 87.95206290, tar-aux-loss: 88.77731472
Epoch: [34 ] train-acc: 0.92446429, dom-acc: 0.62482143, val-acc: 0.89750000, val_loss: 0.29654053
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 277.34618068, sen-loss: 23.95409682, dom-loss: 77.75787157, src-aux-loss: 87.36426491, tar-aux-loss: 88.26994854
Epoch: [35 ] train-acc: 0.92303571, dom-acc: 0.62348214, val-acc: 0.89250000, val_loss: 0.29754642
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 276.27479076, sen-loss: 23.48929805, dom-loss: 77.91505182, src-aux-loss: 86.97639722, tar-aux-loss: 87.89404273
Epoch: [36 ] train-acc: 0.92625000, dom-acc: 0.63017857, val-acc: 0.89750000, val_loss: 0.29887879
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 274.91537118, sen-loss: 23.31063720, dom-loss: 77.82018566, src-aux-loss: 86.51092982, tar-aux-loss: 87.27362007
Epoch: [37 ] train-acc: 0.93053571, dom-acc: 0.62839286, val-acc: 0.90000000, val_loss: 0.29616469
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 275.15222645, sen-loss: 22.88580396, dom-loss: 77.91230994, src-aux-loss: 86.22346377, tar-aux-loss: 88.13064915
Epoch: [38 ] train-acc: 0.92767857, dom-acc: 0.63151786, val-acc: 0.89500000, val_loss: 0.30062342
---------------------------------------------------

Successfully load model from save path: ./work/models/books_dvd_HATN.ckpt
Best Epoch: [ 33] best val accuracy: 0.00000000 best val loss: 0.29523385
Testing accuracy: 0.87700000
./work/attentions/books_dvd_train_HATN.txt
./work/attentions/books_dvd_test_HATN.txt
loading data...
source domain:  books target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 17009
vocab-size:  83050
['great', 'excellent', 'good', 'best', 'easy', 'highly', 'enjoyable', 'favorite', 'fantastic', 'love', 'brilliant', 'awesome', 'funny', 'well', 'classic', 'entertaining', 'enjoyed', 'interesting', 'useful', 'loved', 'incredible', 'nice', 'solid', 'amazing', 'fascinating', 'superb', 'refreshing', 'amusing', 'essential', 'important', 'readable', 'honest', 'fun', 'valuable', 'invaluable', 'wonderfully', 'compelling', 'sad', 'beautifully', 'inspirational', 'helpful', 'impressive', 'hilarious', 'wonderful', 'inspiring', 'enlightening', 'riveting', 'worthwhile', 'finest', 'pleasant', 'authentic', 'liked', 'simple', 'humorous', 'exceptional', 'emotional']
['disappointing', 'disappointed', 'boring', 'poorly', 'horrible', 'worst', 'useless', 'repetitive', 'hard', 'annoying', 'confusing', 'misleading', 'better', 'difficult', 'tedious', 'terrible', 'awful', 'laughable', 'unnecessary', 'flawed', 'lacks', 'pathetic', 'shallow', 'ridiculous', 'mediocre', 'wasted', 'slow', 'simplistic', 'bad', 'frustrating', 'waste', 'outdated', 'uninspired', 'uninteresting', 'sloppy', 'trite', 'tired', 'wrong', 'predictable', 'silly', 'contrived', 'dissapointed', 'disjointed', 'trying', 'dangerous', 'unbelievable', 'tiresome', 'expensive', 'dull', 'choppy', 'unreadable', 'terribly', 'impossible', 'wastes', 'false', 'sophomoric', 'deceived', 'insulting', 'weak', 'worthless', 'unfortunate', 'incorrect', 'weakest', 'overrated', 'biased', 'inaccurate', 'hackneyed', 'pointless']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 18
max memory size: 20
5600 400 6000 15750 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 410.95931268, sen-loss: 76.73321694, dom-loss: 78.04397219, src-aux-loss: 131.98481476, tar-aux-loss: 124.19730902
Epoch: [1  ] train-acc: 0.68375000, dom-acc: 0.85776786, val-acc: 0.69750000, val_loss: 0.65001917
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 382.38969398, sen-loss: 69.87574345, dom-loss: 73.59005064, src-aux-loss: 123.51073486, tar-aux-loss: 115.41316348
Epoch: [2  ] train-acc: 0.75750000, dom-acc: 0.91294643, val-acc: 0.76750000, val_loss: 0.58134770
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 363.81699157, sen-loss: 62.18749419, dom-loss: 72.31644559, src-aux-loss: 118.59146661, tar-aux-loss: 110.72158521
Epoch: [3  ] train-acc: 0.79732143, dom-acc: 0.90419643, val-acc: 0.80750000, val_loss: 0.50079775
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 351.68668127, sen-loss: 54.29888639, dom-loss: 72.76796246, src-aux-loss: 115.63468146, tar-aux-loss: 108.98515087
Epoch: [4  ] train-acc: 0.82303571, dom-acc: 0.87937500, val-acc: 0.84250000, val_loss: 0.43172556
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 341.75056028, sen-loss: 47.48361993, dom-loss: 73.61816311, src-aux-loss: 113.11978340, tar-aux-loss: 107.52899295
Epoch: [5  ] train-acc: 0.85089286, dom-acc: 0.85276786, val-acc: 0.85500000, val_loss: 0.37371391
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 334.19641566, sen-loss: 42.29137263, dom-loss: 74.72594815, src-aux-loss: 111.11651540, tar-aux-loss: 106.06257987
Epoch: [6  ] train-acc: 0.86178571, dom-acc: 0.80839286, val-acc: 0.86500000, val_loss: 0.34732422
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 330.08789039, sen-loss: 38.91688049, dom-loss: 75.85814267, src-aux-loss: 109.88384867, tar-aux-loss: 105.42901808
Epoch: [7  ] train-acc: 0.87142857, dom-acc: 0.73357143, val-acc: 0.87500000, val_loss: 0.32981640
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 325.54543996, sen-loss: 36.49956429, dom-loss: 76.79391491, src-aux-loss: 108.51126283, tar-aux-loss: 103.74069619
Epoch: [8  ] train-acc: 0.87553571, dom-acc: 0.68857143, val-acc: 0.86250000, val_loss: 0.31621149
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 322.63886428, sen-loss: 35.02274266, dom-loss: 77.40898860, src-aux-loss: 107.35792679, tar-aux-loss: 102.84920758
Epoch: [9  ] train-acc: 0.88250000, dom-acc: 0.65366071, val-acc: 0.88250000, val_loss: 0.31943527
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 322.47405839, sen-loss: 33.48217975, dom-loss: 78.06369942, src-aux-loss: 106.66586751, tar-aux-loss: 104.26231009
Epoch: [10 ] train-acc: 0.88482143, dom-acc: 0.63214286, val-acc: 0.86000000, val_loss: 0.30849409
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 319.51805186, sen-loss: 32.99192484, dom-loss: 78.57845449, src-aux-loss: 106.34169716, tar-aux-loss: 101.60597378
Epoch: [11 ] train-acc: 0.89339286, dom-acc: 0.60937500, val-acc: 0.89250000, val_loss: 0.30873582
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 317.38161445, sen-loss: 31.92875856, dom-loss: 78.41810364, src-aux-loss: 105.10220408, tar-aux-loss: 101.93254733
Epoch: [12 ] train-acc: 0.89321429, dom-acc: 0.61928571, val-acc: 0.88750000, val_loss: 0.31015143
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 316.39677525, sen-loss: 31.33803172, dom-loss: 78.80185330, src-aux-loss: 104.74889761, tar-aux-loss: 101.50799167
Epoch: [13 ] train-acc: 0.89035714, dom-acc: 0.59553571, val-acc: 0.86250000, val_loss: 0.30724940
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 314.47394753, sen-loss: 30.68746272, dom-loss: 79.01286745, src-aux-loss: 104.08217937, tar-aux-loss: 100.69143927
Epoch: [14 ] train-acc: 0.89839286, dom-acc: 0.59205357, val-acc: 0.88250000, val_loss: 0.30206692
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 313.53573966, sen-loss: 30.29884154, dom-loss: 78.97139353, src-aux-loss: 103.60811454, tar-aux-loss: 100.65738839
Epoch: [15 ] train-acc: 0.90303571, dom-acc: 0.58892857, val-acc: 0.88500000, val_loss: 0.29987115
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 312.32393217, sen-loss: 29.64014928, dom-loss: 78.59877026, src-aux-loss: 102.99631840, tar-aux-loss: 101.08869535
Epoch: [16 ] train-acc: 0.90553571, dom-acc: 0.59723214, val-acc: 0.89000000, val_loss: 0.30234906
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 310.89881730, sen-loss: 29.41554752, dom-loss: 78.38043970, src-aux-loss: 102.61622232, tar-aux-loss: 100.48660606
Epoch: [17 ] train-acc: 0.90589286, dom-acc: 0.60330357, val-acc: 0.89250000, val_loss: 0.29956836
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 308.76842809, sen-loss: 28.76407526, dom-loss: 77.90936852, src-aux-loss: 102.24758095, tar-aux-loss: 99.84740323
Epoch: [18 ] train-acc: 0.90714286, dom-acc: 0.61946429, val-acc: 0.88750000, val_loss: 0.30149436
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 307.27768302, sen-loss: 28.46352796, dom-loss: 77.79520082, src-aux-loss: 101.56894630, tar-aux-loss: 99.45000720
Epoch: [19 ] train-acc: 0.90428571, dom-acc: 0.62071429, val-acc: 0.88500000, val_loss: 0.29835811
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 307.54952836, sen-loss: 28.18194491, dom-loss: 77.63643044, src-aux-loss: 101.09784257, tar-aux-loss: 100.63330978
Epoch: [20 ] train-acc: 0.91000000, dom-acc: 0.62366071, val-acc: 0.88750000, val_loss: 0.29977873
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 304.92248392, sen-loss: 27.68580554, dom-loss: 77.31933248, src-aux-loss: 100.70594972, tar-aux-loss: 99.21139467
Epoch: [21 ] train-acc: 0.90875000, dom-acc: 0.63366071, val-acc: 0.88500000, val_loss: 0.29595748
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 305.07177067, sen-loss: 27.45780430, dom-loss: 77.35479772, src-aux-loss: 100.42859334, tar-aux-loss: 99.83057529
Epoch: [22 ] train-acc: 0.91303571, dom-acc: 0.63419643, val-acc: 0.88500000, val_loss: 0.30028525
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 302.97870278, sen-loss: 27.32808257, dom-loss: 76.88394654, src-aux-loss: 100.10096204, tar-aux-loss: 98.66571182
Epoch: [23 ] train-acc: 0.91267857, dom-acc: 0.64794643, val-acc: 0.88750000, val_loss: 0.29507479
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 302.90484118, sen-loss: 26.80634899, dom-loss: 76.97768629, src-aux-loss: 99.72473431, tar-aux-loss: 99.39607090
Epoch: [24 ] train-acc: 0.91535714, dom-acc: 0.64785714, val-acc: 0.88750000, val_loss: 0.29772672
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 302.11314273, sen-loss: 26.59760557, dom-loss: 77.01792282, src-aux-loss: 99.33922857, tar-aux-loss: 99.15838701
Epoch: [25 ] train-acc: 0.91571429, dom-acc: 0.63910714, val-acc: 0.89250000, val_loss: 0.29486865
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 300.90742540, sen-loss: 26.22785215, dom-loss: 77.15890670, src-aux-loss: 98.94705069, tar-aux-loss: 98.57361561
Epoch: [26 ] train-acc: 0.91589286, dom-acc: 0.63687500, val-acc: 0.88500000, val_loss: 0.29667145
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 300.48541760, sen-loss: 26.11627748, dom-loss: 77.30208862, src-aux-loss: 98.78402102, tar-aux-loss: 98.28303027
Epoch: [27 ] train-acc: 0.91607143, dom-acc: 0.62839286, val-acc: 0.88500000, val_loss: 0.29467201
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 299.16834283, sen-loss: 25.75212291, dom-loss: 77.51699352, src-aux-loss: 98.18367290, tar-aux-loss: 97.71555400
Epoch: [28 ] train-acc: 0.91821429, dom-acc: 0.62098214, val-acc: 0.88000000, val_loss: 0.30279160
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 299.06465101, sen-loss: 25.48991659, dom-loss: 77.51051265, src-aux-loss: 97.91817105, tar-aux-loss: 98.14605033
Epoch: [29 ] train-acc: 0.92017857, dom-acc: 0.61544643, val-acc: 0.89000000, val_loss: 0.29600483
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 298.84404159, sen-loss: 25.35666214, dom-loss: 78.00652683, src-aux-loss: 97.30859572, tar-aux-loss: 98.17225647
Epoch: [30 ] train-acc: 0.92232143, dom-acc: 0.59696429, val-acc: 0.89000000, val_loss: 0.29606566
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 298.25748205, sen-loss: 24.95187637, dom-loss: 78.15868688, src-aux-loss: 97.27131873, tar-aux-loss: 97.87560177
Epoch: [31 ] train-acc: 0.92142857, dom-acc: 0.58839286, val-acc: 0.88250000, val_loss: 0.30207756
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 298.03962851, sen-loss: 24.63702426, dom-loss: 78.37571996, src-aux-loss: 96.91107929, tar-aux-loss: 98.11580646
Epoch: [32 ] train-acc: 0.92214286, dom-acc: 0.56616071, val-acc: 0.88500000, val_loss: 0.30217999
---------------------------------------------------

Successfully load model from save path: ./work/models/books_electronics_HATN.ckpt
Best Epoch: [ 27] best val accuracy: 0.00000000 best val loss: 0.29467201
Testing accuracy: 0.86200000
./work/attentions/books_electronics_train_HATN.txt
./work/attentions/books_electronics_test_HATN.txt
loading data...
source domain:  books target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 13856
vocab-size:  78006
['great', 'good', 'excellent', 'best', 'easy', 'highly', 'enjoyable', 'love', 'favorite', 'fantastic', 'enjoyed', 'brilliant', 'entertaining', 'well', 'awesome', 'classic', 'funny', 'loved', 'interesting', 'nice', 'incredible', 'useful', 'amazing', 'fascinating', 'superb', 'solid', 'valuable', 'amusing', 'readable', 'compelling', 'wonderful', 'refreshing', 'fun', 'important', 'wonderfully', 'sad', 'inspiring', 'inspirational', 'invaluable', 'essential', 'riveting', 'impressive', 'finest', 'hilarious', 'enlightening', 'liked', 'fabulous', 'authentic', 'pleasant', 'honest', 'outstanding', 'real', 'worthwhile']
['disappointing', 'boring', 'disappointed', 'poorly', 'worst', 'horrible', 'useless', 'annoying', 'repetitive', 'hard', 'confusing', 'misleading', 'better', 'difficult', 'awful', 'ridiculous', 'pathetic', 'laughable', 'unnecessary', 'mediocre', 'tedious', 'lacks', 'shallow', 'terrible', 'slow', 'simplistic', 'wasted', 'frustrating', 'flawed', 'silly', 'bad', 'waste', 'trite', 'outdated', 'uninspired', 'contrived', 'uninteresting', 'tired', 'predictable', 'sloppy', 'dissapointed', 'disjointed', 'weak', 'dangerous', 'unreadable', 'tiresome', 'insulting', 'worthless', 'trying', 'dull', 'sorry', 'choppy', 'unbelievable', 'terribly', 'impossible', 'wastes', 'sophomoric', 'absurd', 'deceived', 'expensive', 'incorrect', 'weakest', 'superficial', 'overrated', 'biased', 'inaccurate', 'hackneyed']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 17
max memory size: 20
5600 400 6000 15750 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 409.05693364, sen-loss: 76.85290730, dom-loss: 78.35469878, src-aux-loss: 133.44113141, tar-aux-loss: 120.40819710
Epoch: [1  ] train-acc: 0.66553571, dom-acc: 0.83625000, val-acc: 0.68500000, val_loss: 0.65277851
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 380.66014814, sen-loss: 70.19480938, dom-loss: 73.80044436, src-aux-loss: 124.99870861, tar-aux-loss: 111.66618282
Epoch: [2  ] train-acc: 0.75910714, dom-acc: 0.91116071, val-acc: 0.75750000, val_loss: 0.58464283
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 363.23026252, sen-loss: 62.58652833, dom-loss: 72.61966932, src-aux-loss: 120.16971523, tar-aux-loss: 107.85435027
Epoch: [3  ] train-acc: 0.79714286, dom-acc: 0.91196429, val-acc: 0.80500000, val_loss: 0.50503361
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 349.39539623, sen-loss: 54.46641156, dom-loss: 72.68359917, src-aux-loss: 116.68364257, tar-aux-loss: 105.56174487
Epoch: [4  ] train-acc: 0.82875000, dom-acc: 0.89937500, val-acc: 0.83750000, val_loss: 0.43023112
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 339.58564115, sen-loss: 47.75326946, dom-loss: 73.32020813, src-aux-loss: 114.22205502, tar-aux-loss: 104.29010868
Epoch: [5  ] train-acc: 0.85017857, dom-acc: 0.86017857, val-acc: 0.85250000, val_loss: 0.37589303
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 331.77390742, sen-loss: 42.54221109, dom-loss: 74.13798368, src-aux-loss: 112.17998230, tar-aux-loss: 102.91372967
Epoch: [6  ] train-acc: 0.86017857, dom-acc: 0.78937500, val-acc: 0.86250000, val_loss: 0.34817320
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 326.98544812, sen-loss: 39.19902536, dom-loss: 74.82243019, src-aux-loss: 110.98193645, tar-aux-loss: 101.98205525
Epoch: [7  ] train-acc: 0.86875000, dom-acc: 0.73089286, val-acc: 0.87000000, val_loss: 0.33522782
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 322.51944804, sen-loss: 36.75751056, dom-loss: 75.78448808, src-aux-loss: 109.51530427, tar-aux-loss: 100.46214485
Epoch: [8  ] train-acc: 0.87678571, dom-acc: 0.68785714, val-acc: 0.86500000, val_loss: 0.31929773
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 321.09698987, sen-loss: 35.33278738, dom-loss: 76.69059587, src-aux-loss: 108.44053417, tar-aux-loss: 100.63307321
Epoch: [9  ] train-acc: 0.88571429, dom-acc: 0.66705357, val-acc: 0.88250000, val_loss: 0.32037473
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 318.87493229, sen-loss: 33.59090702, dom-loss: 77.02632195, src-aux-loss: 107.84891856, tar-aux-loss: 100.40878421
Epoch: [10 ] train-acc: 0.88464286, dom-acc: 0.63803571, val-acc: 0.87000000, val_loss: 0.31282055
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 317.22986531, sen-loss: 33.01429224, dom-loss: 78.08726484, src-aux-loss: 107.18924725, tar-aux-loss: 98.93906087
Epoch: [11 ] train-acc: 0.89285714, dom-acc: 0.62330357, val-acc: 0.88250000, val_loss: 0.31417790
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 316.05474782, sen-loss: 32.02986524, dom-loss: 77.83057266, src-aux-loss: 106.18769282, tar-aux-loss: 100.00661647
Epoch: [12 ] train-acc: 0.89625000, dom-acc: 0.62107143, val-acc: 0.88000000, val_loss: 0.30871254
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 315.97815275, sen-loss: 31.41787867, dom-loss: 78.92689466, src-aux-loss: 105.67424750, tar-aux-loss: 99.95913166
Epoch: [13 ] train-acc: 0.88982143, dom-acc: 0.59517857, val-acc: 0.87000000, val_loss: 0.31179807
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 312.01463795, sen-loss: 30.76301096, dom-loss: 78.88223141, src-aux-loss: 105.13050383, tar-aux-loss: 97.23889035
Epoch: [14 ] train-acc: 0.89803571, dom-acc: 0.59758929, val-acc: 0.87500000, val_loss: 0.30561811
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 311.56101894, sen-loss: 30.33572482, dom-loss: 79.07153571, src-aux-loss: 104.65749007, tar-aux-loss: 97.49626750
Epoch: [15 ] train-acc: 0.90071429, dom-acc: 0.59616071, val-acc: 0.88500000, val_loss: 0.30209467
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 310.17193818, sen-loss: 29.66637121, dom-loss: 78.92033756, src-aux-loss: 104.07741421, tar-aux-loss: 97.50781548
Epoch: [16 ] train-acc: 0.90410714, dom-acc: 0.60330357, val-acc: 0.89000000, val_loss: 0.30472454
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 309.31423664, sen-loss: 29.46780878, dom-loss: 78.51735562, src-aux-loss: 103.64443278, tar-aux-loss: 97.68463981
Epoch: [17 ] train-acc: 0.90553571, dom-acc: 0.60857143, val-acc: 0.88750000, val_loss: 0.30274266
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 307.40490437, sen-loss: 28.82688604, dom-loss: 78.34995669, src-aux-loss: 103.12615716, tar-aux-loss: 97.10190570
Epoch: [18 ] train-acc: 0.90642857, dom-acc: 0.61866071, val-acc: 0.89000000, val_loss: 0.30268648
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 307.10254407, sen-loss: 28.50300603, dom-loss: 78.20361954, src-aux-loss: 102.65273649, tar-aux-loss: 97.74318111
Epoch: [19 ] train-acc: 0.90607143, dom-acc: 0.61946429, val-acc: 0.88250000, val_loss: 0.30052412
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 305.13399887, sen-loss: 28.21620762, dom-loss: 77.89791334, src-aux-loss: 102.21714664, tar-aux-loss: 96.80273104
Epoch: [20 ] train-acc: 0.90839286, dom-acc: 0.63178571, val-acc: 0.89000000, val_loss: 0.30091116
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 303.19808578, sen-loss: 27.77963388, dom-loss: 77.37423527, src-aux-loss: 101.85021263, tar-aux-loss: 96.19400340
Epoch: [21 ] train-acc: 0.90767857, dom-acc: 0.63678571, val-acc: 0.88500000, val_loss: 0.30027753
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 303.79582119, sen-loss: 27.52008943, dom-loss: 77.33845252, src-aux-loss: 101.54695612, tar-aux-loss: 97.39032507
Epoch: [22 ] train-acc: 0.91160714, dom-acc: 0.64839286, val-acc: 0.89000000, val_loss: 0.30100247
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 301.71152711, sen-loss: 27.32846070, dom-loss: 76.75922787, src-aux-loss: 101.20999116, tar-aux-loss: 96.41384572
Epoch: [23 ] train-acc: 0.91232143, dom-acc: 0.65178571, val-acc: 0.89000000, val_loss: 0.29813635
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 300.02491879, sen-loss: 26.85411759, dom-loss: 76.61438203, src-aux-loss: 100.75084668, tar-aux-loss: 95.80557281
Epoch: [24 ] train-acc: 0.91375000, dom-acc: 0.65732143, val-acc: 0.89250000, val_loss: 0.29732165
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 301.05577111, sen-loss: 26.67652227, dom-loss: 76.74725676, src-aux-loss: 100.30399024, tar-aux-loss: 97.32800144
Epoch: [25 ] train-acc: 0.91339286, dom-acc: 0.65508929, val-acc: 0.88750000, val_loss: 0.29860094
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 299.13977456, sen-loss: 26.31126708, dom-loss: 76.78295559, src-aux-loss: 99.99454498, tar-aux-loss: 96.05100751
Epoch: [26 ] train-acc: 0.91446429, dom-acc: 0.65866071, val-acc: 0.89000000, val_loss: 0.29866180
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 298.20882773, sen-loss: 26.13357400, dom-loss: 76.52436370, src-aux-loss: 99.92767960, tar-aux-loss: 95.62320822
Epoch: [27 ] train-acc: 0.91464286, dom-acc: 0.65330357, val-acc: 0.89250000, val_loss: 0.29701507
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 297.87601042, sen-loss: 25.82244360, dom-loss: 77.00917011, src-aux-loss: 99.29953712, tar-aux-loss: 95.74485934
Epoch: [28 ] train-acc: 0.91821429, dom-acc: 0.65526786, val-acc: 0.89250000, val_loss: 0.30134866
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 296.74614882, sen-loss: 25.37991535, dom-loss: 76.84169954, src-aux-loss: 98.86526889, tar-aux-loss: 95.65926558
Epoch: [29 ] train-acc: 0.91803571, dom-acc: 0.64517857, val-acc: 0.89250000, val_loss: 0.29868212
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 295.95060396, sen-loss: 25.33467977, dom-loss: 77.19859236, src-aux-loss: 98.56164336, tar-aux-loss: 94.85568887
Epoch: [30 ] train-acc: 0.91928571, dom-acc: 0.64133929, val-acc: 0.89000000, val_loss: 0.29699859
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 295.87506914, sen-loss: 24.88047225, dom-loss: 77.48158944, src-aux-loss: 98.24345380, tar-aux-loss: 95.26955420
Epoch: [31 ] train-acc: 0.92160714, dom-acc: 0.63330357, val-acc: 0.89500000, val_loss: 0.30249447
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 295.70912981, sen-loss: 24.64732555, dom-loss: 77.59702766, src-aux-loss: 97.90644640, tar-aux-loss: 95.55833054
Epoch: [32 ] train-acc: 0.92160714, dom-acc: 0.62071429, val-acc: 0.88500000, val_loss: 0.30748436
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 294.43154383, sen-loss: 24.57259931, dom-loss: 77.76887476, src-aux-loss: 97.53872210, tar-aux-loss: 94.55134761
Epoch: [33 ] train-acc: 0.92214286, dom-acc: 0.61401786, val-acc: 0.89250000, val_loss: 0.29598153
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 295.30680585, sen-loss: 24.14278513, dom-loss: 78.05586761, src-aux-loss: 97.40358502, tar-aux-loss: 95.70456827
Epoch: [34 ] train-acc: 0.92267857, dom-acc: 0.60473214, val-acc: 0.89500000, val_loss: 0.29756460
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 292.96872306, sen-loss: 24.03188755, dom-loss: 78.13196027, src-aux-loss: 96.84262568, tar-aux-loss: 93.96224779
Epoch: [35 ] train-acc: 0.92517857, dom-acc: 0.59508929, val-acc: 0.89250000, val_loss: 0.29628181
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 292.26871467, sen-loss: 23.57571766, dom-loss: 78.33713174, src-aux-loss: 96.16150588, tar-aux-loss: 94.19436008
Epoch: [36 ] train-acc: 0.92750000, dom-acc: 0.58964286, val-acc: 0.90000000, val_loss: 0.30165389
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 292.87460542, sen-loss: 23.45788576, dom-loss: 78.38421631, src-aux-loss: 96.07702243, tar-aux-loss: 94.95548087
Epoch: [37 ] train-acc: 0.92732143, dom-acc: 0.58455357, val-acc: 0.89500000, val_loss: 0.29741681
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 291.21414495, sen-loss: 23.09926932, dom-loss: 78.43950772, src-aux-loss: 95.84707648, tar-aux-loss: 93.82829171
Epoch: [38 ] train-acc: 0.92875000, dom-acc: 0.58321429, val-acc: 0.89250000, val_loss: 0.30254641
---------------------------------------------------

Successfully load model from save path: ./work/models/books_kitchen_HATN.ckpt
Best Epoch: [ 33] best val accuracy: 0.00000000 best val loss: 0.29598153
Testing accuracy: 0.87083333
./work/attentions/books_kitchen_train_HATN.txt
./work/attentions/books_kitchen_test_HATN.txt
loading data...
source domain:  books target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  9750 30180
vocab-size:  98084
['great', 'good', 'excellent', 'best', 'highly', 'easy', 'enjoyable', 'favorite', 'love', 'funny', 'fantastic', 'brilliant', 'awesome', 'enjoyed', 'classic', 'entertaining', 'well', 'loved', 'interesting', 'inspiring', 'fascinating', 'incredible', 'amazing', 'refreshing', 'solid', 'valuable', 'nice', 'superb', 'compelling', 'hilarious', 'inspirational', 'useful', 'important', 'readable', 'fun', 'invaluable', 'essential', 'impressive', 'wonderfully', 'authentic', 'honest', 'sad', 'enlightening', 'liked', 'simple', 'riveting', 'pleasant', 'loves', 'beautifully', 'greatest', 'real', 'provoking', 'humorous', 'deserves', 'heartwarming', 'emotional', 'believable']
['disappointing', 'boring', 'poorly', 'disappointed', 'horrible', 'worst', 'useless', 'annoying', 'repetitive', 'hard', 'confusing', 'difficult', 'lacks', 'better', 'misleading', 'tedious', 'awful', 'pathetic', 'laughable', 'unnecessary', 'ridiculous', 'simplistic', 'flawed', 'mediocre', 'tired', 'shallow', 'slow', 'frustrating', 'amusing', 'uninteresting', 'terrible', 'wasted', 'outdated', 'uninspired', 'silly', 'waste', 'trite', 'sloppy', 'contrived', 'predictable', 'dissapointed', 'bad', 'disjointed', 'choppy', 'unreadable', 'wrong', 'lame', 'tiresome', 'impossible', 'wastes', 'false', 'insulting', 'worthless', 'trying', 'dangerous', 'sorry', 'clumsy', 'terribly', 'sophomoric', 'absurd', 'rambling', 'deceived', 'weak', 'expensive', 'unlikeable', 'unfortunate', 'weakest', 'superficial', 'repetitious', 'overrated', 'biased', 'inaccurate', 'hackneyed', 'wasting']
max  story size: 189
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
5600 400 6000 15750 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 405.90112567, sen-loss: 76.73069715, dom-loss: 79.41594672, src-aux-loss: 131.16805226, tar-aux-loss: 118.58642864
Epoch: [1  ] train-acc: 0.69142857, dom-acc: 0.65553571, val-acc: 0.69500000, val_loss: 0.64996397
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 378.39426088, sen-loss: 69.82193625, dom-loss: 76.65946662, src-aux-loss: 122.72987270, tar-aux-loss: 109.18298656
Epoch: [2  ] train-acc: 0.75000000, dom-acc: 0.70946429, val-acc: 0.76250000, val_loss: 0.58127093
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 361.49222946, sen-loss: 62.02983010, dom-loss: 75.87968588, src-aux-loss: 117.79486424, tar-aux-loss: 105.78784728
Epoch: [3  ] train-acc: 0.80089286, dom-acc: 0.70241071, val-acc: 0.81250000, val_loss: 0.49884915
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 347.18865037, sen-loss: 54.04012504, dom-loss: 75.45760047, src-aux-loss: 114.41184956, tar-aux-loss: 103.27907497
Epoch: [4  ] train-acc: 0.83017857, dom-acc: 0.69410714, val-acc: 0.84250000, val_loss: 0.42586046
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 335.11676002, sen-loss: 47.12918085, dom-loss: 75.41269386, src-aux-loss: 112.13213009, tar-aux-loss: 100.44275397
Epoch: [5  ] train-acc: 0.85125000, dom-acc: 0.71500000, val-acc: 0.84500000, val_loss: 0.37084982
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 326.83093190, sen-loss: 42.10478723, dom-loss: 75.34919649, src-aux-loss: 109.81497037, tar-aux-loss: 99.56197530
Epoch: [6  ] train-acc: 0.85857143, dom-acc: 0.75964286, val-acc: 0.86000000, val_loss: 0.35030520
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 320.98313689, sen-loss: 39.14465322, dom-loss: 75.18722528, src-aux-loss: 108.63437039, tar-aux-loss: 98.01689011
Epoch: [7  ] train-acc: 0.86946429, dom-acc: 0.76116071, val-acc: 0.87500000, val_loss: 0.32998177
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 316.65677571, sen-loss: 36.83207814, dom-loss: 75.22840035, src-aux-loss: 107.05727720, tar-aux-loss: 97.53902125
Epoch: [8  ] train-acc: 0.87892857, dom-acc: 0.73991071, val-acc: 0.86750000, val_loss: 0.31534874
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 313.70842409, sen-loss: 35.46112095, dom-loss: 75.33817399, src-aux-loss: 106.02297080, tar-aux-loss: 96.88615680
Epoch: [9  ] train-acc: 0.87875000, dom-acc: 0.72669643, val-acc: 0.88250000, val_loss: 0.32227394
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 311.96177435, sen-loss: 33.76032883, dom-loss: 75.42742991, src-aux-loss: 105.24173844, tar-aux-loss: 97.53227603
Epoch: [10 ] train-acc: 0.88250000, dom-acc: 0.70589286, val-acc: 0.86250000, val_loss: 0.30850303
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 308.07755375, sen-loss: 33.05458674, dom-loss: 75.69376600, src-aux-loss: 104.82190967, tar-aux-loss: 94.50729138
Epoch: [11 ] train-acc: 0.89071429, dom-acc: 0.70839286, val-acc: 0.88250000, val_loss: 0.31012022
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 308.19242406, sen-loss: 32.05016999, dom-loss: 75.63464099, src-aux-loss: 103.70978546, tar-aux-loss: 96.79782742
Epoch: [12 ] train-acc: 0.89660714, dom-acc: 0.70482143, val-acc: 0.89000000, val_loss: 0.30247292
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 305.96221709, sen-loss: 31.50539917, dom-loss: 75.98958403, src-aux-loss: 103.18400842, tar-aux-loss: 95.28322619
Epoch: [13 ] train-acc: 0.89303571, dom-acc: 0.68946429, val-acc: 0.86500000, val_loss: 0.30451098
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 303.22564363, sen-loss: 30.81596965, dom-loss: 76.02326423, src-aux-loss: 102.70062220, tar-aux-loss: 93.68578839
Epoch: [14 ] train-acc: 0.89750000, dom-acc: 0.69348214, val-acc: 0.87500000, val_loss: 0.30131111
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 303.16492176, sen-loss: 30.38343708, dom-loss: 76.09329611, src-aux-loss: 102.16381830, tar-aux-loss: 94.52437145
Epoch: [15 ] train-acc: 0.90107143, dom-acc: 0.69214286, val-acc: 0.88500000, val_loss: 0.29907116
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 301.04926538, sen-loss: 29.73318103, dom-loss: 76.09080511, src-aux-loss: 101.70804310, tar-aux-loss: 93.51723599
Epoch: [16 ] train-acc: 0.90446429, dom-acc: 0.68473214, val-acc: 0.88250000, val_loss: 0.29972497
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 301.80141401, sen-loss: 29.57452722, dom-loss: 76.42011911, src-aux-loss: 101.19250160, tar-aux-loss: 94.61426759
Epoch: [17 ] train-acc: 0.90535714, dom-acc: 0.68303571, val-acc: 0.88750000, val_loss: 0.30034438
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 300.21622157, sen-loss: 28.86850959, dom-loss: 76.40900505, src-aux-loss: 100.70332038, tar-aux-loss: 94.23538536
Epoch: [18 ] train-acc: 0.90678571, dom-acc: 0.68366071, val-acc: 0.89000000, val_loss: 0.29901606
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 298.29582548, sen-loss: 28.59389028, dom-loss: 76.69070363, src-aux-loss: 100.08804142, tar-aux-loss: 92.92319161
Epoch: [19 ] train-acc: 0.90785714, dom-acc: 0.67196429, val-acc: 0.89250000, val_loss: 0.29616007
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 298.69574881, sen-loss: 28.25075468, dom-loss: 76.90703762, src-aux-loss: 99.65147656, tar-aux-loss: 93.88648069
Epoch: [20 ] train-acc: 0.91053571, dom-acc: 0.67437500, val-acc: 0.88750000, val_loss: 0.29641920
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 296.49645925, sen-loss: 27.78968388, dom-loss: 76.80379719, src-aux-loss: 99.34724945, tar-aux-loss: 92.55572647
Epoch: [21 ] train-acc: 0.90678571, dom-acc: 0.66151786, val-acc: 0.87750000, val_loss: 0.29539728
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 296.28835607, sen-loss: 27.54640129, dom-loss: 77.04183781, src-aux-loss: 99.07901978, tar-aux-loss: 92.62109822
Epoch: [22 ] train-acc: 0.91250000, dom-acc: 0.66339286, val-acc: 0.89000000, val_loss: 0.29674500
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 295.31527352, sen-loss: 27.33204557, dom-loss: 76.78876346, src-aux-loss: 98.62286323, tar-aux-loss: 92.57160133
Epoch: [23 ] train-acc: 0.91267857, dom-acc: 0.65491071, val-acc: 0.88750000, val_loss: 0.29411107
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 294.64339089, sen-loss: 26.91013811, dom-loss: 77.15152419, src-aux-loss: 98.22748661, tar-aux-loss: 92.35424107
Epoch: [24 ] train-acc: 0.91482143, dom-acc: 0.66401786, val-acc: 0.89000000, val_loss: 0.29605699
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 295.43399811, sen-loss: 26.75253086, dom-loss: 77.27475899, src-aux-loss: 97.96119857, tar-aux-loss: 93.44551080
Epoch: [25 ] train-acc: 0.91232143, dom-acc: 0.63580357, val-acc: 0.89250000, val_loss: 0.29439038
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 293.32492352, sen-loss: 26.30814003, dom-loss: 77.25105107, src-aux-loss: 97.32521731, tar-aux-loss: 92.44051379
Epoch: [26 ] train-acc: 0.91500000, dom-acc: 0.64535714, val-acc: 0.89250000, val_loss: 0.29541969
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 292.17379928, sen-loss: 26.08412344, dom-loss: 77.35506141, src-aux-loss: 97.23844552, tar-aux-loss: 91.49616599
Epoch: [27 ] train-acc: 0.91589286, dom-acc: 0.64616071, val-acc: 0.89000000, val_loss: 0.29356655
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 290.05870318, sen-loss: 25.77654666, dom-loss: 77.35428190, src-aux-loss: 96.72337943, tar-aux-loss: 90.20449507
Epoch: [28 ] train-acc: 0.91803571, dom-acc: 0.65321429, val-acc: 0.88750000, val_loss: 0.29660267
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 290.71385980, sen-loss: 25.47034798, dom-loss: 77.14527643, src-aux-loss: 96.19853020, tar-aux-loss: 91.89970481
Epoch: [29 ] train-acc: 0.91910714, dom-acc: 0.63955357, val-acc: 0.89250000, val_loss: 0.29420215
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 290.00047851, sen-loss: 25.35334964, dom-loss: 77.44760841, src-aux-loss: 95.77853554, tar-aux-loss: 91.42098510
Epoch: [30 ] train-acc: 0.92000000, dom-acc: 0.64571429, val-acc: 0.88750000, val_loss: 0.29521531
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 290.17010665, sen-loss: 24.93570482, dom-loss: 77.54325181, src-aux-loss: 95.63017201, tar-aux-loss: 92.06097949
Epoch: [31 ] train-acc: 0.92071429, dom-acc: 0.65232143, val-acc: 0.88750000, val_loss: 0.29818004
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 288.90930200, sen-loss: 24.69125596, dom-loss: 77.53393006, src-aux-loss: 95.26546842, tar-aux-loss: 91.41864771
Epoch: [32 ] train-acc: 0.92285714, dom-acc: 0.65580357, val-acc: 0.88000000, val_loss: 0.30205691
---------------------------------------------------

Successfully load model from save path: ./work/models/books_video_HATN.ckpt
Best Epoch: [ 27] best val accuracy: 0.00000000 best val loss: 0.29356655
Testing accuracy: 0.87350000
./work/attentions/books_video_train_HATN.txt
./work/attentions/books_video_test_HATN.txt
loading data...
source domain:  dvd target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 9750
vocab-size:  100530
['best', 'good', 'excellent', 'great', 'enjoyable', 'funny', 'entertaining', 'nice', 'fantastic', 'awesome', 'classic', 'love', 'funniest', 'hilarious', 'wonderful', 'loved', 'amazing', 'underrated', 'favorite', 'perfect', 'sad', 'brilliant', 'interesting', 'finest', 'superb', 'easy', 'real', 'solid', 'greatest', 'recommend', 'outstanding', 'beautifully', 'amusing', 'terrific', 'beautiful', 'fine', 'enjoy', 'pleasant', 'fascinating', 'liked', 'worthy', 'memorable', 'incredible']
['worst', 'boring', 'horrible', 'bad', 'disappointing', 'poor', 'terrible', 'awful', 'disappointed', 'dull', 'wasted', 'waste', 'annoying', 'better', 'laughable', 'unfunny', 'forgettable', 'sucks', 'lousy', 'predictable', 'stupid', 'unwatchable', 'uninspired', 'pathetic', 'poorly', 'contrived', 'slow', 'ok', 'pointless', 'sorry', 'unoriginal', 'decent', 'disgusting', 'wrong', 'sucked', 'dissapointed', 'worse', 'biased', 'lame', 'horrendous', 'overrated', 'atrocious', 'depressing', 'disjointed', 'crappy', 'mediocre', 'irritating', 'dumbest', 'okay', 'disapointed', 'ridiculous', 'silly', 'useless', 'weak', 'stupidest', 'frustrating', 'wasting', 'ruined']
max  story size: 226
mean story size: 8
max  sentence size: 783
mean sentence size: 19
max memory size: 20
5600 400 6000 17843 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(100531, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 406.00254393, sen-loss: 77.27335972, dom-loss: 79.34773290, src-aux-loss: 128.30627370, tar-aux-loss: 121.07517886
Epoch: [1  ] train-acc: 0.64732143, dom-acc: 0.70687500, val-acc: 0.67750000, val_loss: 0.65747088
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 379.03942370, sen-loss: 71.51658148, dom-loss: 76.48869562, src-aux-loss: 119.23424178, tar-aux-loss: 111.79990542
Epoch: [2  ] train-acc: 0.75035714, dom-acc: 0.80491071, val-acc: 0.75500000, val_loss: 0.59555179
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 361.68037271, sen-loss: 65.05456370, dom-loss: 75.47669846, src-aux-loss: 114.13564628, tar-aux-loss: 107.01346773
Epoch: [3  ] train-acc: 0.76910714, dom-acc: 0.82535714, val-acc: 0.76000000, val_loss: 0.53009421
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 346.69184208, sen-loss: 57.72135794, dom-loss: 74.82478869, src-aux-loss: 110.43068677, tar-aux-loss: 103.71501136
Epoch: [4  ] train-acc: 0.81785714, dom-acc: 0.82053571, val-acc: 0.80000000, val_loss: 0.45596036
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 335.27715182, sen-loss: 50.80190194, dom-loss: 74.60256952, src-aux-loss: 107.14692259, tar-aux-loss: 102.72575688
Epoch: [5  ] train-acc: 0.82089286, dom-acc: 0.79339286, val-acc: 0.82500000, val_loss: 0.41720256
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 327.65877128, sen-loss: 45.37661105, dom-loss: 74.28179997, src-aux-loss: 105.41524523, tar-aux-loss: 102.58511496
Epoch: [6  ] train-acc: 0.85535714, dom-acc: 0.76428571, val-acc: 0.88000000, val_loss: 0.34733984
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 318.05895543, sen-loss: 40.73418914, dom-loss: 74.50453585, src-aux-loss: 103.26446319, tar-aux-loss: 99.55576378
Epoch: [7  ] train-acc: 0.87035714, dom-acc: 0.75964286, val-acc: 0.89000000, val_loss: 0.32293606
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 313.47112370, sen-loss: 38.09258491, dom-loss: 74.54784918, src-aux-loss: 102.27045834, tar-aux-loss: 98.56023204
Epoch: [8  ] train-acc: 0.87607143, dom-acc: 0.74937500, val-acc: 0.88750000, val_loss: 0.31227362
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 312.62934566, sen-loss: 36.84713456, dom-loss: 74.63540685, src-aux-loss: 100.86737484, tar-aux-loss: 100.27943009
Epoch: [9  ] train-acc: 0.88089286, dom-acc: 0.74526786, val-acc: 0.89000000, val_loss: 0.30447903
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 309.39809012, sen-loss: 35.49823225, dom-loss: 74.74489826, src-aux-loss: 100.21704471, tar-aux-loss: 98.93791366
Epoch: [10 ] train-acc: 0.88214286, dom-acc: 0.74071429, val-acc: 0.89500000, val_loss: 0.29776713
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 309.03945899, sen-loss: 34.93117665, dom-loss: 75.02963603, src-aux-loss: 99.66780031, tar-aux-loss: 99.41084737
Epoch: [11 ] train-acc: 0.88375000, dom-acc: 0.73544643, val-acc: 0.89500000, val_loss: 0.30634221
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 304.97958875, sen-loss: 34.17294446, dom-loss: 75.20741695, src-aux-loss: 98.80111420, tar-aux-loss: 96.79811263
Epoch: [12 ] train-acc: 0.88750000, dom-acc: 0.73357143, val-acc: 0.89750000, val_loss: 0.28908914
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 303.45582485, sen-loss: 33.50725364, dom-loss: 75.16659087, src-aux-loss: 97.93117213, tar-aux-loss: 96.85080779
Epoch: [13 ] train-acc: 0.89160714, dom-acc: 0.72848214, val-acc: 0.88750000, val_loss: 0.29177454
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 302.19175267, sen-loss: 32.77044138, dom-loss: 75.46594417, src-aux-loss: 97.30514210, tar-aux-loss: 96.65022331
Epoch: [14 ] train-acc: 0.89053571, dom-acc: 0.72294643, val-acc: 0.89500000, val_loss: 0.29299420
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 301.57888579, sen-loss: 32.21049047, dom-loss: 75.59177446, src-aux-loss: 96.78672713, tar-aux-loss: 96.98989230
Epoch: [15 ] train-acc: 0.89464286, dom-acc: 0.72232143, val-acc: 0.89250000, val_loss: 0.28581899
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 301.20406175, sen-loss: 31.77671748, dom-loss: 76.00200510, src-aux-loss: 96.07060975, tar-aux-loss: 97.35473102
Epoch: [16 ] train-acc: 0.89607143, dom-acc: 0.71660714, val-acc: 0.89750000, val_loss: 0.28337249
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 300.14220452, sen-loss: 31.35833971, dom-loss: 76.00245678, src-aux-loss: 95.55938196, tar-aux-loss: 97.22202557
Epoch: [17 ] train-acc: 0.89339286, dom-acc: 0.71151786, val-acc: 0.89250000, val_loss: 0.28999844
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 297.68262410, sen-loss: 30.96731617, dom-loss: 76.19391966, src-aux-loss: 95.06663334, tar-aux-loss: 95.45475465
Epoch: [18 ] train-acc: 0.89910714, dom-acc: 0.70705357, val-acc: 0.89000000, val_loss: 0.27802253
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 296.66693330, sen-loss: 30.64157359, dom-loss: 76.21517611, src-aux-loss: 94.54890078, tar-aux-loss: 95.26128173
Epoch: [19 ] train-acc: 0.90000000, dom-acc: 0.70178571, val-acc: 0.89000000, val_loss: 0.27463222
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 297.64833975, sen-loss: 30.33921552, dom-loss: 76.68268764, src-aux-loss: 94.19060135, tar-aux-loss: 96.43583494
Epoch: [20 ] train-acc: 0.90107143, dom-acc: 0.70098214, val-acc: 0.89250000, val_loss: 0.27395549
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 295.01580524, sen-loss: 29.89764141, dom-loss: 76.38258779, src-aux-loss: 93.72175479, tar-aux-loss: 95.01382059
Epoch: [21 ] train-acc: 0.89946429, dom-acc: 0.69741071, val-acc: 0.90000000, val_loss: 0.27701557
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 294.86954093, sen-loss: 29.56665360, dom-loss: 76.80162460, src-aux-loss: 93.39276177, tar-aux-loss: 95.10849929
Epoch: [22 ] train-acc: 0.89821429, dom-acc: 0.69607143, val-acc: 0.89750000, val_loss: 0.28867239
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 294.24231124, sen-loss: 29.45620821, dom-loss: 76.68298215, src-aux-loss: 92.82784152, tar-aux-loss: 95.27527905
Epoch: [23 ] train-acc: 0.90196429, dom-acc: 0.68946429, val-acc: 0.89750000, val_loss: 0.27340981
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 294.33807492, sen-loss: 29.05501379, dom-loss: 76.98791951, src-aux-loss: 92.46165758, tar-aux-loss: 95.83348262
Epoch: [24 ] train-acc: 0.90535714, dom-acc: 0.68830357, val-acc: 0.89500000, val_loss: 0.26903132
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 291.14554739, sen-loss: 28.93134373, dom-loss: 76.98939532, src-aux-loss: 92.04125994, tar-aux-loss: 93.18354887
Epoch: [25 ] train-acc: 0.90571429, dom-acc: 0.68410714, val-acc: 0.89250000, val_loss: 0.26819319
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 293.42780614, sen-loss: 28.44979327, dom-loss: 77.12244570, src-aux-loss: 91.59827143, tar-aux-loss: 96.25729656
Epoch: [26 ] train-acc: 0.90357143, dom-acc: 0.68544643, val-acc: 0.89750000, val_loss: 0.27264166
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 290.83974123, sen-loss: 28.30514238, dom-loss: 76.87930673, src-aux-loss: 91.15651530, tar-aux-loss: 94.49877751
Epoch: [27 ] train-acc: 0.90589286, dom-acc: 0.68508929, val-acc: 0.90000000, val_loss: 0.27124205
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 290.85586262, sen-loss: 27.93125145, dom-loss: 77.45376712, src-aux-loss: 90.82405746, tar-aux-loss: 94.64678657
Epoch: [28 ] train-acc: 0.90839286, dom-acc: 0.67419643, val-acc: 0.90000000, val_loss: 0.26301673
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 289.73605251, sen-loss: 27.90660137, dom-loss: 77.38217312, src-aux-loss: 90.53161871, tar-aux-loss: 93.91565859
Epoch: [29 ] train-acc: 0.90767857, dom-acc: 0.67098214, val-acc: 0.89250000, val_loss: 0.26098600
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 289.23327351, sen-loss: 27.58921537, dom-loss: 77.06326538, src-aux-loss: 89.95234519, tar-aux-loss: 94.62844801
Epoch: [30 ] train-acc: 0.90625000, dom-acc: 0.67580357, val-acc: 0.89500000, val_loss: 0.27317920
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 288.51372147, sen-loss: 27.17544802, dom-loss: 77.43634909, src-aux-loss: 89.69273698, tar-aux-loss: 94.20918554
Epoch: [31 ] train-acc: 0.91053571, dom-acc: 0.67258929, val-acc: 0.89750000, val_loss: 0.26440802
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 286.88937807, sen-loss: 26.93652135, dom-loss: 77.44680864, src-aux-loss: 89.06227827, tar-aux-loss: 93.44377077
Epoch: [32 ] train-acc: 0.91017857, dom-acc: 0.67348214, val-acc: 0.90000000, val_loss: 0.26945469
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 286.80171323, sen-loss: 26.62378172, dom-loss: 77.26512718, src-aux-loss: 88.80876511, tar-aux-loss: 94.10403925
Epoch: [33 ] train-acc: 0.89910714, dom-acc: 0.68303571, val-acc: 0.88750000, val_loss: 0.30078608
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 286.48213148, sen-loss: 26.77445916, dom-loss: 77.51861560, src-aux-loss: 88.37224299, tar-aux-loss: 93.81681389
Epoch: [34 ] train-acc: 0.91500000, dom-acc: 0.67044643, val-acc: 0.89500000, val_loss: 0.26108661
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_books_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26098600
Testing accuracy: 0.88016667
./work/attentions/dvd_books_train_HATN.txt
./work/attentions/dvd_books_test_HATN.txt
loading data...
source domain:  dvd target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 17009
vocab-size:  85442
['good', 'best', 'excellent', 'great', 'funny', 'enjoyable', 'nice', 'entertaining', 'classic', 'awesome', 'fantastic', 'funniest', 'hilarious', 'love', 'perfect', 'underrated', 'better', 'loved', 'amazing', 'sad', 'favorite', 'brilliant', 'interesting', 'superb', 'finest', 'wonderful', 'recommend', 'solid', 'easy', 'beautifully', 'fine', 'real', 'outstanding', 'enjoy', 'terrific', 'amusing', 'happy', 'beautiful', 'fascinating', 'sweet', 'well', 'likeable', 'worthy', 'incredible']
['worst', 'boring', 'horrible', 'bad', 'disappointing', 'poor', 'awful', 'terrible', 'disappointed', 'dull', 'wasted', 'annoying', 'waste', 'decent', 'forgettable', 'sucks', 'lousy', 'stupid', 'worse', 'laughable', 'predictable', 'unwatchable', 'unfunny', 'uninspired', 'pathetic', 'ok', 'poorly', 'wrong', 'contrived', 'slow', 'dissapointed', 'disgusting', 'unoriginal', 'crappy', 'pointless', 'sorry', 'atrocious', 'mediocre', 'lame', 'sucked', 'overrated', 'biased', 'defective', 'disapointed', 'depressing', 'ridiculous', 'horrendous', 'irritating', 'dumbest', 'okay', 'silly', 'useless', 'cheap', 'weak', 'stupidest', 'unconvincing', 'frustrating', 'wasting', 'ruined']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 18
max memory size: 20
5600 400 6000 17843 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 393.53175783, sen-loss: 77.37788785, dom-loss: 78.50667113, src-aux-loss: 124.51940572, tar-aux-loss: 113.12779093
Epoch: [1  ] train-acc: 0.64625000, dom-acc: 0.82500000, val-acc: 0.69500000, val_loss: 0.65855527
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 366.34877729, sen-loss: 71.88917047, dom-loss: 74.77004999, src-aux-loss: 116.00831562, tar-aux-loss: 103.68124080
Epoch: [2  ] train-acc: 0.75553571, dom-acc: 0.87848214, val-acc: 0.75500000, val_loss: 0.59928417
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 348.51827216, sen-loss: 65.54432809, dom-loss: 73.62303829, src-aux-loss: 111.56774163, tar-aux-loss: 97.78316581
Epoch: [3  ] train-acc: 0.77875000, dom-acc: 0.87178571, val-acc: 0.77250000, val_loss: 0.53019994
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 336.52212787, sen-loss: 57.92792204, dom-loss: 73.80545628, src-aux-loss: 107.83711994, tar-aux-loss: 96.95163012
Epoch: [4  ] train-acc: 0.81767857, dom-acc: 0.85375000, val-acc: 0.81500000, val_loss: 0.45140609
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 324.43361592, sen-loss: 50.64009649, dom-loss: 74.25869280, src-aux-loss: 104.25291061, tar-aux-loss: 95.28191543
Epoch: [5  ] train-acc: 0.81714286, dom-acc: 0.82526786, val-acc: 0.82500000, val_loss: 0.41371673
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 315.20802402, sen-loss: 44.69564456, dom-loss: 74.88912994, src-aux-loss: 102.11879456, tar-aux-loss: 93.50445545
Epoch: [6  ] train-acc: 0.85625000, dom-acc: 0.76500000, val-acc: 0.88250000, val_loss: 0.33724955
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 309.20162559, sen-loss: 40.28536841, dom-loss: 75.76454014, src-aux-loss: 100.10032415, tar-aux-loss: 93.05139166
Epoch: [7  ] train-acc: 0.87035714, dom-acc: 0.71937500, val-acc: 0.89000000, val_loss: 0.31275359
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 304.50759077, sen-loss: 37.77557021, dom-loss: 76.38220274, src-aux-loss: 99.13538784, tar-aux-loss: 91.21443051
Epoch: [8  ] train-acc: 0.87696429, dom-acc: 0.68375000, val-acc: 0.89750000, val_loss: 0.30340904
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 303.38719654, sen-loss: 36.41547334, dom-loss: 77.05460948, src-aux-loss: 97.67992085, tar-aux-loss: 92.23719209
Epoch: [9  ] train-acc: 0.88232143, dom-acc: 0.65910714, val-acc: 0.90000000, val_loss: 0.29668894
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 301.38599801, sen-loss: 35.08644389, dom-loss: 77.77949083, src-aux-loss: 96.85378057, tar-aux-loss: 91.66628271
Epoch: [10 ] train-acc: 0.88089286, dom-acc: 0.62339286, val-acc: 0.90000000, val_loss: 0.29199445
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 300.00773954, sen-loss: 34.31988057, dom-loss: 78.33898842, src-aux-loss: 96.03415179, tar-aux-loss: 91.31471884
Epoch: [11 ] train-acc: 0.88732143, dom-acc: 0.61339286, val-acc: 0.90000000, val_loss: 0.28840446
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 296.97925925, sen-loss: 33.68266350, dom-loss: 78.55578595, src-aux-loss: 95.18703705, tar-aux-loss: 89.55377287
Epoch: [12 ] train-acc: 0.88982143, dom-acc: 0.59187500, val-acc: 0.89750000, val_loss: 0.28589404
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 295.98550010, sen-loss: 33.12621268, dom-loss: 78.71330398, src-aux-loss: 94.50307637, tar-aux-loss: 89.64290696
Epoch: [13 ] train-acc: 0.89000000, dom-acc: 0.59312500, val-acc: 0.89250000, val_loss: 0.29073042
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 293.94790220, sen-loss: 32.36075141, dom-loss: 78.92794937, src-aux-loss: 93.79207194, tar-aux-loss: 88.86712956
Epoch: [14 ] train-acc: 0.89142857, dom-acc: 0.58232143, val-acc: 0.89000000, val_loss: 0.29211468
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 293.23995018, sen-loss: 31.98944585, dom-loss: 78.92917049, src-aux-loss: 93.35276020, tar-aux-loss: 88.96857440
Epoch: [15 ] train-acc: 0.89571429, dom-acc: 0.55330357, val-acc: 0.90250000, val_loss: 0.28083548
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 293.40043044, sen-loss: 31.44481520, dom-loss: 78.67596453, src-aux-loss: 92.77724195, tar-aux-loss: 90.50240898
Epoch: [16 ] train-acc: 0.89750000, dom-acc: 0.59062500, val-acc: 0.90000000, val_loss: 0.27852470
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 290.23986864, sen-loss: 31.00457755, dom-loss: 78.57165676, src-aux-loss: 91.96982825, tar-aux-loss: 88.69380820
Epoch: [17 ] train-acc: 0.89589286, dom-acc: 0.58116071, val-acc: 0.89250000, val_loss: 0.28261819
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 288.63613510, sen-loss: 30.66275534, dom-loss: 78.23233604, src-aux-loss: 91.71980220, tar-aux-loss: 88.02124119
Epoch: [18 ] train-acc: 0.89964286, dom-acc: 0.60714286, val-acc: 0.89750000, val_loss: 0.27433905
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 288.10530853, sen-loss: 30.35217293, dom-loss: 78.14739233, src-aux-loss: 90.83117187, tar-aux-loss: 88.77457082
Epoch: [19 ] train-acc: 0.89964286, dom-acc: 0.60785714, val-acc: 0.89750000, val_loss: 0.27244455
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 285.64743137, sen-loss: 29.86654139, dom-loss: 77.99711680, src-aux-loss: 90.41739398, tar-aux-loss: 87.36637908
Epoch: [20 ] train-acc: 0.90178571, dom-acc: 0.60723214, val-acc: 0.89500000, val_loss: 0.27122992
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 286.94102263, sen-loss: 29.52409831, dom-loss: 77.98206526, src-aux-loss: 90.00284612, tar-aux-loss: 89.43201303
Epoch: [21 ] train-acc: 0.89982143, dom-acc: 0.61366071, val-acc: 0.89500000, val_loss: 0.27548465
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 284.17614841, sen-loss: 29.22381336, dom-loss: 77.91686088, src-aux-loss: 89.59272683, tar-aux-loss: 87.44274896
Epoch: [22 ] train-acc: 0.90125000, dom-acc: 0.59267857, val-acc: 0.89250000, val_loss: 0.28373975
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 283.71900892, sen-loss: 29.12657198, dom-loss: 77.80749857, src-aux-loss: 89.16145420, tar-aux-loss: 87.62348491
Epoch: [23 ] train-acc: 0.90321429, dom-acc: 0.61017857, val-acc: 0.89500000, val_loss: 0.26919490
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 283.01535702, sen-loss: 28.76954708, dom-loss: 77.91011971, src-aux-loss: 88.64007485, tar-aux-loss: 87.69561535
Epoch: [24 ] train-acc: 0.90535714, dom-acc: 0.61571429, val-acc: 0.89500000, val_loss: 0.26555926
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 281.42886996, sen-loss: 28.54361024, dom-loss: 77.85794687, src-aux-loss: 88.22049290, tar-aux-loss: 86.80681902
Epoch: [25 ] train-acc: 0.90553571, dom-acc: 0.60232143, val-acc: 0.89500000, val_loss: 0.26592925
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 281.43211675, sen-loss: 28.20438199, dom-loss: 78.04458660, src-aux-loss: 87.73042315, tar-aux-loss: 87.45272619
Epoch: [26 ] train-acc: 0.90321429, dom-acc: 0.60366071, val-acc: 0.89750000, val_loss: 0.27080029
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 281.91121078, sen-loss: 28.11610512, dom-loss: 78.12285054, src-aux-loss: 87.46267426, tar-aux-loss: 88.20958179
Epoch: [27 ] train-acc: 0.90535714, dom-acc: 0.59482143, val-acc: 0.89500000, val_loss: 0.28040165
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 279.52938581, sen-loss: 27.64427097, dom-loss: 78.20593721, src-aux-loss: 87.04513580, tar-aux-loss: 86.63403964
Epoch: [28 ] train-acc: 0.90803571, dom-acc: 0.58062500, val-acc: 0.90000000, val_loss: 0.25910500
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 278.76074505, sen-loss: 27.50624806, dom-loss: 78.25782782, src-aux-loss: 86.72391647, tar-aux-loss: 86.27275258
Epoch: [29 ] train-acc: 0.91107143, dom-acc: 0.59419643, val-acc: 0.90000000, val_loss: 0.25813869
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 279.50780702, sen-loss: 27.32921545, dom-loss: 78.51552856, src-aux-loss: 86.18191040, tar-aux-loss: 87.48115140
Epoch: [30 ] train-acc: 0.90875000, dom-acc: 0.55732143, val-acc: 0.89750000, val_loss: 0.27432507
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 277.98515320, sen-loss: 26.96122495, dom-loss: 78.50528765, src-aux-loss: 85.87572086, tar-aux-loss: 86.64292085
Epoch: [31 ] train-acc: 0.91053571, dom-acc: 0.56133929, val-acc: 0.89250000, val_loss: 0.26236069
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 276.88206339, sen-loss: 26.68824761, dom-loss: 78.56446767, src-aux-loss: 85.34260494, tar-aux-loss: 86.28674299
Epoch: [32 ] train-acc: 0.90875000, dom-acc: 0.56419643, val-acc: 0.89750000, val_loss: 0.26800057
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 276.58335686, sen-loss: 26.31319309, dom-loss: 78.59985292, src-aux-loss: 85.12155408, tar-aux-loss: 86.54875600
Epoch: [33 ] train-acc: 0.90464286, dom-acc: 0.53892857, val-acc: 0.89000000, val_loss: 0.29334861
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 274.84413815, sen-loss: 26.51151729, dom-loss: 78.50529903, src-aux-loss: 84.61429650, tar-aux-loss: 85.21302521
Epoch: [34 ] train-acc: 0.91339286, dom-acc: 0.55848214, val-acc: 0.89500000, val_loss: 0.26086530
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_electronics_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.25813869
Testing accuracy: 0.86783333
./work/attentions/dvd_electronics_train_HATN.txt
./work/attentions/dvd_electronics_test_HATN.txt
loading data...
source domain:  dvd target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 13856
vocab-size:  80685
['best', 'good', 'excellent', 'great', 'funny', 'enjoyable', 'entertaining', 'nice', 'classic', 'awesome', 'fantastic', 'love', 'hilarious', 'loved', 'funniest', 'amazing', 'perfect', 'underrated', 'brilliant', 'sad', 'favorite', 'wonderful', 'interesting', 'superb', 'finest', 'easy', 'recommend', 'solid', 'beautifully', 'outstanding', 'fine', 'beautiful', 'real', 'memorable', 'enjoy', 'terrific', 'pleasant', 'fascinating', 'amusing', 'worthy', 'brilliantly', 'incredible']
['worst', 'boring', 'horrible', 'bad', 'poor', 'disappointing', 'terrible', 'awful', 'disappointed', 'dull', 'wasted', 'better', 'annoying', 'waste', 'stupid', 'unfunny', 'forgettable', 'laughable', 'unwatchable', 'sucks', 'lousy', 'predictable', 'decent', 'worse', 'pathetic', 'poorly', 'uninspired', 'wrong', 'slow', 'sorry', 'contrived', 'unoriginal', 'ok', 'disgusting', 'sucked', 'crappy', 'pointless', 'mediocre', 'lame', 'overrated', 'biased', 'atrocious', 'depressing', 'ridiculous', 'horrendous', 'irritating', 'dumbest', 'defective', 'okay', 'disapointed', 'silly', 'disjointed', 'cheap', 'weak', 'stupidest', 'dissapointed', 'frustrating', 'ruined']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 17
max memory size: 20
5600 400 6000 17843 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 404.93089771, sen-loss: 77.39434594, dom-loss: 78.65411681, src-aux-loss: 129.67498010, tar-aux-loss: 119.20745552
Epoch: [1  ] train-acc: 0.64714286, dom-acc: 0.83330357, val-acc: 0.68500000, val_loss: 0.65848356
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 375.33241701, sen-loss: 71.85086924, dom-loss: 74.24440330, src-aux-loss: 120.54671443, tar-aux-loss: 108.69042915
Epoch: [2  ] train-acc: 0.76285714, dom-acc: 0.91053571, val-acc: 0.75750000, val_loss: 0.59814775
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 358.42973137, sen-loss: 65.50348890, dom-loss: 72.80766135, src-aux-loss: 116.00942808, tar-aux-loss: 104.10915101
Epoch: [3  ] train-acc: 0.77000000, dom-acc: 0.89741071, val-acc: 0.76750000, val_loss: 0.53050482
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 344.47114944, sen-loss: 57.92772076, dom-loss: 72.50211215, src-aux-loss: 112.25723046, tar-aux-loss: 101.78408587
Epoch: [4  ] train-acc: 0.82053571, dom-acc: 0.86678571, val-acc: 0.82750000, val_loss: 0.45217448
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 332.74849010, sen-loss: 50.52152458, dom-loss: 72.57648665, src-aux-loss: 108.73001897, tar-aux-loss: 100.92045605
Epoch: [5  ] train-acc: 0.83375000, dom-acc: 0.81714286, val-acc: 0.84250000, val_loss: 0.40450656
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 323.97473621, sen-loss: 44.81651789, dom-loss: 72.92474568, src-aux-loss: 107.07763004, tar-aux-loss: 99.15584409
Epoch: [6  ] train-acc: 0.86071429, dom-acc: 0.74642857, val-acc: 0.88750000, val_loss: 0.33728775
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 317.68061495, sen-loss: 40.30047500, dom-loss: 73.66491550, src-aux-loss: 104.96695918, tar-aux-loss: 98.74826384
Epoch: [7  ] train-acc: 0.87089286, dom-acc: 0.72339286, val-acc: 0.88750000, val_loss: 0.31203675
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 312.08762169, sen-loss: 37.82106151, dom-loss: 74.52664369, src-aux-loss: 103.98014110, tar-aux-loss: 95.75977457
Epoch: [8  ] train-acc: 0.87767857, dom-acc: 0.69339286, val-acc: 0.89750000, val_loss: 0.30291569
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 312.01570630, sen-loss: 36.55800004, dom-loss: 75.70864308, src-aux-loss: 102.34958446, tar-aux-loss: 97.39947873
Epoch: [9  ] train-acc: 0.87964286, dom-acc: 0.67785714, val-acc: 0.89500000, val_loss: 0.29355028
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 310.47954392, sen-loss: 35.23051804, dom-loss: 76.39760625, src-aux-loss: 101.81254327, tar-aux-loss: 97.03887707
Epoch: [10 ] train-acc: 0.88232143, dom-acc: 0.64848214, val-acc: 0.89250000, val_loss: 0.29097241
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 309.81535339, sen-loss: 34.63407654, dom-loss: 77.35263419, src-aux-loss: 101.15760773, tar-aux-loss: 96.67103326
Epoch: [11 ] train-acc: 0.88482143, dom-acc: 0.64910714, val-acc: 0.89750000, val_loss: 0.29439366
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 306.18075085, sen-loss: 33.92330557, dom-loss: 77.61205333, src-aux-loss: 100.11477542, tar-aux-loss: 94.53061509
Epoch: [12 ] train-acc: 0.88964286, dom-acc: 0.61866071, val-acc: 0.90000000, val_loss: 0.28244311
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 306.94436336, sen-loss: 33.31654654, dom-loss: 78.49431020, src-aux-loss: 99.02990836, tar-aux-loss: 96.10359782
Epoch: [13 ] train-acc: 0.89017857, dom-acc: 0.62348214, val-acc: 0.90250000, val_loss: 0.28340214
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 302.78145313, sen-loss: 32.63372055, dom-loss: 78.39475554, src-aux-loss: 98.41572052, tar-aux-loss: 93.33725721
Epoch: [14 ] train-acc: 0.89017857, dom-acc: 0.61973214, val-acc: 0.89500000, val_loss: 0.28619093
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 302.62999916, sen-loss: 32.09000510, dom-loss: 78.88611633, src-aux-loss: 97.86228037, tar-aux-loss: 93.79159808
Epoch: [15 ] train-acc: 0.89375000, dom-acc: 0.59937500, val-acc: 0.90000000, val_loss: 0.27692106
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 301.49853992, sen-loss: 31.56614822, dom-loss: 78.63820225, src-aux-loss: 97.04765588, tar-aux-loss: 94.24653208
Epoch: [16 ] train-acc: 0.89607143, dom-acc: 0.60821429, val-acc: 0.90750000, val_loss: 0.27827603
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 299.89542627, sen-loss: 31.25883572, dom-loss: 78.56259274, src-aux-loss: 96.59714377, tar-aux-loss: 93.47685432
Epoch: [17 ] train-acc: 0.89232143, dom-acc: 0.61294643, val-acc: 0.90000000, val_loss: 0.28371295
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 298.64207125, sen-loss: 30.84942740, dom-loss: 78.46542865, src-aux-loss: 95.93868214, tar-aux-loss: 93.38853347
Epoch: [18 ] train-acc: 0.89964286, dom-acc: 0.61785714, val-acc: 0.90000000, val_loss: 0.27221483
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 297.57366419, sen-loss: 30.51315624, dom-loss: 78.38073134, src-aux-loss: 95.31950521, tar-aux-loss: 93.36027145
Epoch: [19 ] train-acc: 0.90089286, dom-acc: 0.61901786, val-acc: 0.90000000, val_loss: 0.26981929
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 295.63699985, sen-loss: 30.14492402, dom-loss: 78.05907738, src-aux-loss: 95.00129646, tar-aux-loss: 92.43170375
Epoch: [20 ] train-acc: 0.90125000, dom-acc: 0.62732143, val-acc: 0.90000000, val_loss: 0.26885414
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 294.28138614, sen-loss: 29.81611884, dom-loss: 77.84274036, src-aux-loss: 94.66517514, tar-aux-loss: 91.95735174
Epoch: [21 ] train-acc: 0.89928571, dom-acc: 0.63678571, val-acc: 0.90500000, val_loss: 0.26907086
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 294.49911976, sen-loss: 29.52845704, dom-loss: 77.88893789, src-aux-loss: 94.19473296, tar-aux-loss: 92.88699287
Epoch: [22 ] train-acc: 0.89857143, dom-acc: 0.64071429, val-acc: 0.89750000, val_loss: 0.27843323
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 292.08770990, sen-loss: 29.38705541, dom-loss: 77.66967100, src-aux-loss: 93.66762221, tar-aux-loss: 91.36336118
Epoch: [23 ] train-acc: 0.90267857, dom-acc: 0.63732143, val-acc: 0.90000000, val_loss: 0.26640218
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 292.19708204, sen-loss: 28.98869118, dom-loss: 77.47290885, src-aux-loss: 93.06082195, tar-aux-loss: 92.67465985
Epoch: [24 ] train-acc: 0.90535714, dom-acc: 0.64303571, val-acc: 0.90000000, val_loss: 0.26312220
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 290.04500890, sen-loss: 28.69724499, dom-loss: 77.52162546, src-aux-loss: 92.74740797, tar-aux-loss: 91.07873046
Epoch: [25 ] train-acc: 0.90357143, dom-acc: 0.64500000, val-acc: 0.90250000, val_loss: 0.26297450
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 291.38215637, sen-loss: 28.40545938, dom-loss: 77.54069161, src-aux-loss: 92.29015577, tar-aux-loss: 93.14584875
Epoch: [26 ] train-acc: 0.90517857, dom-acc: 0.64303571, val-acc: 0.90500000, val_loss: 0.26537955
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 289.07990336, sen-loss: 28.26777629, dom-loss: 77.32616091, src-aux-loss: 91.71182311, tar-aux-loss: 91.77414346
Epoch: [27 ] train-acc: 0.90517857, dom-acc: 0.64276786, val-acc: 0.90000000, val_loss: 0.26741427
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 287.62664962, sen-loss: 27.88549680, dom-loss: 77.77077955, src-aux-loss: 91.40958369, tar-aux-loss: 90.56078893
Epoch: [28 ] train-acc: 0.90714286, dom-acc: 0.63571429, val-acc: 0.90500000, val_loss: 0.25918862
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 287.33587694, sen-loss: 27.77142082, dom-loss: 77.68606257, src-aux-loss: 90.79267341, tar-aux-loss: 91.08572012
Epoch: [29 ] train-acc: 0.90785714, dom-acc: 0.63375000, val-acc: 0.90500000, val_loss: 0.25751987
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 286.31011844, sen-loss: 27.53848621, dom-loss: 77.91341108, src-aux-loss: 90.33442879, tar-aux-loss: 90.52379483
Epoch: [30 ] train-acc: 0.90500000, dom-acc: 0.63196429, val-acc: 0.90000000, val_loss: 0.27045169
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 287.47416139, sen-loss: 27.12926979, dom-loss: 78.00141835, src-aux-loss: 89.93806267, tar-aux-loss: 92.40541095
Epoch: [31 ] train-acc: 0.91000000, dom-acc: 0.62526786, val-acc: 0.90750000, val_loss: 0.26081571
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 284.34710932, sen-loss: 26.91007945, dom-loss: 77.99491751, src-aux-loss: 89.52639151, tar-aux-loss: 89.91572088
Epoch: [32 ] train-acc: 0.90875000, dom-acc: 0.62258929, val-acc: 0.89750000, val_loss: 0.26820070
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 283.68636322, sen-loss: 26.54694524, dom-loss: 78.16037381, src-aux-loss: 89.06350166, tar-aux-loss: 89.91554320
Epoch: [33 ] train-acc: 0.90285714, dom-acc: 0.62223214, val-acc: 0.89000000, val_loss: 0.28637767
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 283.60528588, sen-loss: 26.70274544, dom-loss: 78.20431167, src-aux-loss: 88.69931418, tar-aux-loss: 89.99891466
Epoch: [34 ] train-acc: 0.91357143, dom-acc: 0.61294643, val-acc: 0.90750000, val_loss: 0.25714436
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 283.59394741, sen-loss: 26.40877004, dom-loss: 78.44000959, src-aux-loss: 88.21922541, tar-aux-loss: 90.52594048
Epoch: [35 ] train-acc: 0.91517857, dom-acc: 0.61017857, val-acc: 0.90750000, val_loss: 0.25422877
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 281.86989045, sen-loss: 26.15307830, dom-loss: 78.34214979, src-aux-loss: 87.78388023, tar-aux-loss: 89.59078079
Epoch: [36 ] train-acc: 0.91392857, dom-acc: 0.60830357, val-acc: 0.90500000, val_loss: 0.25484595
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 281.99053717, sen-loss: 25.71325485, dom-loss: 78.17594361, src-aux-loss: 87.28931922, tar-aux-loss: 90.81202084
Epoch: [37 ] train-acc: 0.91607143, dom-acc: 0.61776786, val-acc: 0.90500000, val_loss: 0.25563401
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 280.32455468, sen-loss: 25.54489501, dom-loss: 78.25489450, src-aux-loss: 86.84289533, tar-aux-loss: 89.68186921
Epoch: [38 ] train-acc: 0.91553571, dom-acc: 0.62196429, val-acc: 0.90750000, val_loss: 0.25536707
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 279.79148030, sen-loss: 25.37219518, dom-loss: 78.01941657, src-aux-loss: 86.47052479, tar-aux-loss: 89.92934412
Epoch: [39 ] train-acc: 0.91767857, dom-acc: 0.62571429, val-acc: 0.90250000, val_loss: 0.25788382
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 279.08830428, sen-loss: 25.14571606, dom-loss: 77.87836462, src-aux-loss: 85.85074878, tar-aux-loss: 90.21347529
Epoch: [40 ] train-acc: 0.91732143, dom-acc: 0.64196429, val-acc: 0.90000000, val_loss: 0.26557526
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_kitchen_HATN.ckpt
Best Epoch: [ 35] best val accuracy: 0.00000000 best val loss: 0.25422877
Testing accuracy: 0.87000000
./work/attentions/dvd_kitchen_train_HATN.txt
./work/attentions/dvd_kitchen_test_HATN.txt
loading data...
source domain:  dvd target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  11843 30180
vocab-size:  91852
['best', 'good', 'excellent', 'great', 'funny', 'enjoyable', 'entertaining', 'classic', 'nice', 'fantastic', 'awesome', 'hilarious', 'love', 'loved', 'funniest', 'underrated', 'amazing', 'better', 'wonderful', 'sad', 'perfect', 'favorite', 'brilliant', 'interesting', 'superb', 'finest', 'solid', 'beautifully', 'easy', 'real', 'worthy', 'believable', 'recommend', 'beautiful', 'terrific', 'fine', 'amusing', 'brilliantly', 'enjoy', 'outstanding', 'fascinating', 'likeable', 'spectacular', 'memorable', 'incredible']
['worst', 'boring', 'horrible', 'bad', 'poor', 'disappointing', 'terrible', 'awful', 'disappointed', 'dull', 'wasted', 'annoying', 'stupid', 'waste', 'unfunny', 'worse', 'sucks', 'laughable', 'lousy', 'unwatchable', 'forgettable', 'pathetic', 'wrong', 'uninspired', 'predictable', 'decent', 'ok', 'poorly', 'disgusting', 'contrived', 'slow', 'unoriginal', 'dissapointed', 'sorry', 'atrocious', 'crappy', 'pointless', 'mediocre', 'lame', 'sucked', 'overrated', 'biased', 'okay', 'depressing', 'silly', 'horrendous', 'dumbest', 'disapointed', 'ridiculous', 'disjointed', 'weak', 'stupidest', 'frustrating', 'wasting', 'ruined']
max  story size: 226
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
5600 400 6000 17843 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 399.55225420, sen-loss: 77.48150361, dom-loss: 80.22259659, src-aux-loss: 127.50767094, tar-aux-loss: 114.34048402
Epoch: [1  ] train-acc: 0.64482143, dom-acc: 0.44294643, val-acc: 0.67250000, val_loss: 0.65974897
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 376.78355312, sen-loss: 71.90522403, dom-loss: 78.89289683, src-aux-loss: 119.63596952, tar-aux-loss: 106.34946167
Epoch: [2  ] train-acc: 0.75964286, dom-acc: 0.47026786, val-acc: 0.76750000, val_loss: 0.59830791
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 360.22252107, sen-loss: 65.42419016, dom-loss: 78.69901031, src-aux-loss: 114.75550103, tar-aux-loss: 101.34382105
Epoch: [3  ] train-acc: 0.77910714, dom-acc: 0.53178571, val-acc: 0.77000000, val_loss: 0.52748686
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 345.35253620, sen-loss: 57.81235319, dom-loss: 78.47503835, src-aux-loss: 110.72917116, tar-aux-loss: 98.33597267
Epoch: [4  ] train-acc: 0.82303571, dom-acc: 0.59267857, val-acc: 0.82250000, val_loss: 0.45100543
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 331.45895362, sen-loss: 50.69279182, dom-loss: 78.13232380, src-aux-loss: 106.77426589, tar-aux-loss: 95.85957241
Epoch: [5  ] train-acc: 0.83750000, dom-acc: 0.64982143, val-acc: 0.86250000, val_loss: 0.39919230
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 321.86487126, sen-loss: 45.19086355, dom-loss: 77.70450222, src-aux-loss: 104.60105437, tar-aux-loss: 94.36845064
Epoch: [6  ] train-acc: 0.85267857, dom-acc: 0.62303571, val-acc: 0.87500000, val_loss: 0.34366894
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 315.04346228, sen-loss: 41.10364224, dom-loss: 77.67459601, src-aux-loss: 102.46211666, tar-aux-loss: 93.80310601
Epoch: [7  ] train-acc: 0.86839286, dom-acc: 0.65526786, val-acc: 0.89250000, val_loss: 0.31982327
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 309.14037204, sen-loss: 38.42813601, dom-loss: 77.68630022, src-aux-loss: 101.67672187, tar-aux-loss: 91.34921420
Epoch: [8  ] train-acc: 0.87875000, dom-acc: 0.65026786, val-acc: 0.90000000, val_loss: 0.30703083
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 309.35184813, sen-loss: 37.02879533, dom-loss: 77.52376151, src-aux-loss: 100.08189112, tar-aux-loss: 94.71739990
Epoch: [9  ] train-acc: 0.87946429, dom-acc: 0.65580357, val-acc: 0.89750000, val_loss: 0.29781899
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 303.97522664, sen-loss: 35.50669286, dom-loss: 77.10790455, src-aux-loss: 99.38487166, tar-aux-loss: 91.97575611
Epoch: [10 ] train-acc: 0.88089286, dom-acc: 0.64803571, val-acc: 0.89750000, val_loss: 0.29277784
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 301.80143881, sen-loss: 34.73795550, dom-loss: 77.23374850, src-aux-loss: 98.64459687, tar-aux-loss: 91.18513691
Epoch: [11 ] train-acc: 0.88678571, dom-acc: 0.66901786, val-acc: 0.89750000, val_loss: 0.29138139
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 298.87566638, sen-loss: 34.07916476, dom-loss: 77.04893655, src-aux-loss: 97.83691156, tar-aux-loss: 89.91065413
Epoch: [12 ] train-acc: 0.88750000, dom-acc: 0.65071429, val-acc: 0.90750000, val_loss: 0.28415936
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 298.23984241, sen-loss: 33.45630183, dom-loss: 77.10096413, src-aux-loss: 97.02565438, tar-aux-loss: 90.65692246
Epoch: [13 ] train-acc: 0.88910714, dom-acc: 0.67142857, val-acc: 0.90250000, val_loss: 0.28496301
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 295.51857114, sen-loss: 32.76752651, dom-loss: 77.07174098, src-aux-loss: 96.37452668, tar-aux-loss: 89.30477649
Epoch: [14 ] train-acc: 0.89089286, dom-acc: 0.67446429, val-acc: 0.89500000, val_loss: 0.28766096
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 294.45051742, sen-loss: 32.27625886, dom-loss: 77.03331411, src-aux-loss: 95.92141974, tar-aux-loss: 89.21952480
Epoch: [15 ] train-acc: 0.89196429, dom-acc: 0.66321429, val-acc: 0.90250000, val_loss: 0.28080934
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 294.03478169, sen-loss: 31.64744274, dom-loss: 76.72637415, src-aux-loss: 95.30166042, tar-aux-loss: 90.35930502
Epoch: [16 ] train-acc: 0.89285714, dom-acc: 0.67071429, val-acc: 0.90250000, val_loss: 0.27990732
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 291.64050150, sen-loss: 31.37224029, dom-loss: 76.83119988, src-aux-loss: 94.83040416, tar-aux-loss: 88.60665762
Epoch: [17 ] train-acc: 0.89232143, dom-acc: 0.66982143, val-acc: 0.89000000, val_loss: 0.28789297
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 290.23478889, sen-loss: 30.91556332, dom-loss: 76.82222587, src-aux-loss: 94.12602335, tar-aux-loss: 88.37097621
Epoch: [18 ] train-acc: 0.89875000, dom-acc: 0.66919643, val-acc: 0.90000000, val_loss: 0.27350762
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 289.40266204, sen-loss: 30.56929782, dom-loss: 76.72384483, src-aux-loss: 93.44984037, tar-aux-loss: 88.65967840
Epoch: [19 ] train-acc: 0.90000000, dom-acc: 0.67178571, val-acc: 0.90500000, val_loss: 0.27078906
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 289.45376945, sen-loss: 30.19649024, dom-loss: 76.84950083, src-aux-loss: 93.09346658, tar-aux-loss: 89.31431198
Epoch: [20 ] train-acc: 0.90035714, dom-acc: 0.67044643, val-acc: 0.90250000, val_loss: 0.26976797
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 287.64700079, sen-loss: 29.83907036, dom-loss: 76.68232203, src-aux-loss: 92.66474420, tar-aux-loss: 88.46086502
Epoch: [21 ] train-acc: 0.90178571, dom-acc: 0.67500000, val-acc: 0.90250000, val_loss: 0.26844871
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 284.88898659, sen-loss: 29.58027914, dom-loss: 76.81722230, src-aux-loss: 92.23620528, tar-aux-loss: 86.25528085
Epoch: [22 ] train-acc: 0.89678571, dom-acc: 0.67875000, val-acc: 0.88750000, val_loss: 0.28611046
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 284.56428528, sen-loss: 29.38003111, dom-loss: 76.53676659, src-aux-loss: 91.65971971, tar-aux-loss: 86.98776782
Epoch: [23 ] train-acc: 0.90392857, dom-acc: 0.66919643, val-acc: 0.90250000, val_loss: 0.26591176
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 285.79604721, sen-loss: 29.06897005, dom-loss: 76.80182439, src-aux-loss: 91.09376562, tar-aux-loss: 88.83148628
Epoch: [24 ] train-acc: 0.90339286, dom-acc: 0.67401786, val-acc: 0.90000000, val_loss: 0.26384503
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 281.33744788, sen-loss: 28.84027748, dom-loss: 76.64161575, src-aux-loss: 90.63545054, tar-aux-loss: 85.22010303
Epoch: [25 ] train-acc: 0.90428571, dom-acc: 0.66973214, val-acc: 0.90250000, val_loss: 0.26259953
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 282.97243524, sen-loss: 28.48663192, dom-loss: 76.51771408, src-aux-loss: 90.24828273, tar-aux-loss: 87.71980530
Epoch: [26 ] train-acc: 0.90428571, dom-acc: 0.67267857, val-acc: 0.90250000, val_loss: 0.26736322
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 282.21812797, sen-loss: 28.34170971, dom-loss: 76.55985433, src-aux-loss: 89.62096757, tar-aux-loss: 87.69559675
Epoch: [27 ] train-acc: 0.90357143, dom-acc: 0.68187500, val-acc: 0.90250000, val_loss: 0.27377447
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 278.79600382, sen-loss: 27.85978833, dom-loss: 76.58452594, src-aux-loss: 89.50696152, tar-aux-loss: 84.84472877
Epoch: [28 ] train-acc: 0.90607143, dom-acc: 0.66250000, val-acc: 0.90750000, val_loss: 0.25920141
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 279.39016843, sen-loss: 27.84537551, dom-loss: 76.46775377, src-aux-loss: 88.86537856, tar-aux-loss: 86.21166235
Epoch: [29 ] train-acc: 0.90482143, dom-acc: 0.66196429, val-acc: 0.90750000, val_loss: 0.25812924
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 278.54758668, sen-loss: 27.64389217, dom-loss: 76.63870114, src-aux-loss: 88.40523314, tar-aux-loss: 85.85976154
Epoch: [30 ] train-acc: 0.90642857, dom-acc: 0.68000000, val-acc: 0.90250000, val_loss: 0.26947609
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 279.08343935, sen-loss: 27.19574450, dom-loss: 76.66312003, src-aux-loss: 87.93359584, tar-aux-loss: 87.29097933
Epoch: [31 ] train-acc: 0.91017857, dom-acc: 0.67616071, val-acc: 0.90000000, val_loss: 0.25800517
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 275.80752110, sen-loss: 27.04103285, dom-loss: 76.44668025, src-aux-loss: 87.61007535, tar-aux-loss: 84.70973402
Epoch: [32 ] train-acc: 0.90910714, dom-acc: 0.68232143, val-acc: 0.90000000, val_loss: 0.26798290
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 275.67134976, sen-loss: 26.62229481, dom-loss: 76.54972166, src-aux-loss: 87.10725230, tar-aux-loss: 85.39208150
Epoch: [33 ] train-acc: 0.90142857, dom-acc: 0.68455357, val-acc: 0.89000000, val_loss: 0.29060200
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 275.19691420, sen-loss: 26.69957115, dom-loss: 76.59115636, src-aux-loss: 86.77077091, tar-aux-loss: 85.13541555
Epoch: [34 ] train-acc: 0.91357143, dom-acc: 0.67196429, val-acc: 0.90250000, val_loss: 0.25700256
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 273.57368112, sen-loss: 26.32262220, dom-loss: 76.57473594, src-aux-loss: 86.23506296, tar-aux-loss: 84.44126052
Epoch: [35 ] train-acc: 0.91285714, dom-acc: 0.66803571, val-acc: 0.90250000, val_loss: 0.25447586
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 272.41888928, sen-loss: 26.08835775, dom-loss: 76.44216949, src-aux-loss: 85.82688153, tar-aux-loss: 84.06148100
Epoch: [36 ] train-acc: 0.91553571, dom-acc: 0.66991071, val-acc: 0.89750000, val_loss: 0.25721842
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 273.42083097, sen-loss: 25.76327254, dom-loss: 76.25987571, src-aux-loss: 85.30215913, tar-aux-loss: 86.09552515
Epoch: [37 ] train-acc: 0.91410714, dom-acc: 0.66598214, val-acc: 0.90250000, val_loss: 0.25301617
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 271.63145518, sen-loss: 25.62886810, dom-loss: 76.45442605, src-aux-loss: 84.67207682, tar-aux-loss: 84.87608427
Epoch: [38 ] train-acc: 0.91714286, dom-acc: 0.67116071, val-acc: 0.90250000, val_loss: 0.25792703
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 270.61078525, sen-loss: 25.42677694, dom-loss: 76.49396819, src-aux-loss: 84.14781463, tar-aux-loss: 84.54222518
Epoch: [39 ] train-acc: 0.91928571, dom-acc: 0.67214286, val-acc: 0.90750000, val_loss: 0.25450817
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 270.92418671, sen-loss: 25.18464252, dom-loss: 76.24007070, src-aux-loss: 83.57998973, tar-aux-loss: 85.91948384
Epoch: [40 ] train-acc: 0.91732143, dom-acc: 0.68142857, val-acc: 0.90000000, val_loss: 0.26538548
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 268.77902603, sen-loss: 24.91752319, dom-loss: 76.57837802, src-aux-loss: 83.22181726, tar-aux-loss: 84.06130862
Epoch: [41 ] train-acc: 0.92000000, dom-acc: 0.67553571, val-acc: 0.90750000, val_loss: 0.25440064
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 266.53681326, sen-loss: 24.77589231, dom-loss: 76.34275275, src-aux-loss: 82.96552229, tar-aux-loss: 82.45264661
Epoch: [42 ] train-acc: 0.92035714, dom-acc: 0.66410714, val-acc: 0.91000000, val_loss: 0.25194743
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 267.54163909, sen-loss: 24.58104020, dom-loss: 76.46933985, src-aux-loss: 82.23119712, tar-aux-loss: 84.26006317
Epoch: [43 ] train-acc: 0.92053571, dom-acc: 0.67857143, val-acc: 0.90250000, val_loss: 0.26579094
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 265.05101609, sen-loss: 24.35767671, dom-loss: 76.28625238, src-aux-loss: 81.88973713, tar-aux-loss: 82.51735026
Epoch: [44 ] train-acc: 0.92125000, dom-acc: 0.66294643, val-acc: 0.91000000, val_loss: 0.25240555
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 266.23167038, sen-loss: 24.16373481, dom-loss: 76.26235157, src-aux-loss: 81.14540082, tar-aux-loss: 84.66018319
Epoch: [45 ] train-acc: 0.92053571, dom-acc: 0.66705357, val-acc: 0.91250000, val_loss: 0.25308511
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 264.19126666, sen-loss: 23.86964632, dom-loss: 76.38223547, src-aux-loss: 80.63099664, tar-aux-loss: 83.30838931
Epoch: [46 ] train-acc: 0.92464286, dom-acc: 0.67151786, val-acc: 0.90500000, val_loss: 0.25880456
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 263.82620573, sen-loss: 23.49163128, dom-loss: 76.70981073, src-aux-loss: 80.20445561, tar-aux-loss: 83.42030853
Epoch: [47 ] train-acc: 0.92500000, dom-acc: 0.67196429, val-acc: 0.90750000, val_loss: 0.25967869
---------------------------------------------------

Successfully load model from save path: ./work/models/dvd_video_HATN.ckpt
Best Epoch: [ 42] best val accuracy: 0.00000000 best val loss: 0.25194743
Testing accuracy: 0.88966667
./work/attentions/dvd_video_train_HATN.txt
./work/attentions/dvd_video_test_HATN.txt
loading data...
source domain:  electronics target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 9750
vocab-size:  83050
['great', 'good', 'excellent', 'easy', 'best', 'works', 'perfect', 'happy', 'fantastic', 'awesome', 'satisfied', 'solid', 'outstanding', 'perfectly', 'worth', 'recommended', 'love', 'fine', 'durable', 'reliable', 'comfortable', 'amazing', 'pleased', 'quick', 'arrived', 'impressive', 'old', 'decent', 'low', 'greatest', 'recommend', 'useful', 'enough', 'clear', 'nice', 'cool', 'exactly', 'sound', 'loves', 'slick']
['returned', 'return', 'poor', 'worst', 'disappointed', 'frustrating', 'disappointing', 'useless', 'horrible', 'wrong', 'returning', 'stopped', 'awful', 'lasted', 'poorly', 'bad', 'unreliable', 'failed', 'cheap', 'unacceptable', 'terrible', 'broke', 'hard', 'overpriced', 'impossible', 'expensive', 'worthless', 'ridiculous', 'worked', 'quit', 'beware', 'misleading', 'sad', 'save', 'died', 'uncomfortable', 'difficult', 'short', 'defective', 'unhappy', 'waste', 'annoying', 'stay', 'lousy', 'unusable', 'wasted', 'slow', 'spent', 'lose', 'incompatible', 'goes', 'sucks', 'tried', 'crashed', 'false', 'average', 'cheaply', 'back', 'broken', 'ok', 'flaky', 'flawed', 'trying', 'mediocre']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 18
max memory size: 20
5600 400 6000 23009 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83051, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 408.05838084, sen-loss: 75.76250571, dom-loss: 78.86270911, src-aux-loss: 128.86518151, tar-aux-loss: 124.56798434
Epoch: [1  ] train-acc: 0.68892857, dom-acc: 0.82875000, val-acc: 0.69500000, val_loss: 0.63696307
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 377.53451586, sen-loss: 66.70145786, dom-loss: 74.14800996, src-aux-loss: 120.03224194, tar-aux-loss: 116.65280455
Epoch: [2  ] train-acc: 0.77428571, dom-acc: 0.82553571, val-acc: 0.78000000, val_loss: 0.54503512
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 354.04552674, sen-loss: 55.26848578, dom-loss: 72.20342100, src-aux-loss: 114.97934788, tar-aux-loss: 111.59427124
Epoch: [3  ] train-acc: 0.80750000, dom-acc: 0.62508929, val-acc: 0.83250000, val_loss: 0.42750120
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 340.58408189, sen-loss: 47.03471383, dom-loss: 72.11280257, src-aux-loss: 111.55981696, tar-aux-loss: 109.87674963
Epoch: [4  ] train-acc: 0.83625000, dom-acc: 0.60125000, val-acc: 0.86500000, val_loss: 0.36999372
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 332.90615559, sen-loss: 42.54367273, dom-loss: 72.69248831, src-aux-loss: 108.97410840, tar-aux-loss: 108.69588578
Epoch: [5  ] train-acc: 0.85339286, dom-acc: 0.58830357, val-acc: 0.86500000, val_loss: 0.33795831
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 328.25742245, sen-loss: 39.44798344, dom-loss: 73.63288176, src-aux-loss: 106.92495167, tar-aux-loss: 108.25160587
Epoch: [6  ] train-acc: 0.86982143, dom-acc: 0.57517857, val-acc: 0.89000000, val_loss: 0.30729631
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 323.24985313, sen-loss: 37.00972876, dom-loss: 74.49998599, src-aux-loss: 105.79908329, tar-aux-loss: 105.94105560
Epoch: [7  ] train-acc: 0.87714286, dom-acc: 0.54178571, val-acc: 0.89250000, val_loss: 0.28785747
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 320.22347236, sen-loss: 35.13818495, dom-loss: 75.54873830, src-aux-loss: 104.29421121, tar-aux-loss: 105.24233967
Epoch: [8  ] train-acc: 0.88464286, dom-acc: 0.49616071, val-acc: 0.88750000, val_loss: 0.27578354
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 318.04493999, sen-loss: 33.77336270, dom-loss: 76.66148967, src-aux-loss: 102.95668668, tar-aux-loss: 104.65339923
Epoch: [9  ] train-acc: 0.88821429, dom-acc: 0.48455357, val-acc: 0.90250000, val_loss: 0.25643057
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 316.79072094, sen-loss: 32.38257328, dom-loss: 77.55190772, src-aux-loss: 102.04424804, tar-aux-loss: 104.81199133
Epoch: [10 ] train-acc: 0.88678571, dom-acc: 0.45669643, val-acc: 0.91250000, val_loss: 0.24619363
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 314.14278483, sen-loss: 31.46929400, dom-loss: 78.16185880, src-aux-loss: 101.28337169, tar-aux-loss: 103.22826022
Epoch: [11 ] train-acc: 0.89892857, dom-acc: 0.42035714, val-acc: 0.91500000, val_loss: 0.23885411
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 314.26583481, sen-loss: 30.84237537, dom-loss: 78.78532076, src-aux-loss: 100.64684027, tar-aux-loss: 103.99129760
Epoch: [12 ] train-acc: 0.88482143, dom-acc: 0.41919643, val-acc: 0.91000000, val_loss: 0.23880301
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 310.99917388, sen-loss: 29.86537153, dom-loss: 79.00242412, src-aux-loss: 99.81106472, tar-aux-loss: 102.32031256
Epoch: [13 ] train-acc: 0.90464286, dom-acc: 0.39589286, val-acc: 0.93000000, val_loss: 0.22829908
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 311.46248007, sen-loss: 29.41663048, dom-loss: 79.12851202, src-aux-loss: 99.13572145, tar-aux-loss: 103.78161728
Epoch: [14 ] train-acc: 0.90857143, dom-acc: 0.40830357, val-acc: 0.93000000, val_loss: 0.22334859
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 308.52383208, sen-loss: 28.74495975, dom-loss: 79.26622522, src-aux-loss: 98.44486946, tar-aux-loss: 102.06777835
Epoch: [15 ] train-acc: 0.91142857, dom-acc: 0.42580357, val-acc: 0.93750000, val_loss: 0.21973144
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 307.31904602, sen-loss: 28.21802913, dom-loss: 79.22713268, src-aux-loss: 98.08047891, tar-aux-loss: 101.79340518
Epoch: [16 ] train-acc: 0.91214286, dom-acc: 0.43312500, val-acc: 0.93750000, val_loss: 0.21903478
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 306.83817697, sen-loss: 27.80458671, dom-loss: 79.04456562, src-aux-loss: 97.47855276, tar-aux-loss: 102.51047164
Epoch: [17 ] train-acc: 0.91232143, dom-acc: 0.44348214, val-acc: 0.93500000, val_loss: 0.21835200
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 305.04227138, sen-loss: 27.29674845, dom-loss: 78.93954855, src-aux-loss: 96.85803008, tar-aux-loss: 101.94794279
Epoch: [18 ] train-acc: 0.91535714, dom-acc: 0.45535714, val-acc: 0.92750000, val_loss: 0.21104994
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 301.83515477, sen-loss: 26.81729091, dom-loss: 78.59331685, src-aux-loss: 96.20625001, tar-aux-loss: 100.21829718
Epoch: [19 ] train-acc: 0.91750000, dom-acc: 0.47151786, val-acc: 0.92500000, val_loss: 0.20851435
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 302.42364526, sen-loss: 26.38229841, dom-loss: 78.44890648, src-aux-loss: 96.03039777, tar-aux-loss: 101.56204265
Epoch: [20 ] train-acc: 0.91232143, dom-acc: 0.50732143, val-acc: 0.92000000, val_loss: 0.20818208
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 300.27447772, sen-loss: 26.04032579, dom-loss: 78.01032329, src-aux-loss: 95.53255469, tar-aux-loss: 100.69127208
Epoch: [21 ] train-acc: 0.91625000, dom-acc: 0.50303571, val-acc: 0.92000000, val_loss: 0.20567377
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 300.07005644, sen-loss: 25.68851198, dom-loss: 77.80383569, src-aux-loss: 94.94442749, tar-aux-loss: 101.63327891
Epoch: [22 ] train-acc: 0.92142857, dom-acc: 0.52446429, val-acc: 0.93000000, val_loss: 0.20458253
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 297.14636350, sen-loss: 25.40869034, dom-loss: 77.41784769, src-aux-loss: 94.51501513, tar-aux-loss: 99.80481017
Epoch: [23 ] train-acc: 0.91875000, dom-acc: 0.53919643, val-acc: 0.92250000, val_loss: 0.20327768
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 297.27079844, sen-loss: 25.14227618, dom-loss: 77.30171669, src-aux-loss: 94.11987090, tar-aux-loss: 100.70693302
Epoch: [24 ] train-acc: 0.92357143, dom-acc: 0.52089286, val-acc: 0.92500000, val_loss: 0.20285331
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 294.91143703, sen-loss: 24.81681806, dom-loss: 77.16036260, src-aux-loss: 93.67193234, tar-aux-loss: 99.26232320
Epoch: [25 ] train-acc: 0.92357143, dom-acc: 0.55133929, val-acc: 0.93750000, val_loss: 0.20269096
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 295.99658179, sen-loss: 24.35520758, dom-loss: 77.01605684, src-aux-loss: 93.47188401, tar-aux-loss: 101.15343356
Epoch: [26 ] train-acc: 0.92125000, dom-acc: 0.53625000, val-acc: 0.94250000, val_loss: 0.20919904
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 293.53284192, sen-loss: 24.12028043, dom-loss: 76.89606798, src-aux-loss: 92.99864650, tar-aux-loss: 99.51784647
Epoch: [27 ] train-acc: 0.92607143, dom-acc: 0.56812500, val-acc: 0.92500000, val_loss: 0.19943085
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 293.85675406, sen-loss: 23.82351112, dom-loss: 76.93356234, src-aux-loss: 92.60547453, tar-aux-loss: 100.49420571
Epoch: [28 ] train-acc: 0.92875000, dom-acc: 0.55357143, val-acc: 0.93500000, val_loss: 0.19945467
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 292.62415576, sen-loss: 23.51023139, dom-loss: 76.78018177, src-aux-loss: 92.25941044, tar-aux-loss: 100.07432973
Epoch: [29 ] train-acc: 0.92875000, dom-acc: 0.54303571, val-acc: 0.92250000, val_loss: 0.19900343
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 290.64322925, sen-loss: 23.15028936, dom-loss: 76.88576031, src-aux-loss: 91.78841579, tar-aux-loss: 98.81876647
Epoch: [30 ] train-acc: 0.93053571, dom-acc: 0.55187500, val-acc: 0.93500000, val_loss: 0.19940197
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 291.75849462, sen-loss: 22.88567834, dom-loss: 76.97198868, src-aux-loss: 91.36171389, tar-aux-loss: 100.53911507
Epoch: [31 ] train-acc: 0.92892857, dom-acc: 0.51598214, val-acc: 0.93250000, val_loss: 0.20118645
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 289.70281386, sen-loss: 22.58796284, dom-loss: 77.13428867, src-aux-loss: 91.17161417, tar-aux-loss: 98.80894864
Epoch: [32 ] train-acc: 0.93178571, dom-acc: 0.51830357, val-acc: 0.92750000, val_loss: 0.19718689
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 289.52265406, sen-loss: 22.33992372, dom-loss: 77.35146159, src-aux-loss: 90.65819806, tar-aux-loss: 99.17307210
Epoch: [33 ] train-acc: 0.93232143, dom-acc: 0.52767857, val-acc: 0.93250000, val_loss: 0.19866140
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 288.47403455, sen-loss: 22.04618292, dom-loss: 77.45626694, src-aux-loss: 90.06066805, tar-aux-loss: 98.91091764
Epoch: [34 ] train-acc: 0.93339286, dom-acc: 0.49178571, val-acc: 0.93000000, val_loss: 0.19809686
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 288.90742803, sen-loss: 21.76764603, dom-loss: 77.65203953, src-aux-loss: 89.89411539, tar-aux-loss: 99.59362674
Epoch: [35 ] train-acc: 0.93535714, dom-acc: 0.49000000, val-acc: 0.93750000, val_loss: 0.19708751
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 287.86329055, sen-loss: 21.61002570, dom-loss: 77.80352783, src-aux-loss: 89.33353436, tar-aux-loss: 99.11620218
Epoch: [36 ] train-acc: 0.93607143, dom-acc: 0.46419643, val-acc: 0.93000000, val_loss: 0.19722280
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 287.00557661, sen-loss: 21.23768897, dom-loss: 77.92621601, src-aux-loss: 88.88106167, tar-aux-loss: 98.96061021
Epoch: [37 ] train-acc: 0.93625000, dom-acc: 0.46892857, val-acc: 0.93000000, val_loss: 0.19787282
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 287.33506370, sen-loss: 21.01566409, dom-loss: 78.15276694, src-aux-loss: 88.67814171, tar-aux-loss: 99.48849028
Epoch: [38 ] train-acc: 0.93482143, dom-acc: 0.45276786, val-acc: 0.93000000, val_loss: 0.20206550
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 284.27645898, sen-loss: 20.88734504, dom-loss: 78.31630063, src-aux-loss: 88.02367443, tar-aux-loss: 97.04913843
Epoch: [39 ] train-acc: 0.93892857, dom-acc: 0.43473214, val-acc: 0.93250000, val_loss: 0.19847092
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 285.29147911, sen-loss: 20.44699319, dom-loss: 78.29583597, src-aux-loss: 87.52314281, tar-aux-loss: 99.02550781
Epoch: [40 ] train-acc: 0.94053571, dom-acc: 0.46785714, val-acc: 0.92750000, val_loss: 0.19801924
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_books_HATN.ckpt
Best Epoch: [ 35] best val accuracy: 0.00000000 best val loss: 0.19708751
Testing accuracy: 0.83616667
./work/attentions/electronics_books_train_HATN.txt
./work/attentions/electronics_books_test_HATN.txt
loading data...
source domain:  electronics target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 11843
vocab-size:  85442
['great', 'good', 'easy', 'excellent', 'best', 'works', 'perfect', 'fantastic', 'solid', 'awesome', 'outstanding', 'satisfied', 'perfectly', 'happy', 'fine', 'reliable', 'worth', 'highly', 'comfortable', 'durable', 'love', 'old', 'pleased', 'amazing', 'inexpensive', 'decent', 'impressive', 'quick', 'sturdy', 'nice', 'high', 'simple', 'recommend', 'enough', 'awsome', 'lightweight', 'arrived', 'greatest', 'useful', 'easily', 'cool', 'reasonable', 'impressed', 'dependable', 'expected', 'effective', 'loves', 'clear', 'slick', 'well', 'ultimate']
['returned', 'return', 'poor', 'worst', 'disappointing', 'frustrating', 'disappointed', 'horrible', 'useless', 'stopped', 'bad', 'returning', 'terrible', 'failed', 'poorly', 'awful', 'unreliable', 'broke', 'wrong', 'cheap', 'worthless', 'lasted', 'unacceptable', 'hard', 'overpriced', 'expensive', 'beware', 'quit', 'back', 'ridiculous', 'misleading', 'low', 'sad', 'save', 'defective', 'unhappy', 'waste', 'died', 'uncomfortable', 'tried', 'short', 'lousy', 'impossible', 'slow', 'difficult', 'unusable', 'wasted', 'weak', 'lose', 'broken', 'mediocre', 'scratched', 'crashed', 'fooled', 'false', 'average', 'cheaply', 'unable', 'worked', 'dead', 'ok', 'flaky', 'flawed', 'trying', 'true', 'annoying']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 18
max memory size: 20
5600 400 6000 23009 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(85443, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 396.94515371, sen-loss: 75.65796244, dom-loss: 78.85065573, src-aux-loss: 125.77492821, tar-aux-loss: 116.66160780
Epoch: [1  ] train-acc: 0.68839286, dom-acc: 0.76919643, val-acc: 0.70000000, val_loss: 0.63563895
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 366.38764477, sen-loss: 66.68135101, dom-loss: 74.85051125, src-aux-loss: 117.31006616, tar-aux-loss: 107.54571694
Epoch: [2  ] train-acc: 0.77267857, dom-acc: 0.77116071, val-acc: 0.79000000, val_loss: 0.54556549
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 344.61949921, sen-loss: 55.50059971, dom-loss: 73.33286273, src-aux-loss: 112.69876999, tar-aux-loss: 103.08726424
Epoch: [3  ] train-acc: 0.80642857, dom-acc: 0.61107143, val-acc: 0.83000000, val_loss: 0.42885411
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 332.13041496, sen-loss: 47.22411489, dom-loss: 73.11273926, src-aux-loss: 109.26783204, tar-aux-loss: 102.52572757
Epoch: [4  ] train-acc: 0.83517857, dom-acc: 0.58366071, val-acc: 0.87250000, val_loss: 0.37073365
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 322.76300573, sen-loss: 42.60729592, dom-loss: 73.35171962, src-aux-loss: 106.56647778, tar-aux-loss: 100.23751259
Epoch: [5  ] train-acc: 0.85464286, dom-acc: 0.56044643, val-acc: 0.87500000, val_loss: 0.34095961
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 317.27547050, sen-loss: 39.34795074, dom-loss: 74.01559299, src-aux-loss: 104.71502739, tar-aux-loss: 99.19689858
Epoch: [6  ] train-acc: 0.86964286, dom-acc: 0.55705357, val-acc: 0.89500000, val_loss: 0.30492502
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 313.36432147, sen-loss: 36.81149019, dom-loss: 74.66111690, src-aux-loss: 103.41610509, tar-aux-loss: 98.47560906
Epoch: [7  ] train-acc: 0.87821429, dom-acc: 0.53687500, val-acc: 0.89500000, val_loss: 0.28419665
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 309.30136824, sen-loss: 34.86091623, dom-loss: 75.21565080, src-aux-loss: 102.16603100, tar-aux-loss: 97.05877030
Epoch: [8  ] train-acc: 0.88946429, dom-acc: 0.50598214, val-acc: 0.89750000, val_loss: 0.27182010
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 307.24511719, sen-loss: 33.77229460, dom-loss: 75.93504554, src-aux-loss: 100.81356740, tar-aux-loss: 96.72420931
Epoch: [9  ] train-acc: 0.88785714, dom-acc: 0.50187500, val-acc: 0.91500000, val_loss: 0.25359040
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 304.89002442, sen-loss: 32.31167029, dom-loss: 76.90060771, src-aux-loss: 99.85850304, tar-aux-loss: 95.81924385
Epoch: [10 ] train-acc: 0.88535714, dom-acc: 0.48687500, val-acc: 0.92000000, val_loss: 0.24586067
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 303.63438845, sen-loss: 31.39096674, dom-loss: 77.13877517, src-aux-loss: 99.36202723, tar-aux-loss: 95.74262112
Epoch: [11 ] train-acc: 0.89982143, dom-acc: 0.45580357, val-acc: 0.92250000, val_loss: 0.23493704
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 303.72566128, sen-loss: 30.74482399, dom-loss: 77.88341236, src-aux-loss: 98.62110090, tar-aux-loss: 96.47632396
Epoch: [12 ] train-acc: 0.88428571, dom-acc: 0.46312500, val-acc: 0.91000000, val_loss: 0.24047226
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 301.05261874, sen-loss: 29.82455833, dom-loss: 77.90491021, src-aux-loss: 97.84125602, tar-aux-loss: 95.48189330
Epoch: [13 ] train-acc: 0.90678571, dom-acc: 0.42705357, val-acc: 0.93000000, val_loss: 0.22802886
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 299.97935891, sen-loss: 29.37188746, dom-loss: 78.23941135, src-aux-loss: 97.20934063, tar-aux-loss: 95.15872020
Epoch: [14 ] train-acc: 0.90821429, dom-acc: 0.43687500, val-acc: 0.93000000, val_loss: 0.21942222
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 297.87553453, sen-loss: 28.70032012, dom-loss: 78.30762726, src-aux-loss: 96.67165780, tar-aux-loss: 94.19593138
Epoch: [15 ] train-acc: 0.90964286, dom-acc: 0.43026786, val-acc: 0.93500000, val_loss: 0.21734811
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 297.32083893, sen-loss: 28.21819960, dom-loss: 78.34863335, src-aux-loss: 96.09965432, tar-aux-loss: 94.65435231
Epoch: [16 ] train-acc: 0.91053571, dom-acc: 0.42794643, val-acc: 0.94000000, val_loss: 0.21713807
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 295.11420679, sen-loss: 27.77565002, dom-loss: 78.15971929, src-aux-loss: 95.65884918, tar-aux-loss: 93.51998794
Epoch: [17 ] train-acc: 0.91303571, dom-acc: 0.43133929, val-acc: 0.93500000, val_loss: 0.21625903
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 295.27244282, sen-loss: 27.26576459, dom-loss: 78.19100738, src-aux-loss: 95.19878960, tar-aux-loss: 94.61688197
Epoch: [18 ] train-acc: 0.91535714, dom-acc: 0.45321429, val-acc: 0.93250000, val_loss: 0.20750740
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 292.53247905, sen-loss: 26.80486114, dom-loss: 77.80334997, src-aux-loss: 94.66376287, tar-aux-loss: 93.26050490
Epoch: [19 ] train-acc: 0.91660714, dom-acc: 0.45428571, val-acc: 0.93250000, val_loss: 0.20536590
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 292.73585200, sen-loss: 26.38989234, dom-loss: 77.97949088, src-aux-loss: 94.14234036, tar-aux-loss: 94.22413081
Epoch: [20 ] train-acc: 0.91678571, dom-acc: 0.45660714, val-acc: 0.92500000, val_loss: 0.20438001
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 290.97214866, sen-loss: 26.05536850, dom-loss: 77.67057025, src-aux-loss: 93.71906143, tar-aux-loss: 93.52714866
Epoch: [21 ] train-acc: 0.91964286, dom-acc: 0.46491071, val-acc: 0.92750000, val_loss: 0.20335685
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 290.37193656, sen-loss: 25.67930014, dom-loss: 77.69503659, src-aux-loss: 93.20500135, tar-aux-loss: 93.79259920
Epoch: [22 ] train-acc: 0.92250000, dom-acc: 0.47973214, val-acc: 0.93000000, val_loss: 0.20179626
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 287.61000752, sen-loss: 25.39407498, dom-loss: 77.55511254, src-aux-loss: 92.58974671, tar-aux-loss: 92.07107335
Epoch: [23 ] train-acc: 0.91982143, dom-acc: 0.47660714, val-acc: 0.92500000, val_loss: 0.20185162
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 287.60634470, sen-loss: 25.25650665, dom-loss: 77.29738408, src-aux-loss: 92.39462113, tar-aux-loss: 92.65783280
Epoch: [24 ] train-acc: 0.92321429, dom-acc: 0.46455357, val-acc: 0.92500000, val_loss: 0.20046711
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 287.35235357, sen-loss: 24.77702333, dom-loss: 77.44736284, src-aux-loss: 92.03517932, tar-aux-loss: 93.09278810
Epoch: [25 ] train-acc: 0.92696429, dom-acc: 0.47330357, val-acc: 0.93500000, val_loss: 0.20045008
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 285.94450974, sen-loss: 24.35239253, dom-loss: 77.37854058, src-aux-loss: 91.83000207, tar-aux-loss: 92.38357359
Epoch: [26 ] train-acc: 0.92232143, dom-acc: 0.46008929, val-acc: 0.93750000, val_loss: 0.20561278
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 286.47404146, sen-loss: 24.09691516, dom-loss: 77.46260744, src-aux-loss: 91.54369718, tar-aux-loss: 93.37082124
Epoch: [27 ] train-acc: 0.92732143, dom-acc: 0.47946429, val-acc: 0.93500000, val_loss: 0.19967306
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 284.37081981, sen-loss: 23.81412973, dom-loss: 77.39659202, src-aux-loss: 90.73014545, tar-aux-loss: 92.42995268
Epoch: [28 ] train-acc: 0.92767857, dom-acc: 0.47133929, val-acc: 0.92750000, val_loss: 0.19744256
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 283.90523934, sen-loss: 23.47385926, dom-loss: 77.43791306, src-aux-loss: 90.44703490, tar-aux-loss: 92.54643059
Epoch: [29 ] train-acc: 0.92875000, dom-acc: 0.47687500, val-acc: 0.93000000, val_loss: 0.19819996
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 282.26172543, sen-loss: 23.23859591, dom-loss: 77.61114734, src-aux-loss: 90.11366367, tar-aux-loss: 91.29831851
Epoch: [30 ] train-acc: 0.92875000, dom-acc: 0.46339286, val-acc: 0.93500000, val_loss: 0.19996764
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 283.55733705, sen-loss: 22.91108201, dom-loss: 77.61322087, src-aux-loss: 89.76210189, tar-aux-loss: 93.27093112
Epoch: [31 ] train-acc: 0.92767857, dom-acc: 0.44687500, val-acc: 0.93250000, val_loss: 0.20558129
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 282.18152475, sen-loss: 22.60956568, dom-loss: 77.83735430, src-aux-loss: 89.44553912, tar-aux-loss: 92.28906506
Epoch: [32 ] train-acc: 0.93267857, dom-acc: 0.45187500, val-acc: 0.93500000, val_loss: 0.19869995
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 280.62476230, sen-loss: 22.34484481, dom-loss: 77.84502381, src-aux-loss: 88.99587703, tar-aux-loss: 91.43901694
Epoch: [33 ] train-acc: 0.93392857, dom-acc: 0.46053571, val-acc: 0.93750000, val_loss: 0.19669756
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 279.98114157, sen-loss: 21.96220819, dom-loss: 77.95618027, src-aux-loss: 88.57268631, tar-aux-loss: 91.49006736
Epoch: [34 ] train-acc: 0.93464286, dom-acc: 0.45008929, val-acc: 0.93500000, val_loss: 0.19606799
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 281.03816605, sen-loss: 21.71262524, dom-loss: 78.07690525, src-aux-loss: 88.26608151, tar-aux-loss: 92.98255366
Epoch: [35 ] train-acc: 0.93446429, dom-acc: 0.45348214, val-acc: 0.93250000, val_loss: 0.19669713
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 278.89505553, sen-loss: 21.59034524, dom-loss: 78.08123231, src-aux-loss: 87.73480916, tar-aux-loss: 91.48867017
Epoch: [36 ] train-acc: 0.93750000, dom-acc: 0.44366071, val-acc: 0.93250000, val_loss: 0.19634637
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 278.97086215, sen-loss: 21.19371232, dom-loss: 78.19029242, src-aux-loss: 87.44720209, tar-aux-loss: 92.13965690
Epoch: [37 ] train-acc: 0.93785714, dom-acc: 0.45473214, val-acc: 0.93750000, val_loss: 0.19725688
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 277.35121012, sen-loss: 20.86766074, dom-loss: 78.06286049, src-aux-loss: 87.22531199, tar-aux-loss: 91.19537747
Epoch: [38 ] train-acc: 0.93821429, dom-acc: 0.43464286, val-acc: 0.93500000, val_loss: 0.19985938
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 276.30880928, sen-loss: 20.74608472, dom-loss: 78.03862286, src-aux-loss: 86.77205181, tar-aux-loss: 90.75204951
Epoch: [39 ] train-acc: 0.93892857, dom-acc: 0.44696429, val-acc: 0.93500000, val_loss: 0.19687772
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_dvd_HATN.ckpt
Best Epoch: [ 34] best val accuracy: 0.00000000 best val loss: 0.19606799
Testing accuracy: 0.83866667
./work/attentions/electronics_dvd_train_HATN.txt
./work/attentions/electronics_dvd_test_HATN.txt
loading data...
source domain:  electronics target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 13856
vocab-size:  49470
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'happy', 'solid', 'fantastic', 'awesome', 'outstanding', 'perfectly', 'satisfied', 'fine', 'recommended', 'worth', 'love', 'reliable', 'inexpensive', 'comfortable', 'durable', 'quick', 'low', 'pleased', 'cool', 'impressive', 'old', 'decent', 'nice', 'greatest', 'amazing', 'reasonable', 'recommend', 'superb', 'impressed', 'quickly', 'much', 'high', 'slick', 'arrived', 'useful', 'loves', 'enough', 'awsome', 'effective', 'clear', 'lightweight', 'well']
['returned', 'poor', 'return', 'worst', 'disappointed', 'disappointing', 'useless', 'frustrating', 'horrible', 'stopped', 'bad', 'returning', 'terrible', 'poorly', 'wrong', 'cheap', 'failed', 'broke', 'awful', 'unreliable', 'worthless', 'unacceptable', 'hard', 'lasted', 'overpriced', 'beware', 'quit', 'died', 'ridiculous', 'misleading', 'sad', 'defective', 'impossible', 'unusable', 'waste', 'difficult', 'save', 'uncomfortable', 'wasted', 'unhappy', 'weak', 'ok', 'stay', 'lousy', 'slow', 'lose', 'trying', 'goes', 'sucks', 'tried', 'went', 'false', 'average', 'cheaply', 'dead', 'broken', 'flaky', 'flawed', 'true', 'mediocre', 'annoying']
max  story size: 129
mean story size: 6
max  sentence size: 440
mean sentence size: 15
max memory size: 20
5600 400 6000 23009 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 391.87669468, sen-loss: 75.69529778, dom-loss: 79.19028258, src-aux-loss: 128.19124293, tar-aux-loss: 108.79987234
Epoch: [1  ] train-acc: 0.69267857, dom-acc: 0.71366071, val-acc: 0.71000000, val_loss: 0.63435906
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 363.62479258, sen-loss: 66.65589100, dom-loss: 76.12388581, src-aux-loss: 119.33798963, tar-aux-loss: 101.50702709
Epoch: [2  ] train-acc: 0.77642857, dom-acc: 0.78910714, val-acc: 0.79500000, val_loss: 0.54406416
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 341.77174854, sen-loss: 55.69805360, dom-loss: 75.24532700, src-aux-loss: 114.13820517, tar-aux-loss: 96.69016463
Epoch: [3  ] train-acc: 0.81035714, dom-acc: 0.77473214, val-acc: 0.84750000, val_loss: 0.42645520
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 325.54322505, sen-loss: 47.09582630, dom-loss: 74.82967424, src-aux-loss: 109.83001316, tar-aux-loss: 93.78770953
Epoch: [4  ] train-acc: 0.83625000, dom-acc: 0.73901786, val-acc: 0.86750000, val_loss: 0.36880755
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 317.29611325, sen-loss: 42.65518427, dom-loss: 74.71632862, src-aux-loss: 107.33569562, tar-aux-loss: 92.58890682
Epoch: [5  ] train-acc: 0.85160714, dom-acc: 0.74375000, val-acc: 0.86500000, val_loss: 0.34551945
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 310.84588909, sen-loss: 39.57887432, dom-loss: 74.57534778, src-aux-loss: 105.24499303, tar-aux-loss: 91.44667536
Epoch: [6  ] train-acc: 0.86589286, dom-acc: 0.73000000, val-acc: 0.89500000, val_loss: 0.30358177
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 307.18085575, sen-loss: 37.15832669, dom-loss: 74.68288213, src-aux-loss: 103.77021509, tar-aux-loss: 91.56943363
Epoch: [7  ] train-acc: 0.87357143, dom-acc: 0.73526786, val-acc: 0.89750000, val_loss: 0.28375483
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 302.04825544, sen-loss: 35.20167093, dom-loss: 74.79917586, src-aux-loss: 102.94040799, tar-aux-loss: 89.10700154
Epoch: [8  ] train-acc: 0.88339286, dom-acc: 0.73785714, val-acc: 0.89750000, val_loss: 0.27253491
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 298.81547165, sen-loss: 34.05241609, dom-loss: 75.06287837, src-aux-loss: 101.27440774, tar-aux-loss: 88.42576796
Epoch: [9  ] train-acc: 0.88107143, dom-acc: 0.71607143, val-acc: 0.91250000, val_loss: 0.25451279
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 297.38563728, sen-loss: 32.65783118, dom-loss: 75.03625101, src-aux-loss: 100.09655672, tar-aux-loss: 89.59499753
Epoch: [10 ] train-acc: 0.88535714, dom-acc: 0.70616071, val-acc: 0.91250000, val_loss: 0.24169225
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 293.84898043, sen-loss: 31.60176891, dom-loss: 75.44857663, src-aux-loss: 99.49303085, tar-aux-loss: 87.30560374
Epoch: [11 ] train-acc: 0.89785714, dom-acc: 0.72035714, val-acc: 0.92250000, val_loss: 0.23456454
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 294.40338111, sen-loss: 30.94721057, dom-loss: 75.50419164, src-aux-loss: 98.56497043, tar-aux-loss: 89.38700753
Epoch: [12 ] train-acc: 0.88428571, dom-acc: 0.68910714, val-acc: 0.91250000, val_loss: 0.23619217
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 291.95166731, sen-loss: 29.98807524, dom-loss: 76.04199064, src-aux-loss: 98.02996725, tar-aux-loss: 87.89163512
Epoch: [13 ] train-acc: 0.90285714, dom-acc: 0.71107143, val-acc: 0.93750000, val_loss: 0.22314794
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 290.26513433, sen-loss: 29.60108922, dom-loss: 76.08946103, src-aux-loss: 97.22232544, tar-aux-loss: 87.35225850
Epoch: [14 ] train-acc: 0.90500000, dom-acc: 0.70375000, val-acc: 0.92750000, val_loss: 0.21547052
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 289.15487194, sen-loss: 28.86291824, dom-loss: 76.31325233, src-aux-loss: 96.61880153, tar-aux-loss: 87.35989809
Epoch: [15 ] train-acc: 0.90678571, dom-acc: 0.69330357, val-acc: 0.94000000, val_loss: 0.21694638
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 286.30710316, sen-loss: 28.45059873, dom-loss: 76.43173271, src-aux-loss: 96.23318774, tar-aux-loss: 85.19158447
Epoch: [16 ] train-acc: 0.90803571, dom-acc: 0.69696429, val-acc: 0.93500000, val_loss: 0.21314648
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 286.87619519, sen-loss: 27.96183057, dom-loss: 76.73585612, src-aux-loss: 95.60508615, tar-aux-loss: 86.57342213
Epoch: [17 ] train-acc: 0.91107143, dom-acc: 0.68955357, val-acc: 0.94250000, val_loss: 0.21280263
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 287.41689038, sen-loss: 27.56844649, dom-loss: 77.07850718, src-aux-loss: 95.03319609, tar-aux-loss: 87.73674047
Epoch: [18 ] train-acc: 0.91285714, dom-acc: 0.67875000, val-acc: 0.93500000, val_loss: 0.20408040
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 283.89488530, sen-loss: 27.03860027, dom-loss: 77.24414831, src-aux-loss: 94.39521092, tar-aux-loss: 85.21692848
Epoch: [19 ] train-acc: 0.91303571, dom-acc: 0.66696429, val-acc: 0.93500000, val_loss: 0.20093425
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 283.26915073, sen-loss: 26.62781511, dom-loss: 77.08970165, src-aux-loss: 94.05346704, tar-aux-loss: 85.49816644
Epoch: [20 ] train-acc: 0.91285714, dom-acc: 0.66232143, val-acc: 0.92750000, val_loss: 0.20018388
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 284.51931667, sen-loss: 26.28781269, dom-loss: 77.57626629, src-aux-loss: 93.56355339, tar-aux-loss: 87.09168297
Epoch: [21 ] train-acc: 0.91660714, dom-acc: 0.64973214, val-acc: 0.93500000, val_loss: 0.19808380
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 281.94918609, sen-loss: 26.00499850, dom-loss: 77.54919952, src-aux-loss: 93.10140252, tar-aux-loss: 85.29358470
Epoch: [22 ] train-acc: 0.91732143, dom-acc: 0.65651786, val-acc: 0.94750000, val_loss: 0.19951935
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 281.12539816, sen-loss: 25.60513323, dom-loss: 77.53189540, src-aux-loss: 92.80038589, tar-aux-loss: 85.18798435
Epoch: [23 ] train-acc: 0.91607143, dom-acc: 0.64294643, val-acc: 0.93250000, val_loss: 0.19601256
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 279.12541175, sen-loss: 25.36889970, dom-loss: 77.88328987, src-aux-loss: 92.33396792, tar-aux-loss: 83.53925329
Epoch: [24 ] train-acc: 0.92071429, dom-acc: 0.64098214, val-acc: 0.94500000, val_loss: 0.19435276
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 280.41725898, sen-loss: 24.98623062, dom-loss: 77.80725908, src-aux-loss: 92.09747845, tar-aux-loss: 85.52628928
Epoch: [25 ] train-acc: 0.92142857, dom-acc: 0.64892857, val-acc: 0.94000000, val_loss: 0.19443363
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 279.82024121, sen-loss: 24.57689530, dom-loss: 77.94393098, src-aux-loss: 91.80816746, tar-aux-loss: 85.49124610
Epoch: [26 ] train-acc: 0.92196429, dom-acc: 0.65392857, val-acc: 0.94250000, val_loss: 0.19648547
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 278.23846936, sen-loss: 24.32338170, dom-loss: 77.72817320, src-aux-loss: 91.39821976, tar-aux-loss: 84.78869599
Epoch: [27 ] train-acc: 0.92464286, dom-acc: 0.65142857, val-acc: 0.94250000, val_loss: 0.19037609
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 277.67510891, sen-loss: 24.09395862, dom-loss: 78.10049599, src-aux-loss: 90.93414098, tar-aux-loss: 84.54651278
Epoch: [28 ] train-acc: 0.92535714, dom-acc: 0.64437500, val-acc: 0.94000000, val_loss: 0.18969490
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 276.30022049, sen-loss: 23.70197815, dom-loss: 77.91558558, src-aux-loss: 90.46824098, tar-aux-loss: 84.21441579
Epoch: [29 ] train-acc: 0.92535714, dom-acc: 0.63741071, val-acc: 0.94000000, val_loss: 0.18874820
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 276.08175778, sen-loss: 23.39923205, dom-loss: 77.93722534, src-aux-loss: 90.11219496, tar-aux-loss: 84.63310558
Epoch: [30 ] train-acc: 0.92750000, dom-acc: 0.64437500, val-acc: 0.94000000, val_loss: 0.18806235
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 275.21469021, sen-loss: 23.13427706, dom-loss: 77.90966785, src-aux-loss: 89.90647441, tar-aux-loss: 84.26427054
Epoch: [31 ] train-acc: 0.93089286, dom-acc: 0.64964286, val-acc: 0.94000000, val_loss: 0.18984035
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 275.87907362, sen-loss: 22.89419916, dom-loss: 77.89338207, src-aux-loss: 89.57773775, tar-aux-loss: 85.51375467
Epoch: [32 ] train-acc: 0.93107143, dom-acc: 0.64017857, val-acc: 0.94000000, val_loss: 0.18730561
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 271.93673134, sen-loss: 22.59983721, dom-loss: 77.90076953, src-aux-loss: 89.18234533, tar-aux-loss: 82.25377864
Epoch: [33 ] train-acc: 0.93392857, dom-acc: 0.64080357, val-acc: 0.94000000, val_loss: 0.18642457
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 272.93840384, sen-loss: 22.23785514, dom-loss: 77.80438650, src-aux-loss: 88.67555130, tar-aux-loss: 84.22061163
Epoch: [34 ] train-acc: 0.93214286, dom-acc: 0.64303571, val-acc: 0.93750000, val_loss: 0.18463984
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 273.38751602, sen-loss: 21.99368975, dom-loss: 77.83077216, src-aux-loss: 88.44822705, tar-aux-loss: 85.11482787
Epoch: [35 ] train-acc: 0.93071429, dom-acc: 0.63848214, val-acc: 0.93750000, val_loss: 0.18556221
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 270.55434752, sen-loss: 21.98680549, dom-loss: 77.68111128, src-aux-loss: 87.92839295, tar-aux-loss: 82.95803893
Epoch: [36 ] train-acc: 0.93464286, dom-acc: 0.63812500, val-acc: 0.93750000, val_loss: 0.18304768
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 271.34672761, sen-loss: 21.50991227, dom-loss: 77.59225667, src-aux-loss: 87.49861598, tar-aux-loss: 84.74594450
Epoch: [37 ] train-acc: 0.93642857, dom-acc: 0.65008929, val-acc: 0.93750000, val_loss: 0.18393898
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 270.63381505, sen-loss: 21.14247611, dom-loss: 77.70375192, src-aux-loss: 87.27023506, tar-aux-loss: 84.51735133
Epoch: [38 ] train-acc: 0.93714286, dom-acc: 0.64589286, val-acc: 0.93750000, val_loss: 0.18362151
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 268.14519286, sen-loss: 21.05456640, dom-loss: 77.57891685, src-aux-loss: 86.89703506, tar-aux-loss: 82.61467552
Epoch: [39 ] train-acc: 0.93821429, dom-acc: 0.64383929, val-acc: 0.93750000, val_loss: 0.18222922
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 268.92322254, sen-loss: 20.60642969, dom-loss: 77.39679545, src-aux-loss: 86.47236580, tar-aux-loss: 84.44763148
Epoch: [40 ] train-acc: 0.94107143, dom-acc: 0.65437500, val-acc: 0.93500000, val_loss: 0.18514054
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 267.30667424, sen-loss: 20.50062515, dom-loss: 77.33160365, src-aux-loss: 86.05593276, tar-aux-loss: 83.41851443
Epoch: [41 ] train-acc: 0.93785714, dom-acc: 0.67169643, val-acc: 0.93500000, val_loss: 0.19072728
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 265.39293694, sen-loss: 20.19071259, dom-loss: 77.45003104, src-aux-loss: 85.62198132, tar-aux-loss: 82.13021165
Epoch: [42 ] train-acc: 0.93928571, dom-acc: 0.64348214, val-acc: 0.94500000, val_loss: 0.18344599
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 267.04788542, sen-loss: 19.97724738, dom-loss: 77.34195334, src-aux-loss: 85.35577443, tar-aux-loss: 84.37291020
Epoch: [43 ] train-acc: 0.94125000, dom-acc: 0.64232143, val-acc: 0.94750000, val_loss: 0.18246076
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [44 ] loss: 264.02634788, sen-loss: 19.68455017, dom-loss: 77.23968560, src-aux-loss: 85.02328807, tar-aux-loss: 82.07882297
Epoch: [44 ] train-acc: 0.94464286, dom-acc: 0.64660714, val-acc: 0.94250000, val_loss: 0.18042961
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [45 ] loss: 265.13776159, sen-loss: 19.62623749, dom-loss: 77.11819547, src-aux-loss: 84.59261882, tar-aux-loss: 83.80071008
Epoch: [45 ] train-acc: 0.94571429, dom-acc: 0.65125000, val-acc: 0.93500000, val_loss: 0.18416028
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [46 ] loss: 263.19094181, sen-loss: 19.16761474, dom-loss: 77.12550974, src-aux-loss: 84.28673980, tar-aux-loss: 82.61107773
Epoch: [46 ] train-acc: 0.94446429, dom-acc: 0.65473214, val-acc: 0.94000000, val_loss: 0.18093921
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [47 ] loss: 263.66123605, sen-loss: 18.87166348, dom-loss: 77.10487545, src-aux-loss: 83.80100775, tar-aux-loss: 83.88368815
Epoch: [47 ] train-acc: 0.94267857, dom-acc: 0.65776786, val-acc: 0.94000000, val_loss: 0.19163585
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [48 ] loss: 262.50190687, sen-loss: 18.66613096, dom-loss: 77.14853734, src-aux-loss: 83.36732012, tar-aux-loss: 83.31991732
Epoch: [48 ] train-acc: 0.94732143, dom-acc: 0.64937500, val-acc: 0.94250000, val_loss: 0.17973006
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [49 ] loss: 261.27731085, sen-loss: 18.40041136, dom-loss: 77.01304990, src-aux-loss: 82.92819083, tar-aux-loss: 82.93565774
Epoch: [49 ] train-acc: 0.94964286, dom-acc: 0.66642857, val-acc: 0.94250000, val_loss: 0.18259457
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [50 ] loss: 260.35820389, sen-loss: 18.24370942, dom-loss: 76.97618616, src-aux-loss: 82.36789262, tar-aux-loss: 82.77041608
Epoch: [50 ] train-acc: 0.94910714, dom-acc: 0.65464286, val-acc: 0.94250000, val_loss: 0.18813029
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [51 ] loss: 259.48332787, sen-loss: 18.04326957, dom-loss: 77.01539457, src-aux-loss: 81.93658251, tar-aux-loss: 82.48808151
Epoch: [51 ] train-acc: 0.95071429, dom-acc: 0.65473214, val-acc: 0.94250000, val_loss: 0.18367353
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [52 ] loss: 260.19806170, sen-loss: 17.76388610, dom-loss: 77.07714230, src-aux-loss: 81.63593364, tar-aux-loss: 83.72110093
Epoch: [52 ] train-acc: 0.95285714, dom-acc: 0.64812500, val-acc: 0.94250000, val_loss: 0.18601453
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [53 ] loss: 257.52109075, sen-loss: 17.45786914, dom-loss: 77.02303880, src-aux-loss: 80.97297996, tar-aux-loss: 82.06720346
Epoch: [53 ] train-acc: 0.95321429, dom-acc: 0.65687500, val-acc: 0.94000000, val_loss: 0.18524125
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_kitchen_HATN.ckpt
Best Epoch: [ 48] best val accuracy: 0.00000000 best val loss: 0.17973006
Testing accuracy: 0.90116667
./work/attentions/electronics_kitchen_train_HATN.txt
./work/attentions/electronics_kitchen_test_HATN.txt
loading data...
source domain:  electronics target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  17009 30180
vocab-size:  83059
['great', 'good', 'excellent', 'easy', 'best', 'perfect', 'works', 'solid', 'outstanding', 'awesome', 'happy', 'fantastic', 'satisfied', 'worth', 'perfectly', 'recommended', 'durable', 'reliable', 'fine', 'love', 'inexpensive', 'pleased', 'old', 'arrived', 'comfortable', 'impressive', 'decent', 'useful', 'amazing', 'expected', 'sturdy', 'cool', 'superb', 'awsome', 'supposed', 'nice', 'greatest', 'reasonable', 'quick', 'recommend', 'effective', 'high', 'slick', 'well']
['returned', 'return', 'poor', 'worst', 'stopped', 'disappointing', 'useless', 'frustrating', 'disappointed', 'horrible', 'failed', 'bad', 'wrong', 'returning', 'awful', 'lasted', 'poorly', 'terrible', 'unreliable', 'worthless', 'broke', 'unacceptable', 'cheap', 'hard', 'overpriced', 'beware', 'expensive', 'quit', 'ridiculous', 'misleading', 'died', 'unusable', 'impossible', 'sad', 'slow', 'uncomfortable', 'unhappy', 'save', 'ok', 'short', 'defective', 'low', 'wasted', 'went', 'lousy', 'waste', 'difficult', 'broken', 'annoying', 'goes', 'scratched', 'sucks', 'crashed', 'average', 'cheaply', 'unable', 'lose', 'back', 'dead', 'flaky', 'flawed', 'mediocre', 'worse', 'stay']
max  story size: 129
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
5600 400 6000 23009 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 405.20387983, sen-loss: 75.79848951, dom-loss: 78.99656087, src-aux-loss: 129.07708204, tar-aux-loss: 121.33174700
Epoch: [1  ] train-acc: 0.69017857, dom-acc: 0.80169643, val-acc: 0.70250000, val_loss: 0.63667738
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 374.74154234, sen-loss: 66.87073433, dom-loss: 74.88304186, src-aux-loss: 120.55174786, tar-aux-loss: 112.43601793
Epoch: [2  ] train-acc: 0.77625000, dom-acc: 0.81160714, val-acc: 0.79000000, val_loss: 0.54598635
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 353.27158356, sen-loss: 55.72646141, dom-loss: 73.41035849, src-aux-loss: 116.00778002, tar-aux-loss: 108.12698263
Epoch: [3  ] train-acc: 0.80964286, dom-acc: 0.63589286, val-acc: 0.83250000, val_loss: 0.42658323
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 338.89491010, sen-loss: 47.42321053, dom-loss: 73.28488445, src-aux-loss: 112.61685753, tar-aux-loss: 105.56995732
Epoch: [4  ] train-acc: 0.83553571, dom-acc: 0.59705357, val-acc: 0.86250000, val_loss: 0.36961591
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 329.99276280, sen-loss: 42.79035333, dom-loss: 73.94165474, src-aux-loss: 109.81317586, tar-aux-loss: 103.44757843
Epoch: [5  ] train-acc: 0.84982143, dom-acc: 0.59250000, val-acc: 0.86000000, val_loss: 0.34706321
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 324.87279940, sen-loss: 39.57982117, dom-loss: 74.48907918, src-aux-loss: 108.21573758, tar-aux-loss: 102.58815992
Epoch: [6  ] train-acc: 0.86928571, dom-acc: 0.56767857, val-acc: 0.89500000, val_loss: 0.30228335
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 321.49310970, sen-loss: 36.92208628, dom-loss: 74.95799702, src-aux-loss: 106.56848782, tar-aux-loss: 103.04453760
Epoch: [7  ] train-acc: 0.87732143, dom-acc: 0.53767857, val-acc: 0.90000000, val_loss: 0.28238323
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 316.50176048, sen-loss: 35.11366607, dom-loss: 75.80953366, src-aux-loss: 105.59297854, tar-aux-loss: 99.98558193
Epoch: [8  ] train-acc: 0.88500000, dom-acc: 0.49562500, val-acc: 0.90750000, val_loss: 0.26998463
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 314.73041725, sen-loss: 33.82937463, dom-loss: 76.54103363, src-aux-loss: 104.08662611, tar-aux-loss: 100.27338123
Epoch: [9  ] train-acc: 0.88892857, dom-acc: 0.48937500, val-acc: 0.90750000, val_loss: 0.24965414
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 312.62507606, sen-loss: 32.29967743, dom-loss: 77.37191051, src-aux-loss: 103.06041491, tar-aux-loss: 99.89307213
Epoch: [10 ] train-acc: 0.88553571, dom-acc: 0.47848214, val-acc: 0.90750000, val_loss: 0.24186747
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 310.13717151, sen-loss: 31.46616964, dom-loss: 77.95490772, src-aux-loss: 102.34387434, tar-aux-loss: 98.37221938
Epoch: [11 ] train-acc: 0.90017857, dom-acc: 0.43473214, val-acc: 0.91750000, val_loss: 0.23256159
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 311.54032612, sen-loss: 30.79538947, dom-loss: 78.32939231, src-aux-loss: 102.02164567, tar-aux-loss: 100.39389938
Epoch: [12 ] train-acc: 0.88607143, dom-acc: 0.44607143, val-acc: 0.91000000, val_loss: 0.23406138
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 307.15232015, sen-loss: 29.82237126, dom-loss: 78.40837175, src-aux-loss: 101.07006639, tar-aux-loss: 97.85151166
Epoch: [13 ] train-acc: 0.90482143, dom-acc: 0.41205357, val-acc: 0.93250000, val_loss: 0.22423577
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 307.38557982, sen-loss: 29.49811620, dom-loss: 78.55691594, src-aux-loss: 100.45344287, tar-aux-loss: 98.87710649
Epoch: [14 ] train-acc: 0.90535714, dom-acc: 0.42428571, val-acc: 0.92500000, val_loss: 0.21708722
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 303.89727616, sen-loss: 28.64329269, dom-loss: 78.71119022, src-aux-loss: 99.73003578, tar-aux-loss: 96.81275588
Epoch: [15 ] train-acc: 0.90857143, dom-acc: 0.42482143, val-acc: 0.93750000, val_loss: 0.21537928
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 302.43733525, sen-loss: 28.20089380, dom-loss: 78.50906402, src-aux-loss: 99.54033887, tar-aux-loss: 96.18703711
Epoch: [16 ] train-acc: 0.91035714, dom-acc: 0.43482143, val-acc: 0.93750000, val_loss: 0.21574146
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 302.34064841, sen-loss: 27.76977715, dom-loss: 78.36037028, src-aux-loss: 98.77970105, tar-aux-loss: 97.43079895
Epoch: [17 ] train-acc: 0.91196429, dom-acc: 0.43375000, val-acc: 0.93500000, val_loss: 0.21433379
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 300.44102550, sen-loss: 27.22619427, dom-loss: 78.15568322, src-aux-loss: 97.97425091, tar-aux-loss: 97.08489621
Epoch: [18 ] train-acc: 0.91446429, dom-acc: 0.45330357, val-acc: 0.92750000, val_loss: 0.20765813
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 297.39810514, sen-loss: 26.87047511, dom-loss: 77.75873774, src-aux-loss: 97.51607937, tar-aux-loss: 95.25281346
Epoch: [19 ] train-acc: 0.91642857, dom-acc: 0.45482143, val-acc: 0.92750000, val_loss: 0.20545338
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 297.68523788, sen-loss: 26.40723657, dom-loss: 77.87740242, src-aux-loss: 97.01151860, tar-aux-loss: 96.38907945
Epoch: [20 ] train-acc: 0.91678571, dom-acc: 0.48062500, val-acc: 0.93000000, val_loss: 0.20448126
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 295.39714599, sen-loss: 25.98161793, dom-loss: 77.58606821, src-aux-loss: 96.49263364, tar-aux-loss: 95.33682567
Epoch: [21 ] train-acc: 0.91928571, dom-acc: 0.48294643, val-acc: 0.92750000, val_loss: 0.20344959
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 295.10585856, sen-loss: 25.69626873, dom-loss: 77.47132838, src-aux-loss: 96.05507207, tar-aux-loss: 95.88318902
Epoch: [22 ] train-acc: 0.91946429, dom-acc: 0.49598214, val-acc: 0.93500000, val_loss: 0.20368864
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 293.86100268, sen-loss: 25.35046398, dom-loss: 77.41435993, src-aux-loss: 95.55264819, tar-aux-loss: 95.54353040
Epoch: [23 ] train-acc: 0.92035714, dom-acc: 0.50437500, val-acc: 0.93000000, val_loss: 0.20176366
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 291.56752753, sen-loss: 25.14722733, dom-loss: 77.29880553, src-aux-loss: 95.19113678, tar-aux-loss: 93.93035901
Epoch: [24 ] train-acc: 0.92357143, dom-acc: 0.48026786, val-acc: 0.93250000, val_loss: 0.20092787
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 292.83586001, sen-loss: 24.71975400, dom-loss: 77.35914487, src-aux-loss: 94.68335378, tar-aux-loss: 96.07360721
Epoch: [25 ] train-acc: 0.92553571, dom-acc: 0.48348214, val-acc: 0.93500000, val_loss: 0.20094141
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 290.68767476, sen-loss: 24.24843481, dom-loss: 77.50782287, src-aux-loss: 94.38510633, tar-aux-loss: 94.54630947
Epoch: [26 ] train-acc: 0.92035714, dom-acc: 0.48660714, val-acc: 0.93500000, val_loss: 0.20719108
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 289.95904469, sen-loss: 24.11550270, dom-loss: 77.41755909, src-aux-loss: 94.08720481, tar-aux-loss: 94.33877772
Epoch: [27 ] train-acc: 0.92625000, dom-acc: 0.49348214, val-acc: 0.93500000, val_loss: 0.19973460
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 287.95725894, sen-loss: 23.76373466, dom-loss: 77.64703929, src-aux-loss: 93.43557823, tar-aux-loss: 93.11090630
Epoch: [28 ] train-acc: 0.92946429, dom-acc: 0.49669643, val-acc: 0.93250000, val_loss: 0.19767088
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 288.57562160, sen-loss: 23.38878590, dom-loss: 77.66194439, src-aux-loss: 93.04522908, tar-aux-loss: 94.47966206
Epoch: [29 ] train-acc: 0.93107143, dom-acc: 0.49098214, val-acc: 0.93250000, val_loss: 0.19816081
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 286.53899431, sen-loss: 23.08778475, dom-loss: 77.72060031, src-aux-loss: 92.77829492, tar-aux-loss: 92.95231646
Epoch: [30 ] train-acc: 0.93071429, dom-acc: 0.47339286, val-acc: 0.93750000, val_loss: 0.19933034
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 287.26821303, sen-loss: 22.78231335, dom-loss: 77.87023968, src-aux-loss: 92.15487134, tar-aux-loss: 94.46078908
Epoch: [31 ] train-acc: 0.92821429, dom-acc: 0.44562500, val-acc: 0.93750000, val_loss: 0.20219554
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 286.84855723, sen-loss: 22.48859517, dom-loss: 78.20658970, src-aux-loss: 91.96042013, tar-aux-loss: 94.19295108
Epoch: [32 ] train-acc: 0.93267857, dom-acc: 0.45517857, val-acc: 0.93750000, val_loss: 0.19840330
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 285.48993468, sen-loss: 22.27036519, dom-loss: 78.14435899, src-aux-loss: 91.50787264, tar-aux-loss: 93.56733608
Epoch: [33 ] train-acc: 0.93446429, dom-acc: 0.45151786, val-acc: 0.93750000, val_loss: 0.19753397
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 284.58424997, sen-loss: 21.94162965, dom-loss: 78.30578983, src-aux-loss: 90.92660153, tar-aux-loss: 93.41022795
Epoch: [34 ] train-acc: 0.93517857, dom-acc: 0.43544643, val-acc: 0.93250000, val_loss: 0.19676553
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 283.95323777, sen-loss: 21.65142138, dom-loss: 78.39873165, src-aux-loss: 90.90769243, tar-aux-loss: 92.99539202
Epoch: [35 ] train-acc: 0.93696429, dom-acc: 0.44669643, val-acc: 0.93500000, val_loss: 0.19616896
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 283.33774233, sen-loss: 21.50083739, dom-loss: 78.45745772, src-aux-loss: 90.02714628, tar-aux-loss: 93.35230231
Epoch: [36 ] train-acc: 0.93785714, dom-acc: 0.44214286, val-acc: 0.93250000, val_loss: 0.19731744
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 283.28474593, sen-loss: 21.11245878, dom-loss: 78.37600762, src-aux-loss: 89.80400068, tar-aux-loss: 93.99227875
Epoch: [37 ] train-acc: 0.93821429, dom-acc: 0.44160714, val-acc: 0.93750000, val_loss: 0.19749425
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 282.54920435, sen-loss: 20.80405853, dom-loss: 78.31925082, src-aux-loss: 89.58360672, tar-aux-loss: 93.84228736
Epoch: [38 ] train-acc: 0.93714286, dom-acc: 0.43357143, val-acc: 0.93750000, val_loss: 0.19806083
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 280.20122457, sen-loss: 20.65401037, dom-loss: 78.31639344, src-aux-loss: 88.93718117, tar-aux-loss: 92.29364014
Epoch: [39 ] train-acc: 0.94089286, dom-acc: 0.44741071, val-acc: 0.93750000, val_loss: 0.19824538
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 279.28785872, sen-loss: 20.27346289, dom-loss: 78.14056391, src-aux-loss: 88.58035570, tar-aux-loss: 92.29347652
Epoch: [40 ] train-acc: 0.94000000, dom-acc: 0.44955357, val-acc: 0.93750000, val_loss: 0.19806400
---------------------------------------------------

Successfully load model from save path: ./work/models/electronics_video_HATN.ckpt
Best Epoch: [ 35] best val accuracy: 0.00000000 best val loss: 0.19616896
Testing accuracy: 0.83450000
./work/attentions/electronics_video_train_HATN.txt
./work/attentions/electronics_video_test_HATN.txt
loading data...
source domain:  kitchen target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 9750
vocab-size:  78006
['great', 'love', 'easy', 'best', 'excellent', 'good', 'perfect', 'happy', 'pleased', 'loves', 'wonderful', 'amazing', 'nice', 'satisfied', 'fantastic', 'awesome', 'well', 'useful', 'solid', 'favorite', 'attractive', 'fabulous', 'quick', 'perfectly', 'outstanding', 'elegant', 'fine', 'durable', 'comfortable', 'sturdy', 'impressed', 'terrific', 'reasonable', 'works', 'incredible', 'exceptional', 'safe', 'decent', 'gorgeous', 'pretty', 'easiest', 'lovely', 'beautiful', 'unbelievable', 'ultimate']
['disappointed', 'poor', 'disappointing', 'terrible', 'broke', 'worst', 'poorly', 'bad', 'useless', 'horrible', 'returned', 'defective', 'wrong', 'broken', 'impossible', 'flimsy', 'awful', 'cheap', 'dissapointed', 'worked', 'difficult', 'stopped', 'dangerous', 'ruined', 'worse', 'hard', 'worthless', 'overpriced', 'frustrating', 'disapointed', 'misleading', 'sad', 'return', 'stuck', 'lasted', 'unacceptable', 'unusable', 'unhappy', 'failed', 'flawed', 'uneven', 'lousy', 'ridiculous', 'ugly', 'refunded', 'messy', 'hate', 'leaky', 'wasted', 'unfortunately', 'false', 'expensive', 'dissappointed', 'overrated']
max  story size: 189
mean story size: 7
max  sentence size: 702
mean sentence size: 17
max memory size: 20
5600 400 6000 19856 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78007, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 414.54378915, sen-loss: 75.65720081, dom-loss: 78.97210562, src-aux-loss: 139.00316966, tar-aux-loss: 120.91131181
Epoch: [1  ] train-acc: 0.69732143, dom-acc: 0.79321429, val-acc: 0.71000000, val_loss: 0.63189501
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 385.66354251, sen-loss: 65.95499897, dom-loss: 74.55592203, src-aux-loss: 131.23574436, tar-aux-loss: 113.91687506
Epoch: [2  ] train-acc: 0.78410714, dom-acc: 0.80241071, val-acc: 0.80500000, val_loss: 0.53008813
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 360.99679565, sen-loss: 53.58548039, dom-loss: 72.96143764, src-aux-loss: 125.19872129, tar-aux-loss: 109.25115657
Epoch: [3  ] train-acc: 0.84910714, dom-acc: 0.64008929, val-acc: 0.84500000, val_loss: 0.41192544
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 342.88048053, sen-loss: 43.10155553, dom-loss: 72.51065576, src-aux-loss: 120.77768642, tar-aux-loss: 106.49058133
Epoch: [4  ] train-acc: 0.86714286, dom-acc: 0.58598214, val-acc: 0.84750000, val_loss: 0.35664827
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 332.93135667, sen-loss: 38.01074725, dom-loss: 72.91182834, src-aux-loss: 117.78873330, tar-aux-loss: 104.22004914
Epoch: [5  ] train-acc: 0.88089286, dom-acc: 0.56250000, val-acc: 0.87000000, val_loss: 0.33596215
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 327.55600834, sen-loss: 34.43189496, dom-loss: 74.08547920, src-aux-loss: 115.14783508, tar-aux-loss: 103.89079779
Epoch: [6  ] train-acc: 0.88750000, dom-acc: 0.55839286, val-acc: 0.88500000, val_loss: 0.32641345
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 322.28685594, sen-loss: 32.11364394, dom-loss: 74.64472377, src-aux-loss: 113.36936539, tar-aux-loss: 102.15912259
Epoch: [7  ] train-acc: 0.89678571, dom-acc: 0.53553571, val-acc: 0.89500000, val_loss: 0.31246945
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 319.57440686, sen-loss: 30.46492774, dom-loss: 75.76905751, src-aux-loss: 111.81384474, tar-aux-loss: 101.52657801
Epoch: [8  ] train-acc: 0.90285714, dom-acc: 0.51330357, val-acc: 0.89250000, val_loss: 0.31129470
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 317.89579368, sen-loss: 29.22103421, dom-loss: 76.56419450, src-aux-loss: 110.63183421, tar-aux-loss: 101.47873068
Epoch: [9  ] train-acc: 0.90375000, dom-acc: 0.48955357, val-acc: 0.89750000, val_loss: 0.30165619
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 315.90029693, sen-loss: 28.23978151, dom-loss: 77.11437213, src-aux-loss: 109.42514938, tar-aux-loss: 101.12099147
Epoch: [10 ] train-acc: 0.90696429, dom-acc: 0.45839286, val-acc: 0.90250000, val_loss: 0.30653861
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 313.15369439, sen-loss: 27.55311134, dom-loss: 77.61267322, src-aux-loss: 108.83045971, tar-aux-loss: 99.15744931
Epoch: [11 ] train-acc: 0.91232143, dom-acc: 0.44294643, val-acc: 0.89750000, val_loss: 0.30389640
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 312.26893950, sen-loss: 26.90210340, dom-loss: 78.00193244, src-aux-loss: 107.64370477, tar-aux-loss: 99.72119778
Epoch: [12 ] train-acc: 0.91196429, dom-acc: 0.44964286, val-acc: 0.91250000, val_loss: 0.29134300
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 309.97852063, sen-loss: 26.27093501, dom-loss: 78.36207652, src-aux-loss: 106.77610993, tar-aux-loss: 98.56939965
Epoch: [13 ] train-acc: 0.91607143, dom-acc: 0.45714286, val-acc: 0.89500000, val_loss: 0.29858789
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 310.28367829, sen-loss: 25.88222609, dom-loss: 78.54146206, src-aux-loss: 105.86070710, tar-aux-loss: 99.99928397
Epoch: [14 ] train-acc: 0.91750000, dom-acc: 0.45357143, val-acc: 0.90250000, val_loss: 0.29474244
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 306.83448339, sen-loss: 25.31211450, dom-loss: 78.75966197, src-aux-loss: 105.41161513, tar-aux-loss: 97.35109162
Epoch: [15 ] train-acc: 0.91464286, dom-acc: 0.46803571, val-acc: 0.89500000, val_loss: 0.31043249
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 308.95315766, sen-loss: 24.98950855, dom-loss: 78.93788612, src-aux-loss: 104.42030925, tar-aux-loss: 100.60545456
Epoch: [16 ] train-acc: 0.91714286, dom-acc: 0.46026786, val-acc: 0.91750000, val_loss: 0.28423616
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 304.02867484, sen-loss: 24.56605382, dom-loss: 78.76633483, src-aux-loss: 103.90608096, tar-aux-loss: 96.79020506
Epoch: [17 ] train-acc: 0.92107143, dom-acc: 0.45410714, val-acc: 0.90750000, val_loss: 0.29056051
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 304.06520653, sen-loss: 24.23037675, dom-loss: 78.93956238, src-aux-loss: 103.03199387, tar-aux-loss: 97.86327475
Epoch: [18 ] train-acc: 0.92214286, dom-acc: 0.45687500, val-acc: 0.91750000, val_loss: 0.28540507
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 303.23649120, sen-loss: 23.86305533, dom-loss: 78.88700259, src-aux-loss: 102.77570921, tar-aux-loss: 97.71072263
Epoch: [19 ] train-acc: 0.92321429, dom-acc: 0.46544643, val-acc: 0.91750000, val_loss: 0.28278416
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 302.20877337, sen-loss: 23.53819001, dom-loss: 78.65949154, src-aux-loss: 101.96350491, tar-aux-loss: 98.04758680
Epoch: [20 ] train-acc: 0.92571429, dom-acc: 0.46651786, val-acc: 0.92000000, val_loss: 0.28350258
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 300.11654449, sen-loss: 23.31500775, dom-loss: 78.68238181, src-aux-loss: 101.62889779, tar-aux-loss: 96.49025738
Epoch: [21 ] train-acc: 0.92660714, dom-acc: 0.47616071, val-acc: 0.91000000, val_loss: 0.29873630
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 298.91010952, sen-loss: 22.89626259, dom-loss: 78.48396939, src-aux-loss: 100.77817273, tar-aux-loss: 96.75170422
Epoch: [22 ] train-acc: 0.92821429, dom-acc: 0.48892857, val-acc: 0.92250000, val_loss: 0.28468943
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 298.63257837, sen-loss: 22.62815145, dom-loss: 78.31938863, src-aux-loss: 100.25628245, tar-aux-loss: 97.42875516
Epoch: [23 ] train-acc: 0.92910714, dom-acc: 0.49250000, val-acc: 0.92250000, val_loss: 0.28563648
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 296.32210207, sen-loss: 22.51904910, dom-loss: 78.20416826, src-aux-loss: 99.63927966, tar-aux-loss: 95.95960373
Epoch: [24 ] train-acc: 0.92642857, dom-acc: 0.48714286, val-acc: 0.92500000, val_loss: 0.28070295
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 295.82411790, sen-loss: 22.28379646, dom-loss: 78.18623370, src-aux-loss: 99.09323514, tar-aux-loss: 96.26085389
Epoch: [25 ] train-acc: 0.93035714, dom-acc: 0.49714286, val-acc: 0.93000000, val_loss: 0.27973866
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 295.54375315, sen-loss: 22.22588668, dom-loss: 77.90191394, src-aux-loss: 98.76612955, tar-aux-loss: 96.64982218
Epoch: [26 ] train-acc: 0.92857143, dom-acc: 0.49955357, val-acc: 0.92500000, val_loss: 0.27618784
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 294.83259654, sen-loss: 21.94577320, dom-loss: 77.83002406, src-aux-loss: 98.25426352, tar-aux-loss: 96.80253392
Epoch: [27 ] train-acc: 0.93071429, dom-acc: 0.51732143, val-acc: 0.92750000, val_loss: 0.27824789
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 293.93168950, sen-loss: 21.71104178, dom-loss: 77.70846313, src-aux-loss: 97.71627420, tar-aux-loss: 96.79590976
Epoch: [28 ] train-acc: 0.93303571, dom-acc: 0.50053571, val-acc: 0.92500000, val_loss: 0.27670175
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 291.79380035, sen-loss: 21.35619722, dom-loss: 77.68240339, src-aux-loss: 97.55194271, tar-aux-loss: 95.20325613
Epoch: [29 ] train-acc: 0.93196429, dom-acc: 0.51455357, val-acc: 0.92250000, val_loss: 0.27141863
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 290.63842988, sen-loss: 21.22981773, dom-loss: 77.44222611, src-aux-loss: 96.94481498, tar-aux-loss: 95.02157259
Epoch: [30 ] train-acc: 0.93178571, dom-acc: 0.52678571, val-acc: 0.92750000, val_loss: 0.27325231
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 290.51830363, sen-loss: 21.05414055, dom-loss: 77.42057508, src-aux-loss: 96.34846425, tar-aux-loss: 95.69512475
Epoch: [31 ] train-acc: 0.93375000, dom-acc: 0.49669643, val-acc: 0.91000000, val_loss: 0.29379442
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 290.24828649, sen-loss: 20.85521204, dom-loss: 77.32092404, src-aux-loss: 95.87219429, tar-aux-loss: 96.19995654
Epoch: [32 ] train-acc: 0.93660714, dom-acc: 0.49544643, val-acc: 0.92000000, val_loss: 0.28393796
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 287.86929941, sen-loss: 20.60958666, dom-loss: 77.32035077, src-aux-loss: 95.32662725, tar-aux-loss: 94.61273533
Epoch: [33 ] train-acc: 0.93625000, dom-acc: 0.49383929, val-acc: 0.91500000, val_loss: 0.28857476
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 288.16359878, sen-loss: 20.37300956, dom-loss: 77.33113742, src-aux-loss: 94.73156488, tar-aux-loss: 95.72788435
Epoch: [34 ] train-acc: 0.93660714, dom-acc: 0.52339286, val-acc: 0.92750000, val_loss: 0.27398086
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_books_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.27141863
Testing accuracy: 0.84833333
./work/attentions/kitchen_books_train_HATN.txt
./work/attentions/kitchen_books_test_HATN.txt
loading data...
source domain:  kitchen target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 11843
vocab-size:  80685
['great', 'easy', 'love', 'best', 'good', 'excellent', 'perfect', 'happy', 'pleased', 'loves', 'wonderful', 'nice', 'fantastic', 'amazing', 'works', 'satisfied', 'awesome', 'solid', 'useful', 'attractive', 'perfectly', 'fabulous', 'favorite', 'comfortable', 'sturdy', 'outstanding', 'quick', 'durable', 'terrific', 'fine', 'elegant', 'safe', 'simple', 'highly', 'impressed', 'gorgeous', 'easiest', 'incredible', 'exceptional', 'reasonable', 'decent', 'fast', 'lovely', 'pretty', 'loved', 'beautiful', 'unbelievable', 'well', 'ultimate']
['disappointed', 'poor', 'disappointing', 'terrible', 'worst', 'poorly', 'broke', 'useless', 'bad', 'horrible', 'returned', 'defective', 'wrong', 'broken', 'flimsy', 'awful', 'impossible', 'cheap', 'dissapointed', 'difficult', 'worked', 'stopped', 'dangerous', 'hard', 'stuck', 'worthless', 'ruined', 'worse', 'overpriced', 'frustrating', 'disapointed', 'misleading', 'sad', 'unhappy', 'fell', 'lasted', 'unacceptable', 'unusable', 'failed', 'flawed', 'uneven', 'sorry', 'rusted', 'lousy', 'ridiculous', 'frustrated', 'ugly', 'messy', 'hate', 'uncomfortable', 'leaky', 'better', 'return', 'went', 'wasted', 'expensive', 'overrated']
max  story size: 226
mean story size: 7
max  sentence size: 783
mean sentence size: 17
max memory size: 20
5600 400 6000 19856 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(80686, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 409.54335856, sen-loss: 75.58868611, dom-loss: 78.78887367, src-aux-loss: 136.67297632, tar-aux-loss: 118.49282390
Epoch: [1  ] train-acc: 0.71125000, dom-acc: 0.79205357, val-acc: 0.72500000, val_loss: 0.63025826
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 380.57829142, sen-loss: 65.91979775, dom-loss: 74.67661071, src-aux-loss: 128.71407461, tar-aux-loss: 111.26780868
Epoch: [2  ] train-acc: 0.78928571, dom-acc: 0.79803571, val-acc: 0.81500000, val_loss: 0.53000498
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 356.23676419, sen-loss: 53.88310170, dom-loss: 73.34074223, src-aux-loss: 123.18066853, tar-aux-loss: 105.83225167
Epoch: [3  ] train-acc: 0.84732143, dom-acc: 0.65250000, val-acc: 0.84750000, val_loss: 0.41395026
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 339.89715219, sen-loss: 43.22215870, dom-loss: 72.99247277, src-aux-loss: 118.69312090, tar-aux-loss: 104.98940092
Epoch: [4  ] train-acc: 0.86196429, dom-acc: 0.59089286, val-acc: 0.86000000, val_loss: 0.35808507
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 329.16353202, sen-loss: 38.14335373, dom-loss: 73.19356126, src-aux-loss: 115.91588360, tar-aux-loss: 101.91073394
Epoch: [5  ] train-acc: 0.87946429, dom-acc: 0.55794643, val-acc: 0.87000000, val_loss: 0.33672509
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 325.07436848, sen-loss: 34.78557359, dom-loss: 74.42569101, src-aux-loss: 113.70774060, tar-aux-loss: 102.15536213
Epoch: [6  ] train-acc: 0.88607143, dom-acc: 0.53803571, val-acc: 0.88250000, val_loss: 0.33307976
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 318.72272372, sen-loss: 32.47252005, dom-loss: 75.00562465, src-aux-loss: 111.60923856, tar-aux-loss: 99.63534212
Epoch: [7  ] train-acc: 0.89571429, dom-acc: 0.53008929, val-acc: 0.89750000, val_loss: 0.31474519
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 315.50021815, sen-loss: 30.73044807, dom-loss: 75.89183152, src-aux-loss: 110.08600020, tar-aux-loss: 98.79193747
Epoch: [8  ] train-acc: 0.89946429, dom-acc: 0.50455357, val-acc: 0.89750000, val_loss: 0.30951053
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 315.04175949, sen-loss: 29.57242955, dom-loss: 76.37992787, src-aux-loss: 109.00685996, tar-aux-loss: 100.08254069
Epoch: [9  ] train-acc: 0.90375000, dom-acc: 0.48366071, val-acc: 0.90250000, val_loss: 0.30125052
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 311.91533279, sen-loss: 28.51707025, dom-loss: 77.03303355, src-aux-loss: 108.06432146, tar-aux-loss: 98.30090755
Epoch: [10 ] train-acc: 0.90642857, dom-acc: 0.46696429, val-acc: 0.90250000, val_loss: 0.30512252
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 308.66706634, sen-loss: 27.74567748, dom-loss: 77.21651077, src-aux-loss: 107.13134456, tar-aux-loss: 96.57353157
Epoch: [11 ] train-acc: 0.90982143, dom-acc: 0.45142857, val-acc: 0.90000000, val_loss: 0.30157289
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 307.86153793, sen-loss: 27.10649508, dom-loss: 77.59207672, src-aux-loss: 106.15684527, tar-aux-loss: 97.00612009
Epoch: [12 ] train-acc: 0.90964286, dom-acc: 0.45651786, val-acc: 0.91250000, val_loss: 0.29090881
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 307.68159318, sen-loss: 26.37995706, dom-loss: 77.74145466, src-aux-loss: 105.19129753, tar-aux-loss: 98.36888421
Epoch: [13 ] train-acc: 0.91321429, dom-acc: 0.44205357, val-acc: 0.90500000, val_loss: 0.29652759
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 305.11201191, sen-loss: 26.02742010, dom-loss: 77.88668483, src-aux-loss: 104.39178860, tar-aux-loss: 96.80611819
Epoch: [14 ] train-acc: 0.91589286, dom-acc: 0.45294643, val-acc: 0.90500000, val_loss: 0.29198620
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 302.57933211, sen-loss: 25.47635616, dom-loss: 77.96851432, src-aux-loss: 103.92470604, tar-aux-loss: 95.20975506
Epoch: [15 ] train-acc: 0.91035714, dom-acc: 0.43767857, val-acc: 0.89000000, val_loss: 0.32114717
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 303.30722070, sen-loss: 25.16333984, dom-loss: 78.14347929, src-aux-loss: 103.00968462, tar-aux-loss: 96.99071735
Epoch: [16 ] train-acc: 0.91660714, dom-acc: 0.44919643, val-acc: 0.91000000, val_loss: 0.28593314
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 300.18455839, sen-loss: 24.68429468, dom-loss: 77.98824793, src-aux-loss: 102.31675386, tar-aux-loss: 95.19526225
Epoch: [17 ] train-acc: 0.92035714, dom-acc: 0.44205357, val-acc: 0.91500000, val_loss: 0.28969854
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 299.71240091, sen-loss: 24.41801199, dom-loss: 78.01037073, src-aux-loss: 101.55415320, tar-aux-loss: 95.72986335
Epoch: [18 ] train-acc: 0.91982143, dom-acc: 0.45750000, val-acc: 0.91500000, val_loss: 0.29041782
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 298.99323106, sen-loss: 24.05750161, dom-loss: 78.12288600, src-aux-loss: 100.97185093, tar-aux-loss: 95.84099221
Epoch: [19 ] train-acc: 0.92232143, dom-acc: 0.44964286, val-acc: 0.91500000, val_loss: 0.28386888
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 297.67763281, sen-loss: 23.67835650, dom-loss: 77.96375287, src-aux-loss: 100.54452789, tar-aux-loss: 95.49099481
Epoch: [20 ] train-acc: 0.92392857, dom-acc: 0.44508929, val-acc: 0.91750000, val_loss: 0.28479254
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 297.15003347, sen-loss: 23.42775655, dom-loss: 78.23070818, src-aux-loss: 100.24260581, tar-aux-loss: 95.24896300
Epoch: [21 ] train-acc: 0.92321429, dom-acc: 0.44642857, val-acc: 0.91500000, val_loss: 0.30152437
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 294.94565201, sen-loss: 23.08504545, dom-loss: 78.13495260, src-aux-loss: 99.46474290, tar-aux-loss: 94.26091015
Epoch: [22 ] train-acc: 0.92500000, dom-acc: 0.45687500, val-acc: 0.91750000, val_loss: 0.28934166
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 295.03688741, sen-loss: 22.74883583, dom-loss: 78.15935409, src-aux-loss: 98.93842995, tar-aux-loss: 95.19026691
Epoch: [23 ] train-acc: 0.92500000, dom-acc: 0.45991071, val-acc: 0.91500000, val_loss: 0.28295803
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 293.34407401, sen-loss: 22.59892018, dom-loss: 77.91351509, src-aux-loss: 98.16203934, tar-aux-loss: 94.66960096
Epoch: [24 ] train-acc: 0.92625000, dom-acc: 0.45044643, val-acc: 0.92000000, val_loss: 0.28548682
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 293.13819337, sen-loss: 22.35098402, dom-loss: 78.18684542, src-aux-loss: 97.71860427, tar-aux-loss: 94.88175970
Epoch: [25 ] train-acc: 0.92607143, dom-acc: 0.45116071, val-acc: 0.91500000, val_loss: 0.28164646
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 291.29944634, sen-loss: 22.27464123, dom-loss: 78.04076511, src-aux-loss: 97.42078727, tar-aux-loss: 93.56325352
Epoch: [26 ] train-acc: 0.92714286, dom-acc: 0.44633929, val-acc: 0.91250000, val_loss: 0.28008604
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 291.98666549, sen-loss: 22.00519586, dom-loss: 78.11674148, src-aux-loss: 96.68848300, tar-aux-loss: 95.17624444
Epoch: [27 ] train-acc: 0.92857143, dom-acc: 0.45089286, val-acc: 0.91500000, val_loss: 0.28800356
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 290.46914220, sen-loss: 21.80145147, dom-loss: 78.02115351, src-aux-loss: 96.48024035, tar-aux-loss: 94.16629660
Epoch: [28 ] train-acc: 0.92982143, dom-acc: 0.44803571, val-acc: 0.92250000, val_loss: 0.27927050
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 288.83319759, sen-loss: 21.43229094, dom-loss: 78.18053991, src-aux-loss: 96.03709835, tar-aux-loss: 93.18326837
Epoch: [29 ] train-acc: 0.93000000, dom-acc: 0.45357143, val-acc: 0.92500000, val_loss: 0.27835348
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 288.13105178, sen-loss: 21.26584183, dom-loss: 77.98990017, src-aux-loss: 95.59237677, tar-aux-loss: 93.28293228
Epoch: [30 ] train-acc: 0.93160714, dom-acc: 0.45223214, val-acc: 0.92250000, val_loss: 0.28155807
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 287.76747727, sen-loss: 21.13250922, dom-loss: 77.94354790, src-aux-loss: 95.10472935, tar-aux-loss: 93.58669102
Epoch: [31 ] train-acc: 0.93053571, dom-acc: 0.43401786, val-acc: 0.91500000, val_loss: 0.30390736
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 288.07248950, sen-loss: 20.95188208, dom-loss: 77.82945246, src-aux-loss: 94.64340645, tar-aux-loss: 94.64774990
Epoch: [32 ] train-acc: 0.93482143, dom-acc: 0.44937500, val-acc: 0.92250000, val_loss: 0.28986633
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 284.40020847, sen-loss: 20.64301166, dom-loss: 77.67701340, src-aux-loss: 94.06519872, tar-aux-loss: 92.01498741
Epoch: [33 ] train-acc: 0.93589286, dom-acc: 0.45517857, val-acc: 0.92000000, val_loss: 0.29114166
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 286.09749365, sen-loss: 20.40052304, dom-loss: 77.90504944, src-aux-loss: 93.58079290, tar-aux-loss: 94.21112883
Epoch: [34 ] train-acc: 0.93482143, dom-acc: 0.46767857, val-acc: 0.92500000, val_loss: 0.27842727
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_dvd_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.27835348
Testing accuracy: 0.84733333
./work/attentions/kitchen_dvd_train_HATN.txt
./work/attentions/kitchen_dvd_test_HATN.txt
loading data...
source domain:  kitchen target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 17009
vocab-size:  49470
['great', 'love', 'easy', 'best', 'good', 'excellent', 'perfect', 'happy', 'pleased', 'wonderful', 'loves', 'nice', 'satisfied', 'awesome', 'well', 'fantastic', 'amazing', 'attractive', 'useful', 'solid', 'fabulous', 'favorite', 'comfortable', 'quick', 'perfectly', 'highly', 'outstanding', 'fine', 'works', 'incredible', 'elegant', 'terrific', 'impressed', 'gorgeous', 'exceptional', 'reasonable', 'decent', 'durable', 'lovely', 'easiest', 'loved', 'beautiful', 'ultimate']
['disappointed', 'poor', 'disappointing', 'terrible', 'worst', 'poorly', 'broke', 'bad', 'useless', 'horrible', 'returned', 'wrong', 'defective', 'broken', 'cheap', 'awful', 'flimsy', 'worked', 'impossible', 'dissapointed', 'hard', 'difficult', 'dangerous', 'ruined', 'worthless', 'stopped', 'overpriced', 'frustrating', 'disapointed', 'misleading', 'sad', 'stuck', 'unhappy', 'worse', 'ridiculous', 'failed', 'frustrated', 'flawed', 'lasted', 'hate', 'uneven', 'sorry', 'unacceptable', 'still', 'return', 'went', 'lousy', 'unusable', 'expensive', 'ugly', 'refunded', 'dissappointed', 'leaky', 'rusted', 'better', 'unfortunately', 'overrated']
max  story size: 129
mean story size: 6
max  sentence size: 440
mean sentence size: 15
max memory size: 20
5600 400 6000 19856 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(49471, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 411.66848636, sen-loss: 75.60756809, dom-loss: 79.26163059, src-aux-loss: 137.83090174, tar-aux-loss: 118.96838570
Epoch: [1  ] train-acc: 0.69553571, dom-acc: 0.67705357, val-acc: 0.70750000, val_loss: 0.63104707
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 384.65987110, sen-loss: 66.00077349, dom-loss: 76.24012393, src-aux-loss: 130.18315601, tar-aux-loss: 112.23581600
Epoch: [2  ] train-acc: 0.78660714, dom-acc: 0.68732143, val-acc: 0.81000000, val_loss: 0.53182888
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 360.58416224, sen-loss: 53.72092110, dom-loss: 75.40188032, src-aux-loss: 124.59849215, tar-aux-loss: 106.86287004
Epoch: [3  ] train-acc: 0.84392857, dom-acc: 0.60071429, val-acc: 0.83500000, val_loss: 0.41066217
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 343.28693581, sen-loss: 42.99226131, dom-loss: 75.37077755, src-aux-loss: 120.13485801, tar-aux-loss: 104.78904027
Epoch: [4  ] train-acc: 0.86375000, dom-acc: 0.65901786, val-acc: 0.85250000, val_loss: 0.35639518
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 332.26294136, sen-loss: 37.78699854, dom-loss: 75.29360276, src-aux-loss: 116.79428846, tar-aux-loss: 102.38804919
Epoch: [5  ] train-acc: 0.88303571, dom-acc: 0.65535714, val-acc: 0.87250000, val_loss: 0.33708432
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 325.80120945, sen-loss: 34.42461143, dom-loss: 75.44474983, src-aux-loss: 114.55860102, tar-aux-loss: 101.37324822
Epoch: [6  ] train-acc: 0.88910714, dom-acc: 0.70723214, val-acc: 0.88000000, val_loss: 0.32585981
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 319.72275305, sen-loss: 32.10655785, dom-loss: 75.51971471, src-aux-loss: 112.52563667, tar-aux-loss: 99.57084256
Epoch: [7  ] train-acc: 0.89571429, dom-acc: 0.66598214, val-acc: 0.89000000, val_loss: 0.31488648
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 315.76423693, sen-loss: 30.36079660, dom-loss: 75.89892507, src-aux-loss: 110.96338099, tar-aux-loss: 98.54113436
Epoch: [8  ] train-acc: 0.89982143, dom-acc: 0.68767857, val-acc: 0.89500000, val_loss: 0.31416401
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 314.53466606, sen-loss: 29.26607195, dom-loss: 76.11041915, src-aux-loss: 110.12392288, tar-aux-loss: 99.03425246
Epoch: [9  ] train-acc: 0.90553571, dom-acc: 0.65651786, val-acc: 0.89750000, val_loss: 0.30228251
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 311.29043770, sen-loss: 28.17319287, dom-loss: 76.45368767, src-aux-loss: 108.66079605, tar-aux-loss: 98.00276005
Epoch: [10 ] train-acc: 0.90928571, dom-acc: 0.62678571, val-acc: 0.90250000, val_loss: 0.30261782
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 308.56348181, sen-loss: 27.55679474, dom-loss: 76.55473882, src-aux-loss: 108.10465723, tar-aux-loss: 96.34729153
Epoch: [11 ] train-acc: 0.91017857, dom-acc: 0.61473214, val-acc: 0.90250000, val_loss: 0.30036193
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 307.81802845, sen-loss: 26.88508105, dom-loss: 76.68335336, src-aux-loss: 107.19662911, tar-aux-loss: 97.05296624
Epoch: [12 ] train-acc: 0.91267857, dom-acc: 0.65482143, val-acc: 0.90750000, val_loss: 0.29415780
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 305.78965974, sen-loss: 26.22090176, dom-loss: 76.80301976, src-aux-loss: 106.23204434, tar-aux-loss: 96.53369576
Epoch: [13 ] train-acc: 0.91517857, dom-acc: 0.67080357, val-acc: 0.90750000, val_loss: 0.30118680
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 304.77037764, sen-loss: 25.78601234, dom-loss: 77.09428334, src-aux-loss: 105.37236667, tar-aux-loss: 96.51771653
Epoch: [14 ] train-acc: 0.91678571, dom-acc: 0.65428571, val-acc: 0.91000000, val_loss: 0.29194525
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 302.35841084, sen-loss: 25.21116731, dom-loss: 77.16690320, src-aux-loss: 104.74068660, tar-aux-loss: 95.23965496
Epoch: [15 ] train-acc: 0.91357143, dom-acc: 0.64892857, val-acc: 0.90000000, val_loss: 0.31318802
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 302.74468136, sen-loss: 24.89250159, dom-loss: 77.35233754, src-aux-loss: 103.98453492, tar-aux-loss: 96.51530701
Epoch: [16 ] train-acc: 0.91732143, dom-acc: 0.65232143, val-acc: 0.91750000, val_loss: 0.28629649
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 299.91049099, sen-loss: 24.45578510, dom-loss: 77.44941157, src-aux-loss: 103.42230356, tar-aux-loss: 94.58299088
Epoch: [17 ] train-acc: 0.92160714, dom-acc: 0.60482143, val-acc: 0.92250000, val_loss: 0.29166886
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 299.02938628, sen-loss: 24.16633166, dom-loss: 77.50042361, src-aux-loss: 102.52144277, tar-aux-loss: 94.84119010
Epoch: [18 ] train-acc: 0.92267857, dom-acc: 0.63491071, val-acc: 0.92250000, val_loss: 0.28921643
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 298.18188572, sen-loss: 23.81253012, dom-loss: 77.63200343, src-aux-loss: 101.91578561, tar-aux-loss: 94.82156742
Epoch: [19 ] train-acc: 0.92375000, dom-acc: 0.60687500, val-acc: 0.92000000, val_loss: 0.28350067
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 297.18274879, sen-loss: 23.41245911, dom-loss: 77.78183931, src-aux-loss: 101.45825863, tar-aux-loss: 94.53018999
Epoch: [20 ] train-acc: 0.92571429, dom-acc: 0.59098214, val-acc: 0.91750000, val_loss: 0.29233578
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 296.51348257, sen-loss: 23.17531187, dom-loss: 77.77886838, src-aux-loss: 101.26200193, tar-aux-loss: 94.29729933
Epoch: [21 ] train-acc: 0.92571429, dom-acc: 0.60455357, val-acc: 0.91250000, val_loss: 0.29792294
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 295.84010458, sen-loss: 22.81658796, dom-loss: 77.90780997, src-aux-loss: 100.32599640, tar-aux-loss: 94.78971058
Epoch: [22 ] train-acc: 0.92732143, dom-acc: 0.59062500, val-acc: 0.92250000, val_loss: 0.29250026
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 294.23917389, sen-loss: 22.44344475, dom-loss: 77.72370392, src-aux-loss: 99.89426953, tar-aux-loss: 94.17775637
Epoch: [23 ] train-acc: 0.92750000, dom-acc: 0.62169643, val-acc: 0.92750000, val_loss: 0.28356957
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 292.73297644, sen-loss: 22.35757729, dom-loss: 78.00702161, src-aux-loss: 99.27363396, tar-aux-loss: 93.09474158
Epoch: [24 ] train-acc: 0.92875000, dom-acc: 0.61035714, val-acc: 0.92500000, val_loss: 0.28366265
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_electronics_HATN.ckpt
Best Epoch: [ 19] best val accuracy: 0.00000000 best val loss: 0.28350067
Testing accuracy: 0.89083333
./work/attentions/kitchen_electronics_train_HATN.txt
./work/attentions/kitchen_electronics_test_HATN.txt
loading data...
source domain:  kitchen target domain: video
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  13856 30180
vocab-size:  78115
['great', 'love', 'easy', 'best', 'excellent', 'good', 'perfect', 'happy', 'pleased', 'loves', 'wonderful', 'nice', 'satisfied', 'amazing', 'fantastic', 'awesome', 'well', 'useful', 'favorite', 'solid', 'attractive', 'fabulous', 'perfectly', 'outstanding', 'comfortable', 'sturdy', 'quick', 'durable', 'fine', 'reasonable', 'works', 'elegant', 'impressed', 'easiest', 'incredible', 'terrific', 'exceptional', 'safe', 'simple', 'decent', 'lovely', 'pretty', 'gorgeous', 'ultimate']
['disappointed', 'poor', 'disappointing', 'terrible', 'broke', 'poorly', 'worst', 'useless', 'bad', 'horrible', 'returned', 'defective', 'wrong', 'broken', 'flimsy', 'cheap', 'awful', 'impossible', 'dissapointed', 'difficult', 'worked', 'dangerous', 'stopped', 'hard', 'worthless', 'stuck', 'ruined', 'overpriced', 'frustrating', 'worse', 'return', 'disapointed', 'sad', 'misleading', 'lasted', 'unusable', 'unhappy', 'failed', 'flawed', 'hate', 'uneven', 'rusted', 'unacceptable', 'lousy', 'ridiculous', 'ugly', 'refunded', 'messy', 'leaky', 'sucks', 'wasted', 'unfortunately', 'inconsistent', 'frustrated', 'expensive', 'overrated', 'fell']
max  story size: 104
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
5600 400 6000 19856 30180
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 413.48593616, sen-loss: 75.66148388, dom-loss: 78.86704040, src-aux-loss: 139.73736846, tar-aux-loss: 119.22004372
Epoch: [1  ] train-acc: 0.70339286, dom-acc: 0.78812500, val-acc: 0.72000000, val_loss: 0.63126123
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 386.46341467, sen-loss: 65.99176463, dom-loss: 74.64843690, src-aux-loss: 131.86117893, tar-aux-loss: 113.96203256
Epoch: [2  ] train-acc: 0.78607143, dom-acc: 0.79294643, val-acc: 0.80750000, val_loss: 0.53107369
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 363.01277041, sen-loss: 53.86649567, dom-loss: 73.10659766, src-aux-loss: 126.31262445, tar-aux-loss: 109.72705251
Epoch: [3  ] train-acc: 0.84714286, dom-acc: 0.64732143, val-acc: 0.84500000, val_loss: 0.41372076
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 344.21812224, sen-loss: 43.45400587, dom-loss: 73.09866208, src-aux-loss: 121.74791133, tar-aux-loss: 105.91754079
Epoch: [4  ] train-acc: 0.86857143, dom-acc: 0.59642857, val-acc: 0.84750000, val_loss: 0.35976806
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 333.31578279, sen-loss: 38.09061223, dom-loss: 73.65554649, src-aux-loss: 118.58412725, tar-aux-loss: 102.98549837
Epoch: [5  ] train-acc: 0.87875000, dom-acc: 0.57875000, val-acc: 0.87000000, val_loss: 0.33598584
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 328.43451643, sen-loss: 34.76540166, dom-loss: 74.65968889, src-aux-loss: 116.01282644, tar-aux-loss: 102.99660015
Epoch: [6  ] train-acc: 0.88642857, dom-acc: 0.55312500, val-acc: 0.88500000, val_loss: 0.32657403
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 322.95030141, sen-loss: 32.33624919, dom-loss: 75.02174538, src-aux-loss: 114.06964910, tar-aux-loss: 101.52265835
Epoch: [7  ] train-acc: 0.89642857, dom-acc: 0.52973214, val-acc: 0.89500000, val_loss: 0.31088701
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 320.48018146, sen-loss: 30.73600934, dom-loss: 76.11587089, src-aux-loss: 112.52846563, tar-aux-loss: 101.09983599
Epoch: [8  ] train-acc: 0.90017857, dom-acc: 0.49955357, val-acc: 0.89750000, val_loss: 0.31136259
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 319.57490373, sen-loss: 29.53775664, dom-loss: 76.61280298, src-aux-loss: 111.45405662, tar-aux-loss: 101.97028732
Epoch: [9  ] train-acc: 0.90500000, dom-acc: 0.47848214, val-acc: 0.90500000, val_loss: 0.30033010
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 315.95094657, sen-loss: 28.41366465, dom-loss: 77.19969696, src-aux-loss: 110.10372043, tar-aux-loss: 100.23386419
Epoch: [10 ] train-acc: 0.90696429, dom-acc: 0.46410714, val-acc: 0.90000000, val_loss: 0.30243257
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 312.36729884, sen-loss: 27.60762782, dom-loss: 77.51735270, src-aux-loss: 109.17272025, tar-aux-loss: 98.06959808
Epoch: [11 ] train-acc: 0.90857143, dom-acc: 0.44883929, val-acc: 0.89750000, val_loss: 0.30676305
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 312.01554585, sen-loss: 27.06670615, dom-loss: 77.67595851, src-aux-loss: 108.43032789, tar-aux-loss: 98.84255266
Epoch: [12 ] train-acc: 0.91053571, dom-acc: 0.45785714, val-acc: 0.91750000, val_loss: 0.28825521
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 309.78152037, sen-loss: 26.37343171, dom-loss: 77.75108796, src-aux-loss: 107.38562167, tar-aux-loss: 98.27138072
Epoch: [13 ] train-acc: 0.91267857, dom-acc: 0.44696429, val-acc: 0.90500000, val_loss: 0.29145011
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 308.81968236, sen-loss: 25.99721873, dom-loss: 77.84857726, src-aux-loss: 106.40399480, tar-aux-loss: 98.56989211
Epoch: [14 ] train-acc: 0.91464286, dom-acc: 0.46482143, val-acc: 0.90750000, val_loss: 0.28747427
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 306.83354449, sen-loss: 25.44921472, dom-loss: 78.00534087, src-aux-loss: 106.04892254, tar-aux-loss: 97.33006424
Epoch: [15 ] train-acc: 0.91232143, dom-acc: 0.46116071, val-acc: 0.89000000, val_loss: 0.31390426
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 305.65995836, sen-loss: 25.08530277, dom-loss: 78.03704751, src-aux-loss: 105.08359891, tar-aux-loss: 97.45400864
Epoch: [16 ] train-acc: 0.91589286, dom-acc: 0.46705357, val-acc: 0.92000000, val_loss: 0.27993214
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 303.38502383, sen-loss: 24.71305338, dom-loss: 77.91826004, src-aux-loss: 104.33744210, tar-aux-loss: 96.41626853
Epoch: [17 ] train-acc: 0.92125000, dom-acc: 0.44875000, val-acc: 0.91250000, val_loss: 0.28497437
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 303.29528952, sen-loss: 24.37354580, dom-loss: 77.95932347, src-aux-loss: 103.59147370, tar-aux-loss: 97.37094623
Epoch: [18 ] train-acc: 0.92178571, dom-acc: 0.46258929, val-acc: 0.91000000, val_loss: 0.28339434
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 301.05928588, sen-loss: 24.00094287, dom-loss: 78.04342234, src-aux-loss: 103.11305296, tar-aux-loss: 95.90186828
Epoch: [19 ] train-acc: 0.92321429, dom-acc: 0.45375000, val-acc: 0.91250000, val_loss: 0.27817452
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 300.36728024, sen-loss: 23.66127420, dom-loss: 77.95743823, src-aux-loss: 102.41166121, tar-aux-loss: 96.33690631
Epoch: [20 ] train-acc: 0.92482143, dom-acc: 0.46098214, val-acc: 0.91500000, val_loss: 0.28269920
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 298.60929060, sen-loss: 23.42461805, dom-loss: 78.20553517, src-aux-loss: 101.97916478, tar-aux-loss: 94.99997377
Epoch: [21 ] train-acc: 0.92553571, dom-acc: 0.45562500, val-acc: 0.91000000, val_loss: 0.29065874
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 297.65062642, sen-loss: 23.02667286, dom-loss: 78.13475102, src-aux-loss: 101.26488745, tar-aux-loss: 95.22431427
Epoch: [22 ] train-acc: 0.92607143, dom-acc: 0.46000000, val-acc: 0.91500000, val_loss: 0.28223926
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 298.04027152, sen-loss: 22.64873093, dom-loss: 78.19809175, src-aux-loss: 100.68425173, tar-aux-loss: 96.50919634
Epoch: [23 ] train-acc: 0.92875000, dom-acc: 0.46160714, val-acc: 0.91750000, val_loss: 0.27836531
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 295.32075834, sen-loss: 22.62246860, dom-loss: 78.16852033, src-aux-loss: 99.95632285, tar-aux-loss: 94.57344592
Epoch: [24 ] train-acc: 0.92750000, dom-acc: 0.45857143, val-acc: 0.91500000, val_loss: 0.27409887
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 295.10248613, sen-loss: 22.37492821, dom-loss: 78.16794920, src-aux-loss: 99.33653486, tar-aux-loss: 95.22307336
Epoch: [25 ] train-acc: 0.92892857, dom-acc: 0.44955357, val-acc: 0.92000000, val_loss: 0.27130869
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 292.80564404, sen-loss: 22.28170376, dom-loss: 78.28805429, src-aux-loss: 98.96733540, tar-aux-loss: 93.26855022
Epoch: [26 ] train-acc: 0.92750000, dom-acc: 0.45669643, val-acc: 0.91500000, val_loss: 0.26867488
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 293.97632551, sen-loss: 21.96119714, dom-loss: 78.15961945, src-aux-loss: 98.36281902, tar-aux-loss: 95.49269021
Epoch: [27 ] train-acc: 0.93178571, dom-acc: 0.45000000, val-acc: 0.91500000, val_loss: 0.27415347
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 291.38469958, sen-loss: 21.76497121, dom-loss: 78.29101670, src-aux-loss: 97.81098968, tar-aux-loss: 93.51772153
Epoch: [28 ] train-acc: 0.93178571, dom-acc: 0.45428571, val-acc: 0.92250000, val_loss: 0.26992357
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 291.32100224, sen-loss: 21.39051514, dom-loss: 78.35253525, src-aux-loss: 97.47648752, tar-aux-loss: 94.10146332
Epoch: [29 ] train-acc: 0.92928571, dom-acc: 0.46348214, val-acc: 0.92500000, val_loss: 0.26452002
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 289.30807900, sen-loss: 21.28722462, dom-loss: 78.08278370, src-aux-loss: 96.92587590, tar-aux-loss: 93.01219559
Epoch: [30 ] train-acc: 0.93375000, dom-acc: 0.45892857, val-acc: 0.91750000, val_loss: 0.26673031
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 289.83589792, sen-loss: 21.15292424, dom-loss: 78.05046678, src-aux-loss: 96.35140932, tar-aux-loss: 94.28109729
Epoch: [31 ] train-acc: 0.93267857, dom-acc: 0.43035714, val-acc: 0.91250000, val_loss: 0.28485143
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 288.46148753, sen-loss: 20.90796885, dom-loss: 77.99550325, src-aux-loss: 95.83286899, tar-aux-loss: 93.72514564
Epoch: [32 ] train-acc: 0.93571429, dom-acc: 0.44491071, val-acc: 0.91500000, val_loss: 0.27630258
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 286.71840692, sen-loss: 20.69988270, dom-loss: 77.80727613, src-aux-loss: 95.23114687, tar-aux-loss: 92.98010129
Epoch: [33 ] train-acc: 0.93607143, dom-acc: 0.45151786, val-acc: 0.91750000, val_loss: 0.27433440
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 287.66497326, sen-loss: 20.45876489, dom-loss: 77.97919089, src-aux-loss: 94.65622544, tar-aux-loss: 94.57079190
Epoch: [34 ] train-acc: 0.93714286, dom-acc: 0.45776786, val-acc: 0.92500000, val_loss: 0.26738748
---------------------------------------------------

Successfully load model from save path: ./work/models/kitchen_video_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26452002
Testing accuracy: 0.84033333
./work/attentions/kitchen_video_train_HATN.txt
./work/attentions/kitchen_video_test_HATN.txt
loading data...
source domain:  video target domain: books
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 9750
vocab-size:  98084
['best', 'funny', 'great', 'good', 'excellent', 'enjoyable', 'classic', 'amazing', 'favorite', 'beautifully', 'awesome', 'brilliant', 'love', 'perfect', 'loved', 'hilarious', 'superb', 'fantastic', 'underrated', 'informative', 'entertaining', 'funniest', 'wonderful', 'enjoyed', 'beautiful', 'solid', 'greatest', 'easy', 'brilliantly', 'nice', 'fabulous', 'sad', 'recommended', 'epic', 'un', 'clever', 'interesting', 'terrific', 'fine', 'poignant', 'incredible', 'cool', 'recommend', 'outstanding', 'masterful', 'decent', 'watchable', 'memorable', 'realistic', 'hard', 'pleasant', 'liked', 'lovely', 'magnificent', 'witty']
['worst', 'horrible', 'bad', 'terrible', 'boring', 'poor', 'disappointing', 'garbled', 'dissapointing', 'bother', 'worse', 'awful', 'harmful', 'wasted', 'wrong', 'read', 'incomplete', 'better', 'disappointed', 'unfunny', 'dull', 'cuts', 'poorly', 'annoying', 'ruined', 'atrocious', 'forgettable', 'dreadful', 'low', 'waste', 'unwatchable', 'laughable', 'uninspired', 'disgusting', 'amusing', 'uncooked', 'miserable', 'weak', 'ok', 'contrived', 'sorry', 'stupidest', 'dumbest', 'pointless', 'unrealistic', 'dissapointed', 'unlikeable', 'mediocre', 'stupid', 'pretentious', 'unbearable', 'unmemorable', 'lousy', 'ridiculous', 'vapid', 'lame', 'unconvincing']
max  story size: 189
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
5600 400 6000 36180 9750
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(98085, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 416.62923241, sen-loss: 76.61726511, dom-loss: 79.39901757, src-aux-loss: 137.39146888, tar-aux-loss: 123.22148186
Epoch: [1  ] train-acc: 0.66642857, dom-acc: 0.69080357, val-acc: 0.65250000, val_loss: 0.65250838
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 386.74171996, sen-loss: 68.93357247, dom-loss: 76.45992261, src-aux-loss: 126.35019755, tar-aux-loss: 114.99802703
Epoch: [2  ] train-acc: 0.74089286, dom-acc: 0.78633929, val-acc: 0.72500000, val_loss: 0.58159345
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 365.10808969, sen-loss: 60.53264120, dom-loss: 75.62916613, src-aux-loss: 117.36170304, tar-aux-loss: 111.58458078
Epoch: [3  ] train-acc: 0.78589286, dom-acc: 0.81232143, val-acc: 0.77250000, val_loss: 0.50408065
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 347.82731533, sen-loss: 53.01377317, dom-loss: 75.17592901, src-aux-loss: 110.92299330, tar-aux-loss: 108.71461922
Epoch: [4  ] train-acc: 0.82053571, dom-acc: 0.82508929, val-acc: 0.82000000, val_loss: 0.43842283
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 334.54103827, sen-loss: 46.71296489, dom-loss: 75.05095208, src-aux-loss: 105.92048347, tar-aux-loss: 106.85663772
Epoch: [5  ] train-acc: 0.84660714, dom-acc: 0.82910714, val-acc: 0.84500000, val_loss: 0.38929650
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 325.33474970, sen-loss: 41.69242930, dom-loss: 74.81999701, src-aux-loss: 102.16695005, tar-aux-loss: 106.65537357
Epoch: [6  ] train-acc: 0.86375000, dom-acc: 0.82169643, val-acc: 0.87250000, val_loss: 0.35305786
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 316.92989230, sen-loss: 37.84365433, dom-loss: 75.04011416, src-aux-loss: 99.04253024, tar-aux-loss: 105.00359493
Epoch: [7  ] train-acc: 0.87732143, dom-acc: 0.80250000, val-acc: 0.87500000, val_loss: 0.32937396
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 310.79730844, sen-loss: 34.90374810, dom-loss: 75.05273968, src-aux-loss: 96.44563186, tar-aux-loss: 104.39518851
Epoch: [8  ] train-acc: 0.88660714, dom-acc: 0.79062500, val-acc: 0.87500000, val_loss: 0.31416169
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 306.50171804, sen-loss: 33.02138706, dom-loss: 75.23461533, src-aux-loss: 94.84370625, tar-aux-loss: 103.40201145
Epoch: [9  ] train-acc: 0.89160714, dom-acc: 0.77875000, val-acc: 0.88250000, val_loss: 0.30193090
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 302.43878078, sen-loss: 31.93064483, dom-loss: 75.59299117, src-aux-loss: 92.66744316, tar-aux-loss: 102.24770206
Epoch: [10 ] train-acc: 0.89500000, dom-acc: 0.76866071, val-acc: 0.88250000, val_loss: 0.29917616
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 299.99464440, sen-loss: 31.10187528, dom-loss: 75.66803038, src-aux-loss: 91.12569255, tar-aux-loss: 102.09904522
Epoch: [11 ] train-acc: 0.89982143, dom-acc: 0.75544643, val-acc: 0.88000000, val_loss: 0.29573312
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 296.94995332, sen-loss: 30.13529959, dom-loss: 75.77287406, src-aux-loss: 89.27458555, tar-aux-loss: 101.76719564
Epoch: [12 ] train-acc: 0.90321429, dom-acc: 0.74973214, val-acc: 0.88000000, val_loss: 0.28857750
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 294.95326591, sen-loss: 29.64181019, dom-loss: 76.02892691, src-aux-loss: 88.19502771, tar-aux-loss: 101.08750045
Epoch: [13 ] train-acc: 0.90625000, dom-acc: 0.74267857, val-acc: 0.88000000, val_loss: 0.28853780
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 292.27877951, sen-loss: 28.89779191, dom-loss: 76.26488072, src-aux-loss: 86.90983415, tar-aux-loss: 100.20627153
Epoch: [14 ] train-acc: 0.90589286, dom-acc: 0.72767857, val-acc: 0.88250000, val_loss: 0.29017127
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 291.31580710, sen-loss: 28.31362421, dom-loss: 76.25649375, src-aux-loss: 85.67767280, tar-aux-loss: 101.06801528
Epoch: [15 ] train-acc: 0.91035714, dom-acc: 0.72919643, val-acc: 0.88500000, val_loss: 0.28145885
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 288.95126295, sen-loss: 27.95474400, dom-loss: 76.63702804, src-aux-loss: 84.51890820, tar-aux-loss: 99.84058279
Epoch: [16 ] train-acc: 0.90982143, dom-acc: 0.70089286, val-acc: 0.89000000, val_loss: 0.27327469
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 287.10924959, sen-loss: 27.29522446, dom-loss: 76.72314382, src-aux-loss: 83.52757537, tar-aux-loss: 99.56330568
Epoch: [17 ] train-acc: 0.91482143, dom-acc: 0.70044643, val-acc: 0.88250000, val_loss: 0.27093846
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 285.60115194, sen-loss: 26.77075493, dom-loss: 77.02916682, src-aux-loss: 82.28390023, tar-aux-loss: 99.51733047
Epoch: [18 ] train-acc: 0.91607143, dom-acc: 0.67544643, val-acc: 0.88750000, val_loss: 0.27254152
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 283.88737273, sen-loss: 26.30071734, dom-loss: 77.00878245, src-aux-loss: 81.45832217, tar-aux-loss: 99.11955094
Epoch: [19 ] train-acc: 0.91714286, dom-acc: 0.66973214, val-acc: 0.88500000, val_loss: 0.26836976
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 283.67924166, sen-loss: 26.05663688, dom-loss: 77.35749942, src-aux-loss: 80.65277469, tar-aux-loss: 99.61233038
Epoch: [20 ] train-acc: 0.91785714, dom-acc: 0.65750000, val-acc: 0.89250000, val_loss: 0.27858850
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 280.26338649, sen-loss: 25.60641379, dom-loss: 77.11039460, src-aux-loss: 79.72514662, tar-aux-loss: 97.82142985
Epoch: [21 ] train-acc: 0.91910714, dom-acc: 0.66598214, val-acc: 0.88250000, val_loss: 0.27206427
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 279.70013332, sen-loss: 25.40460636, dom-loss: 77.25865567, src-aux-loss: 78.79927641, tar-aux-loss: 98.23759550
Epoch: [22 ] train-acc: 0.92053571, dom-acc: 0.65821429, val-acc: 0.88750000, val_loss: 0.26799858
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 277.88883829, sen-loss: 24.96157361, dom-loss: 77.18335795, src-aux-loss: 78.11581045, tar-aux-loss: 97.62809706
Epoch: [23 ] train-acc: 0.92392857, dom-acc: 0.65116071, val-acc: 0.88500000, val_loss: 0.26369503
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 279.16058779, sen-loss: 24.68198743, dom-loss: 77.61860383, src-aux-loss: 77.21666321, tar-aux-loss: 99.64333248
Epoch: [24 ] train-acc: 0.92625000, dom-acc: 0.64616071, val-acc: 0.88500000, val_loss: 0.26549202
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 274.62500072, sen-loss: 24.34012934, dom-loss: 77.54705966, src-aux-loss: 76.59440646, tar-aux-loss: 96.14340657
Epoch: [25 ] train-acc: 0.92714286, dom-acc: 0.64330357, val-acc: 0.88500000, val_loss: 0.26465848
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 275.81893253, sen-loss: 24.05918131, dom-loss: 77.61768639, src-aux-loss: 75.63358548, tar-aux-loss: 98.50847852
Epoch: [26 ] train-acc: 0.92732143, dom-acc: 0.64535714, val-acc: 0.88750000, val_loss: 0.26457787
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 273.01139355, sen-loss: 23.70014739, dom-loss: 77.55669671, src-aux-loss: 75.14015254, tar-aux-loss: 96.61439782
Epoch: [27 ] train-acc: 0.92625000, dom-acc: 0.63848214, val-acc: 0.88500000, val_loss: 0.26066428
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 271.45376396, sen-loss: 23.53901643, dom-loss: 77.59554446, src-aux-loss: 74.07514638, tar-aux-loss: 96.24405611
Epoch: [28 ] train-acc: 0.92928571, dom-acc: 0.63250000, val-acc: 0.88500000, val_loss: 0.26140329
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 271.84377551, sen-loss: 23.09441761, dom-loss: 77.65559226, src-aux-loss: 73.66890356, tar-aux-loss: 97.42486078
Epoch: [29 ] train-acc: 0.92928571, dom-acc: 0.62830357, val-acc: 0.89000000, val_loss: 0.26012874
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 270.38123441, sen-loss: 22.70320709, dom-loss: 77.54834843, src-aux-loss: 73.00477266, tar-aux-loss: 97.12490594
Epoch: [30 ] train-acc: 0.93250000, dom-acc: 0.63517857, val-acc: 0.88750000, val_loss: 0.26261750
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 269.77716684, sen-loss: 22.48785868, dom-loss: 77.60002953, src-aux-loss: 72.34205192, tar-aux-loss: 97.34722680
Epoch: [31 ] train-acc: 0.93339286, dom-acc: 0.63383929, val-acc: 0.89000000, val_loss: 0.26514226
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 268.85193539, sen-loss: 22.26232629, dom-loss: 77.77273875, src-aux-loss: 71.43526790, tar-aux-loss: 97.38160199
Epoch: [32 ] train-acc: 0.93089286, dom-acc: 0.62241071, val-acc: 0.88750000, val_loss: 0.27477434
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 266.54468822, sen-loss: 21.95792137, dom-loss: 77.48404515, src-aux-loss: 70.61662364, tar-aux-loss: 96.48609728
Epoch: [33 ] train-acc: 0.93232143, dom-acc: 0.64580357, val-acc: 0.89000000, val_loss: 0.27807072
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 264.41671681, sen-loss: 21.66853994, dom-loss: 77.58403438, src-aux-loss: 69.94270986, tar-aux-loss: 95.22143418
Epoch: [34 ] train-acc: 0.93392857, dom-acc: 0.64339286, val-acc: 0.89000000, val_loss: 0.27589503
---------------------------------------------------

Successfully load model from save path: ./work/models/video_books_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26012874
Testing accuracy: 0.87483333
./work/attentions/video_books_train_HATN.txt
./work/attentions/video_books_test_HATN.txt
loading data...
source domain:  video target domain: dvd
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 11843
vocab-size:  91852
['best', 'funny', 'great', 'good', 'excellent', 'enjoyable', 'classic', 'favorite', 'amazing', 'love', 'loved', 'beautifully', 'perfect', 'underrated', 'brilliant', 'awesome', 'superb', 'nice', 'funniest', 'hilarious', 'informative', 'greatest', 'fantastic', 'entertaining', 'brilliantly', 'sad', 'enjoyed', 'solid', 'beautiful', 'easy', 'clever', 'epic', 'fabulous', 'fine', 'un', 'wonderful', 'terrific', 'cool', 'liked', 'magnificent', 'wonderfully', 'comedic', 'superbly', 'masterful', 'decent', 'poignant', 'incredible', 'delightful', 'pleasant', 'perfectly', 'outstanding', 'funnier', 'riveting', 'happy', 'recommend', 'romantic']
['worst', 'horrible', 'bad', 'terrible', 'boring', 'poor', 'disappointing', 'garbled', 'bother', 'worse', 'dissapointing', 'awful', 'average', 'read', 'wasted', 'wrong', 'better', 'disappointed', 'unfunny', 'dull', 'poorly', 'annoying', 'unwatchable', 'forgettable', 'ruined', 'ok', 'dissapointed', 'atrocious', 'skip', 'dreadful', 'unnecessary', 'needless', 'uninspired', 'disgusting', 'uncooked', 'pretentious', 'laughable', 'pointless', 'unbelievable', 'contrived', 'saves', 'dumbest', 'sorry', 'stupidest', 'cheesy', 'low', 'hard', 'unrealistic', 'waste', 'unlikeable', 'mediocre', 'amusing', 'sappy', 'miserable', 'weak', 'unmemorable', 'lousy', 'unbearable', 'vapid', 'lame', 'stupid']
max  story size: 226
mean story size: 8
max  sentence size: 959
mean sentence size: 19
max memory size: 20
5600 400 6000 36180 11843
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(91853, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 415.33608603, sen-loss: 76.71485114, dom-loss: 79.96215230, src-aux-loss: 138.70530748, tar-aux-loss: 119.95377421
Epoch: [1  ] train-acc: 0.66160714, dom-acc: 0.46633929, val-acc: 0.66000000, val_loss: 0.65357232
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 386.63554788, sen-loss: 69.19996095, dom-loss: 78.64747739, src-aux-loss: 128.31608659, tar-aux-loss: 110.47202206
Epoch: [2  ] train-acc: 0.76214286, dom-acc: 0.47107143, val-acc: 0.73250000, val_loss: 0.58033723
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 365.11254239, sen-loss: 60.69971046, dom-loss: 78.56238490, src-aux-loss: 120.48638666, tar-aux-loss: 105.36405897
Epoch: [3  ] train-acc: 0.79714286, dom-acc: 0.48241071, val-acc: 0.81250000, val_loss: 0.50211716
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 346.97973061, sen-loss: 52.67249563, dom-loss: 78.47781849, src-aux-loss: 112.86895066, tar-aux-loss: 102.96046603
Epoch: [4  ] train-acc: 0.82428571, dom-acc: 0.49116071, val-acc: 0.81000000, val_loss: 0.43490264
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 331.08282614, sen-loss: 46.30176607, dom-loss: 78.44250059, src-aux-loss: 106.71585435, tar-aux-loss: 99.62270427
Epoch: [5  ] train-acc: 0.84464286, dom-acc: 0.52125000, val-acc: 0.85250000, val_loss: 0.38562578
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 321.60299158, sen-loss: 41.49219730, dom-loss: 78.34717661, src-aux-loss: 102.47572619, tar-aux-loss: 99.28789139
Epoch: [6  ] train-acc: 0.86267857, dom-acc: 0.59464286, val-acc: 0.87250000, val_loss: 0.35480371
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 313.96598768, sen-loss: 37.98267759, dom-loss: 78.29244870, src-aux-loss: 99.31353033, tar-aux-loss: 98.37733042
Epoch: [7  ] train-acc: 0.87625000, dom-acc: 0.61258929, val-acc: 0.87500000, val_loss: 0.33061391
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 306.99115562, sen-loss: 35.12760666, dom-loss: 78.23020685, src-aux-loss: 96.94657308, tar-aux-loss: 96.68676931
Epoch: [8  ] train-acc: 0.88964286, dom-acc: 0.62857143, val-acc: 0.87000000, val_loss: 0.31606776
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 302.22880411, sen-loss: 33.23127184, dom-loss: 78.30031943, src-aux-loss: 95.23886079, tar-aux-loss: 95.45835114
Epoch: [9  ] train-acc: 0.89178571, dom-acc: 0.62098214, val-acc: 0.87500000, val_loss: 0.30649808
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 299.69026709, sen-loss: 32.02219000, dom-loss: 78.17556125, src-aux-loss: 93.62540787, tar-aux-loss: 95.86710894
Epoch: [10 ] train-acc: 0.89714286, dom-acc: 0.62535714, val-acc: 0.88500000, val_loss: 0.30146584
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 295.12328196, sen-loss: 31.00193827, dom-loss: 78.16967195, src-aux-loss: 91.51980686, tar-aux-loss: 94.43186396
Epoch: [11 ] train-acc: 0.89964286, dom-acc: 0.62714286, val-acc: 0.87750000, val_loss: 0.30267102
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 292.32208848, sen-loss: 30.12149274, dom-loss: 78.07507020, src-aux-loss: 89.78516167, tar-aux-loss: 94.34036487
Epoch: [12 ] train-acc: 0.90660714, dom-acc: 0.64580357, val-acc: 0.88250000, val_loss: 0.29390404
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 288.84024453, sen-loss: 29.40941373, dom-loss: 78.14736426, src-aux-loss: 88.13778156, tar-aux-loss: 93.14568454
Epoch: [13 ] train-acc: 0.90678571, dom-acc: 0.63642857, val-acc: 0.88500000, val_loss: 0.29580766
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 286.75262523, sen-loss: 28.65721402, dom-loss: 78.08219647, src-aux-loss: 86.53804982, tar-aux-loss: 93.47516531
Epoch: [14 ] train-acc: 0.90857143, dom-acc: 0.62571429, val-acc: 0.88500000, val_loss: 0.29573745
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 284.64254451, sen-loss: 28.25538163, dom-loss: 77.93381375, src-aux-loss: 85.64695793, tar-aux-loss: 92.80639076
Epoch: [15 ] train-acc: 0.91339286, dom-acc: 0.65348214, val-acc: 0.89000000, val_loss: 0.28676423
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 281.47983289, sen-loss: 27.72832676, dom-loss: 78.02337909, src-aux-loss: 84.21304724, tar-aux-loss: 91.51508147
Epoch: [16 ] train-acc: 0.91375000, dom-acc: 0.62589286, val-acc: 0.89000000, val_loss: 0.28059262
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 280.81842351, sen-loss: 27.19422483, dom-loss: 77.98826212, src-aux-loss: 83.24322891, tar-aux-loss: 92.39270836
Epoch: [17 ] train-acc: 0.91392857, dom-acc: 0.62919643, val-acc: 0.88500000, val_loss: 0.27781066
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 277.21153545, sen-loss: 26.68401300, dom-loss: 77.97985071, src-aux-loss: 82.06723392, tar-aux-loss: 90.48043650
Epoch: [18 ] train-acc: 0.91821429, dom-acc: 0.62839286, val-acc: 0.89000000, val_loss: 0.28188094
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 276.97003603, sen-loss: 26.21561886, dom-loss: 78.05353588, src-aux-loss: 81.09763962, tar-aux-loss: 91.60324097
Epoch: [19 ] train-acc: 0.91857143, dom-acc: 0.61125000, val-acc: 0.89250000, val_loss: 0.27482072
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 275.43899226, sen-loss: 25.91148744, dom-loss: 77.93940437, src-aux-loss: 80.23636812, tar-aux-loss: 91.35173059
Epoch: [20 ] train-acc: 0.92160714, dom-acc: 0.61705357, val-acc: 0.89000000, val_loss: 0.28268027
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 272.11713457, sen-loss: 25.44355261, dom-loss: 77.96409029, src-aux-loss: 78.99168566, tar-aux-loss: 89.71780425
Epoch: [21 ] train-acc: 0.92250000, dom-acc: 0.61383929, val-acc: 0.89500000, val_loss: 0.28201181
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 271.34724522, sen-loss: 25.24801672, dom-loss: 77.89817590, src-aux-loss: 78.04938275, tar-aux-loss: 90.15167090
Epoch: [22 ] train-acc: 0.92553571, dom-acc: 0.63910714, val-acc: 0.89750000, val_loss: 0.27766502
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 268.47647262, sen-loss: 24.79407304, dom-loss: 77.82034326, src-aux-loss: 77.19770268, tar-aux-loss: 88.66435462
Epoch: [23 ] train-acc: 0.92571429, dom-acc: 0.64035714, val-acc: 0.89500000, val_loss: 0.27108774
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 269.74680352, sen-loss: 24.57878322, dom-loss: 77.91566277, src-aux-loss: 76.37303430, tar-aux-loss: 90.87932330
Epoch: [24 ] train-acc: 0.92750000, dom-acc: 0.61071429, val-acc: 0.89250000, val_loss: 0.27379647
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 268.80614614, sen-loss: 24.27211298, dom-loss: 77.86534172, src-aux-loss: 75.87664813, tar-aux-loss: 90.79204220
Epoch: [25 ] train-acc: 0.92785714, dom-acc: 0.60857143, val-acc: 0.88750000, val_loss: 0.27346575
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 265.79272914, sen-loss: 23.97948500, dom-loss: 77.82316750, src-aux-loss: 74.83724463, tar-aux-loss: 89.15283114
Epoch: [26 ] train-acc: 0.92982143, dom-acc: 0.62178571, val-acc: 0.88750000, val_loss: 0.27408528
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 264.63389099, sen-loss: 23.63735256, dom-loss: 77.89573425, src-aux-loss: 74.03696847, tar-aux-loss: 89.06383479
Epoch: [27 ] train-acc: 0.93053571, dom-acc: 0.60000000, val-acc: 0.89000000, val_loss: 0.27232298
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 263.37543750, sen-loss: 23.46005375, dom-loss: 77.83044100, src-aux-loss: 73.00746164, tar-aux-loss: 89.07748181
Epoch: [28 ] train-acc: 0.92928571, dom-acc: 0.60857143, val-acc: 0.88500000, val_loss: 0.27188882
---------------------------------------------------

Successfully load model from save path: ./work/models/video_dvd_HATN.ckpt
Best Epoch: [ 23] best val accuracy: 0.00000000 best val loss: 0.27108774
Testing accuracy: 0.87600000
./work/attentions/video_dvd_train_HATN.txt
./work/attentions/video_dvd_test_HATN.txt
loading data...
source domain:  video target domain: electronics
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 17009
vocab-size:  83059
['best', 'great', 'funny', 'good', 'excellent', 'enjoyable', 'amazing', 'classic', 'beautifully', 'favorite', 'love', 'superb', 'loved', 'nice', 'fantastic', 'awesome', 'perfect', 'underrated', 'brilliant', 'hilarious', 'solid', 'informative', 'greatest', 'funniest', 'brilliantly', 'beautiful', 'hard', 'fine', 'enjoyed', 'entertaining', 'easy', 'terrific', 'immortal', 'epic', 'clever', 'sad', 'un', 'fabulous', 'wonderful', 'perfectly', 'decent', 'magnificent', 'incredible', 'cool', 'wonderfully', 'recommend', 'superbly', 'pleasant', 'masterful', 'liked', 'surprisingly', 'believable', 'outstanding', 'recommended', 'pretty', 'deserved', 'happy']
['worst', 'horrible', 'terrible', 'bad', 'boring', 'poor', 'garbled', 'dissapointing', 'bother', 'disappointing', 'awful', 'worse', 'swallowed', 'average', 'wrong', 'wasted', 'frustrating', 'disappointed', 'dull', 'cuts', 'better', 'unfunny', 'poorly', 'put', 'annoying', 'ok', 'asleep', 'unbelievable', 'dreadful', 'low', 'unwatchable', 'unnecessary', 'needless', 'dissapointed', 'forgettable', 'uninspired', 'watchable', 'uncooked', 'predictable', 'ruined', 'atrocious', 'contrived', 'disgusting', 'horribly', 'sorry', 'stupidest', 'pointless', 'unrealistic', 'unlikeable', 'mediocre', 'miserable', 'unmemorable', 'laughable', 'lousy', 'wasting', 'cheesy', 'unbearable', 'angst', 'blah', 'amusing']
max  story size: 129
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
5600 400 6000 36180 17009
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(83060, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 412.39841866, sen-loss: 76.73644507, dom-loss: 78.35688430, src-aux-loss: 137.35141981, tar-aux-loss: 119.95366949
Epoch: [1  ] train-acc: 0.66035714, dom-acc: 0.80517857, val-acc: 0.65250000, val_loss: 0.65442514
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 382.25943041, sen-loss: 69.41369641, dom-loss: 74.36381984, src-aux-loss: 127.20072275, tar-aux-loss: 111.28119296
Epoch: [2  ] train-acc: 0.75678571, dom-acc: 0.84053571, val-acc: 0.72500000, val_loss: 0.58526760
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 360.02008510, sen-loss: 61.17437467, dom-loss: 73.04639065, src-aux-loss: 119.70545757, tar-aux-loss: 106.09386396
Epoch: [3  ] train-acc: 0.79500000, dom-acc: 0.76625000, val-acc: 0.78750000, val_loss: 0.50867367
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 342.61746144, sen-loss: 53.23018640, dom-loss: 73.32965100, src-aux-loss: 112.93352365, tar-aux-loss: 103.12410045
Epoch: [4  ] train-acc: 0.82160714, dom-acc: 0.62732143, val-acc: 0.81500000, val_loss: 0.44143870
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 329.81412220, sen-loss: 46.69746619, dom-loss: 74.43087071, src-aux-loss: 107.09600228, tar-aux-loss: 101.58978188
Epoch: [5  ] train-acc: 0.84714286, dom-acc: 0.57803571, val-acc: 0.85250000, val_loss: 0.39210543
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 322.45480323, sen-loss: 42.00036323, dom-loss: 75.64322710, src-aux-loss: 102.67429519, tar-aux-loss: 102.13691670
Epoch: [6  ] train-acc: 0.86142857, dom-acc: 0.57446429, val-acc: 0.86000000, val_loss: 0.35930759
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 314.71363330, sen-loss: 38.64652105, dom-loss: 76.37279797, src-aux-loss: 99.48663527, tar-aux-loss: 100.20767933
Epoch: [7  ] train-acc: 0.87267857, dom-acc: 0.62750000, val-acc: 0.87250000, val_loss: 0.33497480
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 309.14479542, sen-loss: 35.90730427, dom-loss: 77.09214532, src-aux-loss: 97.00845063, tar-aux-loss: 99.13689399
Epoch: [8  ] train-acc: 0.88392857, dom-acc: 0.61892857, val-acc: 0.86500000, val_loss: 0.32213965
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 304.22024322, sen-loss: 33.97519692, dom-loss: 77.81602263, src-aux-loss: 94.98630577, tar-aux-loss: 97.44271803
Epoch: [9  ] train-acc: 0.89017857, dom-acc: 0.60982143, val-acc: 0.88500000, val_loss: 0.30761603
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 301.78885913, sen-loss: 32.84871106, dom-loss: 78.47023928, src-aux-loss: 93.22081709, tar-aux-loss: 97.24909228
Epoch: [10 ] train-acc: 0.89589286, dom-acc: 0.54357143, val-acc: 0.88500000, val_loss: 0.30297402
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 298.11357737, sen-loss: 31.85982323, dom-loss: 78.73472613, src-aux-loss: 91.43296546, tar-aux-loss: 96.08606344
Epoch: [11 ] train-acc: 0.89642857, dom-acc: 0.51946429, val-acc: 0.88500000, val_loss: 0.29655883
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 295.77266622, sen-loss: 30.75738728, dom-loss: 78.84600961, src-aux-loss: 89.89372629, tar-aux-loss: 96.27554184
Epoch: [12 ] train-acc: 0.89892857, dom-acc: 0.52241071, val-acc: 0.89250000, val_loss: 0.29456609
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 293.52013373, sen-loss: 30.19818079, dom-loss: 78.82795775, src-aux-loss: 88.71877998, tar-aux-loss: 95.77521342
Epoch: [13 ] train-acc: 0.90267857, dom-acc: 0.50705357, val-acc: 0.87500000, val_loss: 0.30283695
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 290.60654712, sen-loss: 29.33797558, dom-loss: 78.83783466, src-aux-loss: 87.02828777, tar-aux-loss: 95.40244973
Epoch: [14 ] train-acc: 0.90517857, dom-acc: 0.46357143, val-acc: 0.89250000, val_loss: 0.29440859
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 289.66841626, sen-loss: 28.91494767, dom-loss: 78.53571153, src-aux-loss: 86.01748365, tar-aux-loss: 96.20027554
Epoch: [15 ] train-acc: 0.90696429, dom-acc: 0.46910714, val-acc: 0.89250000, val_loss: 0.28277361
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 286.66992450, sen-loss: 28.43207328, dom-loss: 78.25369281, src-aux-loss: 84.70635423, tar-aux-loss: 95.27780455
Epoch: [16 ] train-acc: 0.90928571, dom-acc: 0.48812500, val-acc: 0.88750000, val_loss: 0.27712792
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 284.89393616, sen-loss: 27.82655773, dom-loss: 77.94725436, src-aux-loss: 83.94255239, tar-aux-loss: 95.17757165
Epoch: [17 ] train-acc: 0.91071429, dom-acc: 0.47267857, val-acc: 0.88750000, val_loss: 0.27572489
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 280.94924879, sen-loss: 27.27358814, dom-loss: 77.73192036, src-aux-loss: 82.53411144, tar-aux-loss: 93.40963042
Epoch: [18 ] train-acc: 0.91232143, dom-acc: 0.48017857, val-acc: 0.89000000, val_loss: 0.27696577
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 280.36424208, sen-loss: 26.91530752, dom-loss: 77.42645216, src-aux-loss: 81.68995565, tar-aux-loss: 94.33252782
Epoch: [19 ] train-acc: 0.91375000, dom-acc: 0.46946429, val-acc: 0.89250000, val_loss: 0.27050671
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 278.62207699, sen-loss: 26.54331547, dom-loss: 77.45836037, src-aux-loss: 80.79088211, tar-aux-loss: 93.82951927
Epoch: [20 ] train-acc: 0.91464286, dom-acc: 0.46937500, val-acc: 0.89000000, val_loss: 0.28130439
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 277.32104564, sen-loss: 26.10808223, dom-loss: 77.41361117, src-aux-loss: 79.77140778, tar-aux-loss: 94.02794492
Epoch: [21 ] train-acc: 0.91535714, dom-acc: 0.48526786, val-acc: 0.88000000, val_loss: 0.27679530
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 275.60535264, sen-loss: 25.87375962, dom-loss: 77.43177956, src-aux-loss: 78.73659739, tar-aux-loss: 93.56321466
Epoch: [22 ] train-acc: 0.91803571, dom-acc: 0.47267857, val-acc: 0.88250000, val_loss: 0.27112937
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 273.92174816, sen-loss: 25.38577567, dom-loss: 77.59275532, src-aux-loss: 78.05291089, tar-aux-loss: 92.89030552
Epoch: [23 ] train-acc: 0.92089286, dom-acc: 0.48883929, val-acc: 0.88500000, val_loss: 0.26815581
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 274.24120569, sen-loss: 25.16687275, dom-loss: 77.70962977, src-aux-loss: 77.00382128, tar-aux-loss: 94.36088067
Epoch: [24 ] train-acc: 0.92160714, dom-acc: 0.47892857, val-acc: 0.88250000, val_loss: 0.26821584
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 271.94022655, sen-loss: 24.80833607, dom-loss: 77.90195024, src-aux-loss: 76.51031631, tar-aux-loss: 92.71962225
Epoch: [25 ] train-acc: 0.92285714, dom-acc: 0.46142857, val-acc: 0.88500000, val_loss: 0.26777554
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 270.63656735, sen-loss: 24.62432628, dom-loss: 78.06945616, src-aux-loss: 75.70035419, tar-aux-loss: 92.24243164
Epoch: [26 ] train-acc: 0.92410714, dom-acc: 0.45151786, val-acc: 0.88000000, val_loss: 0.26886275
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 269.30271649, sen-loss: 24.23579536, dom-loss: 78.14822787, src-aux-loss: 74.86925825, tar-aux-loss: 92.04943484
Epoch: [27 ] train-acc: 0.92500000, dom-acc: 0.43125000, val-acc: 0.88250000, val_loss: 0.26495442
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 267.90451789, sen-loss: 23.91487595, dom-loss: 78.21971226, src-aux-loss: 73.81031057, tar-aux-loss: 91.95961928
Epoch: [28 ] train-acc: 0.92500000, dom-acc: 0.42732143, val-acc: 0.88500000, val_loss: 0.26427186
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 267.27342010, sen-loss: 23.55206001, dom-loss: 78.40242505, src-aux-loss: 73.68958557, tar-aux-loss: 91.62935072
Epoch: [29 ] train-acc: 0.92625000, dom-acc: 0.43812500, val-acc: 0.88750000, val_loss: 0.26416385
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 266.84101605, sen-loss: 23.17779021, dom-loss: 78.60750544, src-aux-loss: 72.53029934, tar-aux-loss: 92.52542216
Epoch: [30 ] train-acc: 0.92714286, dom-acc: 0.41866071, val-acc: 0.88250000, val_loss: 0.26622283
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 265.56658912, sen-loss: 22.98527329, dom-loss: 78.56413561, src-aux-loss: 72.15588629, tar-aux-loss: 91.86129504
Epoch: [31 ] train-acc: 0.93000000, dom-acc: 0.42776786, val-acc: 0.88500000, val_loss: 0.26592049
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 263.79532266, sen-loss: 22.71003865, dom-loss: 78.53513706, src-aux-loss: 71.55433810, tar-aux-loss: 90.99580860
Epoch: [32 ] train-acc: 0.92964286, dom-acc: 0.41312500, val-acc: 0.88750000, val_loss: 0.28009030
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 264.32825494, sen-loss: 22.48841060, dom-loss: 78.38788158, src-aux-loss: 70.67343476, tar-aux-loss: 92.77852786
Epoch: [33 ] train-acc: 0.93053571, dom-acc: 0.42348214, val-acc: 0.89000000, val_loss: 0.27238479
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 260.02850366, sen-loss: 22.16130716, dom-loss: 78.26543719, src-aux-loss: 69.88968095, tar-aux-loss: 89.71207780
Epoch: [34 ] train-acc: 0.93000000, dom-acc: 0.45678571, val-acc: 0.89250000, val_loss: 0.28216282
---------------------------------------------------

Successfully load model from save path: ./work/models/video_electronics_HATN.ckpt
Best Epoch: [ 29] best val accuracy: 0.00000000 best val loss: 0.26416385
Testing accuracy: 0.85850000
./work/attentions/video_electronics_train_HATN.txt
./work/attentions/video_electronics_test_HATN.txt
loading data...
source domain:  video target domain: kitchen
train-size:  5600
val-size:  400
test-size:  6000
unlabeled-size:  30180 13856
vocab-size:  78115
['best', 'great', 'funny', 'good', 'excellent', 'enjoyable', 'classic', 'favorite', 'amazing', 'love', 'beautifully', 'superb', 'perfect', 'fantastic', 'awesome', 'loved', 'nice', 'brilliant', 'hilarious', 'underrated', 'informative', 'funniest', 'beautiful', 'solid', 'enjoyed', 'fine', 'hard', 'entertaining', 'greatest', 'easy', 'brilliantly', 'fabulous', 'clever', 'wonderful', 'immortal', 'epic', 'un', 'cool', 'terrific', 'sad', 'decent', 'watchable', 'magnificent', 'recommend', 'superbly', 'pleasant', 'perfectly', 'outstanding', 'recommended', 'masterful', 'surprisingly', 'incredible', 'wonderfully', 'comedic', 'lovely', 'riveting', 'poignant', 'happy']
['worst', 'horrible', 'terrible', 'bad', 'boring', 'poor', 'disappointing', 'worse', 'garbled', 'dissapointing', 'bother', 'awful', 'average', 'wasted', 'wrong', 'read', 'dull', 'disappointed', 'unfunny', 'poorly', 'annoying', 'unwatchable', 'dreadful', 'atrocious', 'ok', 'low', 'dissapointed', 'forgettable', 'unnecessary', 'needless', 'uninspired', 'unbelievable', 'disgusting', 'uncooked', 'asleep', 'predictable', 'waste', 'ruined', 'contrived', 'lackluster', 'dumbest', 'laughable', 'stupidest', 'lousy', 'unrealistic', 'unlikeable', 'mediocre', 'amusing', 'sappy', 'miserable', 'pretentious', 'weak', 'unmemorable', 'pointless', 'empty', 'unbearable', 'vapid']
max  story size: 104
mean story size: 7
max  sentence size: 959
mean sentence size: 18
max memory size: 20
5600 400 6000 36180 13856
joint_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/W_fc:0' shape=(600, 2) dtype=float32_ref>
<tf.Variable 'HATN/joint/sentiment_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
aux_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/pos_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/NP_net/neg_pivot_predictor/b_fc:0' shape=(2,) dtype=float32_ref>
dom_train
<tf.Variable 'HATN/word2vec:0' shape=(78116, 300) dtype=float32_ref>
<tf.Variable 'HATN/w_pe:0' shape=(1, 25, 300) dtype=float32_ref>
<tf.Variable 'HATN/s_pe:0' shape=(1, 20, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_query:0' shape=(1, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/W_w:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/word_attention/b_w:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/W_c:0' shape=(300, 300) dtype=float32_ref>
<tf.Variable 'HATN/P_net/sent_attention/b_c:0' shape=(300,) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/W_fc:0' shape=(300, 2) dtype=float32_ref>
<tf.Variable 'HATN/P_net/domain_classifier/b_fc:0' shape=(2,) dtype=float32_ref>
adapt 0.0 lr 0.005
Epoch: [1  ] loss: 407.65716863, sen-loss: 76.59703660, dom-loss: 78.20302522, src-aux-loss: 141.16539323, tar-aux-loss: 111.69171470
Epoch: [1  ] train-acc: 0.66267857, dom-acc: 0.83098214, val-acc: 0.64500000, val_loss: 0.65266061
---------------------------------------------------

adapt 0.0499583749579 lr 0.00465506222311
Epoch: [2  ] loss: 374.87959909, sen-loss: 69.06998754, dom-loss: 73.76784986, src-aux-loss: 131.02169943, tar-aux-loss: 101.02006137
Epoch: [2  ] train-acc: 0.75714286, dom-acc: 0.89875000, val-acc: 0.72750000, val_loss: 0.58155918
---------------------------------------------------

adapt 0.099667994625 lr 0.00436097974747
Epoch: [3  ] loss: 352.89300203, sen-loss: 60.57176992, dom-loss: 72.49634236, src-aux-loss: 122.49163485, tar-aux-loss: 97.33325589
Epoch: [3  ] train-acc: 0.79625000, dom-acc: 0.89455357, val-acc: 0.80750000, val_loss: 0.50526619
---------------------------------------------------

adapt 0.1 lr 0.00410688450912
Epoch: [4  ] loss: 333.21409321, sen-loss: 52.59400901, dom-loss: 72.70099568, src-aux-loss: 114.88093823, tar-aux-loss: 93.03814930
Epoch: [4  ] train-acc: 0.82875000, dom-acc: 0.85714286, val-acc: 0.82000000, val_loss: 0.43715009
---------------------------------------------------

adapt 0.1 lr 0.0038848475212
Epoch: [5  ] loss: 319.70917392, sen-loss: 46.05080062, dom-loss: 73.57299429, src-aux-loss: 108.58985758, tar-aux-loss: 91.49552202
Epoch: [5  ] train-acc: 0.83892857, dom-acc: 0.85741071, val-acc: 0.84000000, val_loss: 0.38782266
---------------------------------------------------

adapt 0.1 lr 0.00368893973233
Epoch: [6  ] loss: 310.95988250, sen-loss: 41.50751056, dom-loss: 74.30816215, src-aux-loss: 104.10291719, tar-aux-loss: 91.04129255
Epoch: [6  ] train-acc: 0.86321429, dom-acc: 0.80383929, val-acc: 0.85250000, val_loss: 0.35815069
---------------------------------------------------

adapt 0.1 lr 0.00351463328244
Epoch: [7  ] loss: 304.27371860, sen-loss: 38.20193990, dom-loss: 75.36603504, src-aux-loss: 100.68177325, tar-aux-loss: 90.02396828
Epoch: [7  ] train-acc: 0.87732143, dom-acc: 0.73919643, val-acc: 0.85750000, val_loss: 0.33863166
---------------------------------------------------

adapt 0.1 lr 0.00335840689834
Epoch: [8  ] loss: 298.13436460, sen-loss: 35.43477044, dom-loss: 76.27718222, src-aux-loss: 98.28445363, tar-aux-loss: 88.13795787
Epoch: [8  ] train-acc: 0.88607143, dom-acc: 0.69973214, val-acc: 0.87500000, val_loss: 0.31515902
---------------------------------------------------

adapt 0.1 lr 0.00321747829247
Epoch: [9  ] loss: 294.70155907, sen-loss: 33.50345158, dom-loss: 77.31759471, src-aux-loss: 96.93852949, tar-aux-loss: 86.94198287
Epoch: [9  ] train-acc: 0.89071429, dom-acc: 0.65125000, val-acc: 0.88250000, val_loss: 0.30279839
---------------------------------------------------

adapt 0.1 lr 0.00308961812091
Epoch: [10 ] loss: 292.93271661, sen-loss: 32.31482556, dom-loss: 78.31664133, src-aux-loss: 94.68954754, tar-aux-loss: 87.61170375
Epoch: [10 ] train-acc: 0.89625000, dom-acc: 0.63437500, val-acc: 0.87000000, val_loss: 0.29705080
---------------------------------------------------

adapt 0.1 lr 0.00297301778751
Epoch: [11 ] loss: 289.80451918, sen-loss: 31.36401333, dom-loss: 78.70768780, src-aux-loss: 93.35168290, tar-aux-loss: 86.38113654
Epoch: [11 ] train-acc: 0.89910714, dom-acc: 0.60812500, val-acc: 0.87500000, val_loss: 0.29644409
---------------------------------------------------

adapt 0.1 lr 0.00286619367501
Epoch: [12 ] loss: 286.37708044, sen-loss: 30.40140173, dom-loss: 78.94804353, src-aux-loss: 91.09927267, tar-aux-loss: 85.92836457
Epoch: [12 ] train-acc: 0.90250000, dom-acc: 0.59285714, val-acc: 0.88500000, val_loss: 0.28936255
---------------------------------------------------

adapt 0.1 lr 0.00276791655825
Epoch: [13 ] loss: 287.45385075, sen-loss: 29.74489372, dom-loss: 79.35757232, src-aux-loss: 89.93251789, tar-aux-loss: 88.41886771
Epoch: [13 ] train-acc: 0.90250000, dom-acc: 0.58044643, val-acc: 0.88000000, val_loss: 0.29398784
---------------------------------------------------

adapt 0.1 lr 0.00267715876605
Epoch: [14 ] loss: 281.82094121, sen-loss: 29.05340116, dom-loss: 79.29914045, src-aux-loss: 88.65227735, tar-aux-loss: 84.81612164
Epoch: [14 ] train-acc: 0.90571429, dom-acc: 0.57732143, val-acc: 0.88250000, val_loss: 0.29005757
---------------------------------------------------

adapt 0.1 lr 0.00259305407204
Epoch: [15 ] loss: 279.70980191, sen-loss: 28.50324898, dom-loss: 79.19179517, src-aux-loss: 87.42455834, tar-aux-loss: 84.59019911
Epoch: [15 ] train-acc: 0.91142857, dom-acc: 0.58732143, val-acc: 0.89250000, val_loss: 0.28173375
---------------------------------------------------

adapt 0.1 lr 0.00251486685937
Epoch: [16 ] loss: 276.89567947, sen-loss: 27.97125541, dom-loss: 78.92833614, src-aux-loss: 86.14384103, tar-aux-loss: 83.85224724
Epoch: [16 ] train-acc: 0.90857143, dom-acc: 0.56705357, val-acc: 0.89250000, val_loss: 0.27694303
---------------------------------------------------

adapt 0.1 lr 0.00244196813937
Epoch: [17 ] loss: 275.60590124, sen-loss: 27.46561133, dom-loss: 78.65831405, src-aux-loss: 85.13266981, tar-aux-loss: 84.34930617
Epoch: [17 ] train-acc: 0.91357143, dom-acc: 0.58803571, val-acc: 0.89250000, val_loss: 0.27407241
---------------------------------------------------

adapt 0.1 lr 0.0023738167022
Epoch: [18 ] loss: 274.11431181, sen-loss: 26.92073163, dom-loss: 78.48023206, src-aux-loss: 84.19409007, tar-aux-loss: 84.51925915
Epoch: [18 ] train-acc: 0.91482143, dom-acc: 0.56642857, val-acc: 0.90000000, val_loss: 0.27683949
---------------------------------------------------

adapt 0.1 lr 0.00230994415646
Epoch: [19 ] loss: 272.40643930, sen-loss: 26.47651623, dom-loss: 78.19707334, src-aux-loss: 83.09327137, tar-aux-loss: 84.63957942
Epoch: [19 ] train-acc: 0.91857143, dom-acc: 0.55035714, val-acc: 0.89750000, val_loss: 0.26975092
---------------------------------------------------

adapt 0.1 lr 0.00224994294854
Epoch: [20 ] loss: 268.81089139, sen-loss: 26.21634936, dom-loss: 78.02685070, src-aux-loss: 82.35769111, tar-aux-loss: 82.21000034
Epoch: [20 ] train-acc: 0.91982143, dom-acc: 0.57008929, val-acc: 0.89500000, val_loss: 0.27488342
---------------------------------------------------

adapt 0.1 lr 0.00219345668825
Epoch: [21 ] loss: 267.64144349, sen-loss: 25.69882395, dom-loss: 77.81778747, src-aux-loss: 81.30645281, tar-aux-loss: 82.81837845
Epoch: [21 ] train-acc: 0.92017857, dom-acc: 0.57133929, val-acc: 0.89500000, val_loss: 0.27164581
---------------------------------------------------

adapt 0.1 lr 0.00214017227647
Epoch: [22 ] loss: 266.76970530, sen-loss: 25.56432842, dom-loss: 77.94432354, src-aux-loss: 80.40284866, tar-aux-loss: 82.85820460
Epoch: [22 ] train-acc: 0.92160714, dom-acc: 0.56714286, val-acc: 0.89750000, val_loss: 0.27218318
---------------------------------------------------

adapt 0.1 lr 0.00208981345305
Epoch: [23 ] loss: 264.60490966, sen-loss: 25.08112981, dom-loss: 77.88657784, src-aux-loss: 79.58823088, tar-aux-loss: 82.04897076
Epoch: [23 ] train-acc: 0.92196429, dom-acc: 0.56205357, val-acc: 0.89250000, val_loss: 0.26608968
---------------------------------------------------

adapt 0.1 lr 0.0020421354735
Epoch: [24 ] loss: 264.78374863, sen-loss: 24.82897557, dom-loss: 77.93668890, src-aux-loss: 78.67389268, tar-aux-loss: 83.34419048
Epoch: [24 ] train-acc: 0.92303571, dom-acc: 0.55017857, val-acc: 0.89750000, val_loss: 0.26386145
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [25 ] loss: 261.91300511, sen-loss: 24.41974947, dom-loss: 78.04484755, src-aux-loss: 77.99035427, tar-aux-loss: 81.45805418
Epoch: [25 ] train-acc: 0.92535714, dom-acc: 0.53125000, val-acc: 0.90000000, val_loss: 0.26340312
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [26 ] loss: 262.80662227, sen-loss: 24.18324739, dom-loss: 78.14730895, src-aux-loss: 77.14160812, tar-aux-loss: 83.33445662
Epoch: [26 ] train-acc: 0.92607143, dom-acc: 0.52571429, val-acc: 0.89250000, val_loss: 0.26300257
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [27 ] loss: 260.46553230, sen-loss: 23.75399336, dom-loss: 78.33361590, src-aux-loss: 76.50876659, tar-aux-loss: 81.86915815
Epoch: [27 ] train-acc: 0.92642857, dom-acc: 0.51187500, val-acc: 0.90250000, val_loss: 0.25965527
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [28 ] loss: 257.92606199, sen-loss: 23.52585140, dom-loss: 78.47388172, src-aux-loss: 75.47045285, tar-aux-loss: 80.45587522
Epoch: [28 ] train-acc: 0.92803571, dom-acc: 0.49223214, val-acc: 0.90000000, val_loss: 0.25907797
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [29 ] loss: 257.13066697, sen-loss: 23.17514547, dom-loss: 78.52175200, src-aux-loss: 74.83040878, tar-aux-loss: 80.60336143
Epoch: [29 ] train-acc: 0.92982143, dom-acc: 0.47348214, val-acc: 0.90500000, val_loss: 0.25865501
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [30 ] loss: 258.03797376, sen-loss: 22.81851707, dom-loss: 78.76670200, src-aux-loss: 73.98541349, tar-aux-loss: 82.46733975
Epoch: [30 ] train-acc: 0.93107143, dom-acc: 0.46598214, val-acc: 0.90750000, val_loss: 0.25673240
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [31 ] loss: 257.26055717, sen-loss: 22.50196512, dom-loss: 78.78998357, src-aux-loss: 73.37849811, tar-aux-loss: 82.59011060
Epoch: [31 ] train-acc: 0.93232143, dom-acc: 0.47767857, val-acc: 0.90250000, val_loss: 0.25637469
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [32 ] loss: 254.74468911, sen-loss: 22.36152484, dom-loss: 78.79409516, src-aux-loss: 72.40255991, tar-aux-loss: 81.18650961
Epoch: [32 ] train-acc: 0.92964286, dom-acc: 0.42035714, val-acc: 0.89250000, val_loss: 0.26742262
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [33 ] loss: 253.06209123, sen-loss: 21.94665001, dom-loss: 78.83114463, src-aux-loss: 71.67005026, tar-aux-loss: 80.61424565
Epoch: [33 ] train-acc: 0.93160714, dom-acc: 0.44571429, val-acc: 0.89000000, val_loss: 0.26381841
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [34 ] loss: 251.05728662, sen-loss: 21.62907221, dom-loss: 78.71910965, src-aux-loss: 70.84113416, tar-aux-loss: 79.86797115
Epoch: [34 ] train-acc: 0.93053571, dom-acc: 0.48660714, val-acc: 0.89500000, val_loss: 0.26962352
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [35 ] loss: 250.18541920, sen-loss: 21.42769007, dom-loss: 78.66853386, src-aux-loss: 70.06025642, tar-aux-loss: 80.02894062
Epoch: [35 ] train-acc: 0.93839286, dom-acc: 0.46687500, val-acc: 0.90000000, val_loss: 0.25523987
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [36 ] loss: 249.00100899, sen-loss: 21.16544203, dom-loss: 78.42611974, src-aux-loss: 69.08721498, tar-aux-loss: 80.32223356
Epoch: [36 ] train-acc: 0.93660714, dom-acc: 0.45937500, val-acc: 0.90750000, val_loss: 0.25522316
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [37 ] loss: 247.64774430, sen-loss: 20.86399165, dom-loss: 78.13889891, src-aux-loss: 68.56240019, tar-aux-loss: 80.08245230
Epoch: [37 ] train-acc: 0.93464286, dom-acc: 0.50080357, val-acc: 0.89250000, val_loss: 0.25553933
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [38 ] loss: 247.11396539, sen-loss: 20.54636454, dom-loss: 77.89573175, src-aux-loss: 67.78564602, tar-aux-loss: 80.88622308
Epoch: [38 ] train-acc: 0.94071429, dom-acc: 0.49267857, val-acc: 0.90000000, val_loss: 0.25429532
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [39 ] loss: 246.16419339, sen-loss: 20.27698547, dom-loss: 77.72625649, src-aux-loss: 67.28115702, tar-aux-loss: 80.87979585
Epoch: [39 ] train-acc: 0.94107143, dom-acc: 0.58098214, val-acc: 0.90250000, val_loss: 0.25529057
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [40 ] loss: 242.48393416, sen-loss: 19.99300407, dom-loss: 77.41853565, src-aux-loss: 66.11200553, tar-aux-loss: 78.96038705
Epoch: [40 ] train-acc: 0.94160714, dom-acc: 0.54589286, val-acc: 0.90000000, val_loss: 0.25855440
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [41 ] loss: 241.96937406, sen-loss: 19.66236661, dom-loss: 77.37369460, src-aux-loss: 65.56879082, tar-aux-loss: 79.36451936
Epoch: [41 ] train-acc: 0.94446429, dom-acc: 0.58125000, val-acc: 0.89750000, val_loss: 0.25648397
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [42 ] loss: 240.94301081, sen-loss: 19.39356408, dom-loss: 77.13873011, src-aux-loss: 64.63987747, tar-aux-loss: 79.77083796
Epoch: [42 ] train-acc: 0.94357143, dom-acc: 0.59223214, val-acc: 0.90750000, val_loss: 0.25525782
---------------------------------------------------

adapt 0.1 lr 0.002
Epoch: [43 ] loss: 239.34792340, sen-loss: 19.20664163, dom-loss: 76.99694103, src-aux-loss: 64.06456587, tar-aux-loss: 79.07977533
Epoch: [43 ] train-acc: 0.94625000, dom-acc: 0.57464286, val-acc: 0.90250000, val_loss: 0.25641611
---------------------------------------------------

Successfully load model from save path: ./work/models/video_kitchen_HATN.ckpt
Best Epoch: [ 38] best val accuracy: 0.00000000 best val loss: 0.25429532
Testing accuracy: 0.86016667
./work/attentions/video_kitchen_train_HATN.txt
./work/attentions/video_kitchen_test_HATN.txt
